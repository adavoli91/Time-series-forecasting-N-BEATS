{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7cbcdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import nn, tensor, no_grad, optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44d62bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_blocks(len_series: int, filter_size: int, base_dilation: int = 2) -> int:\n",
    "    '''\n",
    "    Function to determine the minimum number of residual blocks for full coverage of the input time series.\n",
    "    \n",
    "    Args:\n",
    "        len_series: Length of the time series.\n",
    "        filter_size: Filter size of the 1D convolutions of the TCN.\n",
    "        base_dilation: Base of the dilation; for the i-th block of the TCN, it is supposed to be `base_dilation`**i.\n",
    "        \n",
    "    Returns:\n",
    "        n_blocks: Minimum number of residual blocks for having full coverage of the input time series.\n",
    "    '''\n",
    "    if base_dilation == 2:\n",
    "        log = np.log2(1 + (len_series - 1)/(2*(filter_size - 1)))\n",
    "    else:\n",
    "        log = np.log(1 + (len_series - 1)/(2*(filter_size - 1)))\n",
    "        log /= np.log(base_dilation)\n",
    "    #\n",
    "    n_blocks = np.ceil(log)\n",
    "    return int(n_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7f0ed219",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, num_chan: int, dilation: int, dict_params: dict, last_block: bool,\n",
    "                 gated_activation: bool = False) -> None:\n",
    "        '''\n",
    "        Residual block of the TCN.\n",
    "\n",
    "        Args:\n",
    "            num_chan: Number of features of the input time series. For a hidden layer, this is the number of filters of the previous one.\n",
    "            dilation: Dilation factor.\n",
    "            dict_params: Dictionary containing information about the model architecture.\n",
    "            last_block: Whether it is the last residual block of the TCN.\n",
    "            gated_activation: Whether to use gated (i.e., tanh*sigmoid) activation function; if false, relu is used.\n",
    "\n",
    "        Returns: None.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        #\n",
    "        filter_size = dict_params['filter_size']\n",
    "        frac_dropout = dict_params['frac_dropout']\n",
    "        num_filters = dict_params['num_filters']\n",
    "        #\n",
    "        self.padding = (filter_size - 1)*dilation\n",
    "        self.last_block = last_block\n",
    "        self.gated_activation = gated_activation\n",
    "        # first convolution\n",
    "        self.conv_1 = nn.Conv1d(in_channels = num_chan, out_channels = num_filters, kernel_size = filter_size,\n",
    "                                dilation = dilation)\n",
    "        self.conv_1 = nn.utils.parametrizations.weight_norm(self.conv_1)\n",
    "        # second convolution\n",
    "        self.conv_2 = nn.Conv1d(in_channels = num_filters, out_channels = num_filters, kernel_size = filter_size,\n",
    "                                dilation = dilation)\n",
    "        self.conv_2 = nn.utils.parametrizations.weight_norm(self.conv_2)\n",
    "        # 1D convolution\n",
    "        self.conv_1x1 = nn.Conv1d(in_channels = num_chan, out_channels = num_filters, kernel_size = 1)\n",
    "        #\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(p = frac_dropout)\n",
    "        \n",
    "    def forward(self, x: tensor) -> tensor:\n",
    "        y = self.conv_1(nn.functional.pad(x, (self.padding, 0)))\n",
    "        if self.gated_activation == False:\n",
    "            y = self.relu(y)\n",
    "        else:\n",
    "            y = self.tanh(y)*self.sigmoid(y)\n",
    "        y = self.dropout(y)\n",
    "        #\n",
    "        y = self.conv_2(nn.functional.pad(y, (self.padding, 0)))\n",
    "        if self.last_block == False:\n",
    "            if self.gated_activation == False:\n",
    "                y = self.relu(y)\n",
    "            else:\n",
    "                y = self.tanh(y)*self.sigmoid(y)\n",
    "        y = self.dropout(y)\n",
    "        #\n",
    "        return self.conv_1x1(x) + y\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, len_series: int, num_feat: int, len_output: int, dict_params: dict, gated_activation: bool = False) -> None:\n",
    "        '''\n",
    "        TCN architecture.\n",
    "\n",
    "        Args:\n",
    "            len_series: Length of the input series.\n",
    "            num_feat: Number of features of the input series.\n",
    "            len_output: Length of the output series.\n",
    "            dict_params: Dictionary containing information about the model architecture.\n",
    "            gated_activation: Whether to use gated (i.e., tanh*sigmoid) activation function; if false, relu is used.\n",
    "\n",
    "        Returns: None.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        #\n",
    "        dict_params = dict_params['model']\n",
    "        #\n",
    "        filter_size = dict_params['filter_size']\n",
    "        frac_dropout = dict_params['frac_dropout']\n",
    "        base_dilation = dict_params['base_dilation']\n",
    "        num_filters = dict_params['num_filters']\n",
    "        self.len_output = len_output\n",
    "        # get number of blocks\n",
    "        n_blocks = get_n_blocks(len_series = len_series, filter_size = filter_size, base_dilation = base_dilation)\n",
    "        # build TCN\n",
    "        list_blocks = []\n",
    "        for i in range(n_blocks):\n",
    "            if i == 0:\n",
    "                list_blocks.append(ResidualBlock(num_chan = num_feat, dilation = base_dilation**i, dict_params = dict_params,\n",
    "                                                 last_block = False, gated_activation = gated_activation))\n",
    "            elif 0 < i < n_blocks - 1:\n",
    "                list_blocks.append(ResidualBlock(num_chan = num_filters, dilation = base_dilation**i, dict_params = dict_params,\n",
    "                                                 last_block = False, gated_activation = gated_activation))\n",
    "            else:\n",
    "                list_blocks.append(ResidualBlock(num_chan = num_filters, dilation = base_dilation**i, dict_params = dict_params,\n",
    "                                                 last_block = True, gated_activation = gated_activation))\n",
    "        self.tcn = nn.ModuleList(list_blocks)\n",
    "        # final convolutional layer, used to fix dimensions\n",
    "        self.conv_final = nn.Conv1d(in_channels = num_filters, out_channels = 1, kernel_size = 1)\n",
    "        \n",
    "    def forward(self, x: tensor) -> tensor:\n",
    "        # TCN\n",
    "        y = self.tcn[0](x)\n",
    "        if len(self.tcn) > 0:\n",
    "            for i in range(1, len(self.tcn)):\n",
    "                y = self.tcn[i](y)\n",
    "        # final convolution\n",
    "        y = self.conv_final(y[:, :, -self.len_output:])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "529cf6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_params = {'data': {'len_series': 20, 'size_train': 0.8, 'size_valid': 0.3, 'horizon_forecast': 5},\n",
    "               'model': {'filter_size': 3, 'frac_dropout': 0.1, 'base_dilation': 2, 'num_filters': 30},\n",
    "               'training': {'batch_size': 128, 'n_epochs': 300, 'patience': 50, 'min_improve_valid_loss': 0.005}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "1ac5fc39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>family</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>161.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>169.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>342.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date      family      y\n",
       "0 2013-01-01  AUTOMOTIVE    0.0\n",
       "1 2013-01-02  AUTOMOTIVE  255.0\n",
       "2 2013-01-03  AUTOMOTIVE  161.0\n",
       "3 2013-01-04  AUTOMOTIVE  169.0\n",
       "4 2013-01-05  AUTOMOTIVE  342.0"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/raw/train.csv', parse_dates = ['date'], index_col = 'id')\n",
    "df = df.groupby(['date', 'family']).agg({'sales': 'sum'}).reset_index()\n",
    "# add all dates\n",
    "df_temp = []\n",
    "for family in df['family'].unique():\n",
    "    df_temp.append(pd.DataFrame({'date': pd.date_range(df['date'].min(), df['date'].max())}).merge(df[df['family'] == family],\n",
    "                                                                                                   on = 'date', how = 'left'))\n",
    "    df_temp[-1]['family'] = family\n",
    "    df_temp[-1]['sales'] = df_temp[-1]['sales'].ffill()\n",
    "df = pd.concat(df_temp).reset_index(drop = True)\n",
    "del df_temp\n",
    "df = df.rename(columns = {'sales': 'y'})\n",
    "#\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "f06c61b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_splitting(df: pd.DataFrame, dict_params: dict) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    '''\n",
    "    Function to split data in training, validation and test set.\n",
    "\n",
    "    Args:\n",
    "        df: Dataframe containing time series. The column representing the main series should be called `y`.\n",
    "        dict_params: Dictionary containing information about the model architecture.\n",
    "\n",
    "    Returns:\n",
    "        df_train: Dataframe corresponding to training set.\n",
    "        df_valid: Dataframe corresponding to validation set.\n",
    "        df_test: Dataframe corresponding to test set.\n",
    "    '''\n",
    "    df_train = df.iloc[:int(df.shape[0]*dict_params['data']['size_train'])].copy().reset_index(drop = True)\n",
    "    df_test = df.iloc[int(df.shape[0]*dict_params['data']['size_train']):].reset_index(drop = True).copy().reset_index(drop = True)\n",
    "    df_valid = df_train.iloc[int(df_train.shape[0]*(1 - dict_params['data']['size_valid'])):].copy().reset_index(drop = True)\n",
    "    df_train = df_train.iloc[:int(df_train.shape[0]*(1 - dict_params['data']['size_valid']))].copy().reset_index(drop = True)\n",
    "    # rescale data\n",
    "    scaler = StandardScaler().fit(df_train[['y']])\n",
    "    df_train[['y']] = scaler.transform(df_train[['y']])\n",
    "    df_valid[['y']] = scaler.transform(df_valid[['y']])\n",
    "    df_test[['y']] = scaler.transform(df_test[['y']])\n",
    "    #\n",
    "    return df_train, df_valid, df_test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "a5f0d1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_y(df: pd.DataFrame, df_future: pd.DataFrame, dict_params: dict, test_set: bool = False,\n",
    "            horizon_forecast: int = None) -> (tensor, tensor, np.array, np.array):\n",
    "    '''\n",
    "    Function obtain a tensor of regressors and one of target series.\n",
    "\n",
    "    Args:\n",
    "        df: Dataframe containing time series. The column representing the main series should be called `y`.\n",
    "        df_future: Same as `df`, but corresponding to its future (e.g., `df_valid` could be the \"future\" of `df_train`).\n",
    "        dict_params: Dictionary containing information about the model architecture.\n",
    "        test_set: Whether `df` is the dataframe corresponding to test set.\n",
    "        horizon_forecast: Length of the series to be predicted.\n",
    "\n",
    "    Returns:\n",
    "        x: Tensor representing regressors.\n",
    "        y: Tensor representing target time series.\n",
    "        date_x: Array containing the dates corresponding to the elements of `x`.\n",
    "        date_y: Array containing the dates corresponding to the elements of `y`.\n",
    "    '''\n",
    "    dict_params = dict_params['data']\n",
    "    #\n",
    "    len_series = dict_params['len_series']\n",
    "    if (horizon_forecast is None) or (horizon_forecast >= len_series):\n",
    "        horizon_forecast = len_series\n",
    "    #\n",
    "    df_present = df.copy()\n",
    "    if test_set == False:\n",
    "        df_future = pd.concat((df_present, df_future)).copy().shift(-len_series)\n",
    "        df_future = df_future.iloc[:df_present.shape[0]].reset_index(drop = True)\n",
    "    else:\n",
    "        df_future = df_present.copy().shift(-len_series).dropna()\n",
    "        df_present = df_present.iloc[:df_future.shape[0]]\n",
    "    #\n",
    "    x = np.array([df_present.loc[i: i + len_series - 1,\n",
    "                                 [col for col in df_present.columns if col != 'date']].values for i in range(df_present.shape[0] - len_series)])\n",
    "    y = np.array([df_future.loc[i: i + horizon_forecast - 1, ['y']].values for i in range(df_future.shape[0] - horizon_forecast)])\n",
    "    date_x = np.array([df_present.loc[i: i + len_series - 1, 'date'].values for i in range(df_present.shape[0] - len_series)])\n",
    "    date_y = np.array([df_future.loc[i: i + horizon_forecast - 1, 'date'].values for i in range(df_future.shape[0] - horizon_forecast)])\n",
    "    #\n",
    "    y = y[:x.shape[0]]\n",
    "    date_y = date_y[:date_x.shape[0]]\n",
    "    #\n",
    "    x = tensor(x.astype(np.float32))\n",
    "    y = tensor(y.astype(np.float32))\n",
    "    #\n",
    "    if len(x.shape) == 2:\n",
    "        x = x.reshape(x.shape[0], 1, -1)\n",
    "        y = y.reshape(y.shape[0], 1, -1)\n",
    "        date_x = date_x.reshape(x.shape[0], 1, -1)\n",
    "        date_y = date_y.reshape(y.shape[0], 1, -1)\n",
    "    if len(x.shape) == 3:\n",
    "        x = tensor(np.einsum('btc -> bct', x))\n",
    "        y = tensor(np.einsum('btc -> bct', y))\n",
    "        date_x = date_x.reshape(x.shape[0], 1, -1)\n",
    "        date_y = date_y.reshape(y.shape[0], 1, -1)\n",
    "    #\n",
    "    return x, y, date_x, date_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "c5326898",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, x: tensor, y: tensor):\n",
    "        '''\n",
    "        Class to create a PyTorch dataset\n",
    "        \n",
    "        Args:\n",
    "            x: Tensor representing regressors.\n",
    "            y: Tensor representing target time series.\n",
    "            \n",
    "        Returns: None.\n",
    "        '''\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "08bdcaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTCN():\n",
    "    def __init__(self, model: torch.nn.Module, dict_params: dict, dataloader_train: torch.utils.data.DataLoader, dataloader_valid: torch.utils.data.DataLoader) -> None:\n",
    "        '''\n",
    "        Class to train the TCN model.\n",
    "        \n",
    "        Args:\n",
    "            model: PyTorch model.\n",
    "            dict_params: Dictionary containing information about the model architecture.\n",
    "            dataloader_train: Dataloader containing training data.\n",
    "            dataloader_valid: Dataloader containing validation data.\n",
    "            \n",
    "        Returns: None.\n",
    "        '''\n",
    "        self.model = model\n",
    "        self.dict_params = dict_params\n",
    "        self.dataloader_train = dataloader_train\n",
    "        self.dataloader_valid = dataloader_valid\n",
    "        #\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(params = model.parameters(), lr = 1e-3)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer = self.optimizer, mode = 'min', factor = 0.5,\n",
    "                                                              patience = 10, threshold = 1e-4, threshold_mode = 'rel',\n",
    "                                                              verbose = False)\n",
    "\n",
    "    def _model_on_batch(self, batch: tuple, model: torch.nn.Module, optimizer: torch.optim, loss_func: torch.nn.modules.loss, perform_training: bool = True) -> float:\n",
    "        '''\n",
    "        Function to perform training on a single batch of data.\n",
    "        \n",
    "        Args:\n",
    "            batch: Batch of data to use for training/evaluation.\n",
    "            model: PyTorch model.\n",
    "            optimizer: Optimizer to use.\n",
    "            loss_func: Loss function to use.\n",
    "            perform_training: Whether to perform training (if not, evaluation is understood).\n",
    "            \n",
    "        Returns:\n",
    "            loss: Value of the loss function.\n",
    "        '''\n",
    "        if perform_training == True:\n",
    "            optimizer.zero_grad()\n",
    "        # get data from the batch\n",
    "        x, y_true = batch\n",
    "        x = x.to('cpu')\n",
    "        y_true = y_true.to('cpu')\n",
    "        # make predictions\n",
    "        y_hat = model(x).to('cpu')\n",
    "        # compute the loss function\n",
    "        loss = loss_func(y_true, y_hat)\n",
    "        if perform_training == True:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        #\n",
    "        return loss.item()\n",
    "\n",
    "    def _train(self) -> float:\n",
    "        '''\n",
    "        Function to train the TCN model on a single epoch.\n",
    "        \n",
    "        Args: None.\n",
    "            \n",
    "        Returns:\n",
    "            loss: Value of the training loss function per batch.\n",
    "        '''\n",
    "        model = self.model\n",
    "        optimizer = self.optimizer\n",
    "        loss_func = self.loss_func\n",
    "        loader = self.dataloader_train\n",
    "        #\n",
    "        model.train()\n",
    "        loss_epoch = 0\n",
    "        # iterate over batches\n",
    "        for batch in loader:\n",
    "            loss_epoch += self._model_on_batch(batch = batch, model = model, optimizer = optimizer, loss_func = loss_func,\n",
    "                                               perform_training = True)\n",
    "        #\n",
    "        return loss_epoch/len(loader)\n",
    "    \n",
    "    def _eval(self) -> float:\n",
    "        '''\n",
    "        Function to evaluate the TCN model on the validation set on a single epoch.\n",
    "        \n",
    "        Args: None.\n",
    "            \n",
    "        Returns:\n",
    "            loss: Value of the validation loss function per batch.\n",
    "        '''\n",
    "        model = self.model\n",
    "        loss_func = self.loss_func\n",
    "        loader = self.dataloader_valid\n",
    "        #\n",
    "        model.eval()\n",
    "        loss_epoch = 0\n",
    "        # iterate over batches\n",
    "        with no_grad():\n",
    "            for batch in loader:\n",
    "                loss_epoch += self._model_on_batch(batch = batch, model = model, optimizer = None, loss_func = loss_func,\n",
    "                                                   perform_training = False)\n",
    "        #\n",
    "        return loss_epoch/len(loader)\n",
    "    \n",
    "    def train_model(self) -> (torch.nn.Module, list, list):\n",
    "        '''\n",
    "        Function to train the TCN model.\n",
    "        \n",
    "        Args: None.\n",
    "            \n",
    "        Returns:\n",
    "            model: Trained TCN model.\n",
    "            list_loss_train: List of training loss function across the epochs.\n",
    "            list_loss_valid: List of validation loss function across the epochs.\n",
    "        '''\n",
    "        model = self.model\n",
    "        dict_params = self.dict_params\n",
    "        n_epochs = dict_params['training']['n_epochs']\n",
    "        patience = dict_params['training']['patience']\n",
    "        min_improve_valid_loss = dict_params['training']['min_improve_valid_loss']\n",
    "        #\n",
    "        list_loss_train, list_loss_valid = [], []\n",
    "        counter_patience = 0\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            loss_train = self._train()\n",
    "            loss_valid = self._eval()\n",
    "            # check validation loss improvement for patience\n",
    "            if (len(list_loss_valid) > 0) and (loss_valid >= np.nanmin(list_loss_valid)*(1 - min_improve_valid_loss)):\n",
    "                counter_patience += 1\n",
    "            # check validation loss w.r.t. best value\n",
    "            if (len(list_loss_valid) == 0) or (loss_valid < np.nanmin(list_loss_valid)):\n",
    "                torch.save(self.model.state_dict(), '../data/artifacts/weights.p')\n",
    "                counter_patience = 0\n",
    "            # scheduler step\n",
    "            self.scheduler.step(loss_valid)\n",
    "            #\n",
    "            print(f'Epoch {epoch}: training loss = {loss_train:.4f}, validation loss = {loss_valid:.4f}. ' +\n",
    "                  f'Learning rate = {self.optimizer.param_groups[0][\"lr\"]}. Patience = {counter_patience}')\n",
    "            #\n",
    "            list_loss_train.append(loss_train)\n",
    "            list_loss_valid.append(loss_valid)\n",
    "            # stop training with patience\n",
    "            if counter_patience >= patience:\n",
    "                print(f'Training stopped at epoch {epoch}; restoring weights from epoch {np.argmin(list_loss_valid) + 1}')\n",
    "                self.model.load_state_dict(torch.load('../data/artifacts/weights.p'))\n",
    "                break\n",
    "        #\n",
    "        return model, list_loss_train, list_loss_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb21be8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_true_y_hat(model: torch.nn.Module, x: torch.tensor, y: torch.tensor, date_y: np.array,\n",
    "                     scaler: sklearn.preprocessing.StandardScaler) -> (np.array, np.array):\n",
    "    '''\n",
    "    Function to get the real time series and its prediction.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained TCN model.\n",
    "        x: Tensor representing regressors.\n",
    "        y: Tensor representing target time series.\n",
    "        date_y: Array containing the dates corresponding to the elements of `y`.\n",
    "        scaler: Scaled used to rescale data.\n",
    "        \n",
    "    Returns:\n",
    "        y_true: Array containing the true values.\n",
    "        y_hat: Array containing the predicted values.\n",
    "    '''\n",
    "    list_date = []\n",
    "    y_true = []\n",
    "    y_hat = []\n",
    "    preds = model(x)\n",
    "    for i in range(np.unique(date_y).shape[0]):\n",
    "        date = np.unique(date_y)[i]\n",
    "        list_date.append(date)\n",
    "        idx = np.where(date_y == date)\n",
    "        y_true.append(y.numpy()[idx].mean())\n",
    "        y_hat.append(preds.detach().numpy()[idx].mean())\n",
    "    y_true = np.array(y_true)\n",
    "    y_hat = np.array(y_hat)\n",
    "    # scale back\n",
    "    y_true = scaler.inverse_transform(y_true.reshape(-1, 1)).ravel()\n",
    "    y_hat = scaler.inverse_transform(y_hat.reshape(-1, 1)).ravel()\n",
    "    #\n",
    "    return y_true, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "68c49dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mape(y_true: np.array, y_hat: np.array) -> float:\n",
    "    '''\n",
    "    Function to compute the MAPE.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Array containing the true values.\n",
    "        y_hat: Array containing the predicted values.\n",
    "        \n",
    "    Returns:\n",
    "        mape: MAPE computed from `y_true` and `y_hat`.\n",
    "    '''\n",
    "    mape = np.mean(abs(y_true[y_true > 0] - y_hat[y_true > 0])/y_true[y_true > 0])\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "12ac013d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training loss = 0.7887, validation loss = 1.2967. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.4731, validation loss = 0.8211. Learning rate = 0.001. Patience = 0\n",
      "Epoch 3: training loss = 0.4235, validation loss = 0.7362. Learning rate = 0.001. Patience = 0\n",
      "Epoch 4: training loss = 0.3863, validation loss = 0.7240. Learning rate = 0.001. Patience = 0\n",
      "Epoch 5: training loss = 0.3640, validation loss = 0.7037. Learning rate = 0.001. Patience = 0\n",
      "Epoch 6: training loss = 0.3614, validation loss = 0.7114. Learning rate = 0.001. Patience = 1\n",
      "Epoch 7: training loss = 0.3627, validation loss = 0.6925. Learning rate = 0.001. Patience = 0\n",
      "Epoch 8: training loss = 0.3566, validation loss = 0.6766. Learning rate = 0.001. Patience = 0\n",
      "Epoch 9: training loss = 0.3381, validation loss = 0.7363. Learning rate = 0.001. Patience = 1\n",
      "Epoch 10: training loss = 0.3401, validation loss = 0.6844. Learning rate = 0.001. Patience = 2\n",
      "Epoch 11: training loss = 0.3237, validation loss = 0.6723. Learning rate = 0.001. Patience = 0\n",
      "Epoch 12: training loss = 0.3317, validation loss = 0.6923. Learning rate = 0.001. Patience = 1\n",
      "Epoch 13: training loss = 0.3397, validation loss = 0.6647. Learning rate = 0.001. Patience = 0\n",
      "Epoch 14: training loss = 0.3299, validation loss = 0.6700. Learning rate = 0.001. Patience = 1\n",
      "Epoch 15: training loss = 0.3248, validation loss = 0.6498. Learning rate = 0.001. Patience = 0\n",
      "Epoch 16: training loss = 0.3187, validation loss = 0.6564. Learning rate = 0.001. Patience = 1\n",
      "Epoch 17: training loss = 0.3115, validation loss = 0.6899. Learning rate = 0.001. Patience = 2\n",
      "Epoch 18: training loss = 0.3090, validation loss = 0.6518. Learning rate = 0.001. Patience = 3\n",
      "Epoch 19: training loss = 0.3150, validation loss = 0.6217. Learning rate = 0.001. Patience = 0\n",
      "Epoch 20: training loss = 0.3139, validation loss = 0.6679. Learning rate = 0.001. Patience = 1\n",
      "Epoch 21: training loss = 0.3030, validation loss = 0.6366. Learning rate = 0.001. Patience = 2\n",
      "Epoch 22: training loss = 0.3098, validation loss = 0.6345. Learning rate = 0.001. Patience = 3\n",
      "Epoch 23: training loss = 0.2989, validation loss = 0.6165. Learning rate = 0.001. Patience = 0\n",
      "Epoch 24: training loss = 0.3055, validation loss = 0.6488. Learning rate = 0.001. Patience = 1\n",
      "Epoch 25: training loss = 0.3031, validation loss = 0.6476. Learning rate = 0.001. Patience = 2\n",
      "Epoch 26: training loss = 0.2909, validation loss = 0.6308. Learning rate = 0.001. Patience = 3\n",
      "Epoch 27: training loss = 0.2950, validation loss = 0.6290. Learning rate = 0.001. Patience = 4\n",
      "Epoch 28: training loss = 0.2977, validation loss = 0.6485. Learning rate = 0.001. Patience = 5\n",
      "Epoch 29: training loss = 0.2936, validation loss = 0.6411. Learning rate = 0.001. Patience = 6\n",
      "Epoch 30: training loss = 0.2968, validation loss = 0.6277. Learning rate = 0.001. Patience = 7\n",
      "Epoch 31: training loss = 0.2819, validation loss = 0.6269. Learning rate = 0.001. Patience = 8\n",
      "Epoch 32: training loss = 0.2847, validation loss = 0.6264. Learning rate = 0.001. Patience = 9\n",
      "Epoch 33: training loss = 0.2784, validation loss = 0.6635. Learning rate = 0.001. Patience = 10\n",
      "Epoch 34: training loss = 0.2748, validation loss = 0.6319. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 35: training loss = 0.2677, validation loss = 0.6286. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 36: training loss = 0.2800, validation loss = 0.6353. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 37: training loss = 0.2757, validation loss = 0.6285. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 38: training loss = 0.2772, validation loss = 0.6314. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 39: training loss = 0.2695, validation loss = 0.6393. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 40: training loss = 0.2634, validation loss = 0.6364. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 41: training loss = 0.2806, validation loss = 0.6345. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 42: training loss = 0.2728, validation loss = 0.6379. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 43: training loss = 0.2703, validation loss = 0.6302. Learning rate = 0.0005. Patience = 20\n",
      "Epoch 44: training loss = 0.2693, validation loss = 0.6481. Learning rate = 0.0005. Patience = 21\n",
      "Epoch 45: training loss = 0.2761, validation loss = 0.6509. Learning rate = 0.00025. Patience = 22\n",
      "Epoch 46: training loss = 0.2646, validation loss = 0.6335. Learning rate = 0.00025. Patience = 23\n",
      "Epoch 47: training loss = 0.2654, validation loss = 0.6432. Learning rate = 0.00025. Patience = 24\n",
      "Epoch 48: training loss = 0.2545, validation loss = 0.6418. Learning rate = 0.00025. Patience = 25\n",
      "Epoch 49: training loss = 0.2622, validation loss = 0.6374. Learning rate = 0.00025. Patience = 26\n",
      "Epoch 50: training loss = 0.2674, validation loss = 0.6494. Learning rate = 0.00025. Patience = 27\n",
      "Epoch 51: training loss = 0.2536, validation loss = 0.6547. Learning rate = 0.00025. Patience = 28\n",
      "Epoch 52: training loss = 0.2595, validation loss = 0.6526. Learning rate = 0.00025. Patience = 29\n",
      "Epoch 53: training loss = 0.2574, validation loss = 0.6504. Learning rate = 0.00025. Patience = 30\n",
      "Epoch 54: training loss = 0.2566, validation loss = 0.6608. Learning rate = 0.00025. Patience = 31\n",
      "Epoch 55: training loss = 0.2554, validation loss = 0.6641. Learning rate = 0.00025. Patience = 32\n",
      "Epoch 56: training loss = 0.2579, validation loss = 0.6544. Learning rate = 0.000125. Patience = 33\n",
      "Epoch 57: training loss = 0.2607, validation loss = 0.6578. Learning rate = 0.000125. Patience = 34\n",
      "Epoch 58: training loss = 0.2628, validation loss = 0.6569. Learning rate = 0.000125. Patience = 35\n",
      "Epoch 59: training loss = 0.2555, validation loss = 0.6522. Learning rate = 0.000125. Patience = 36\n",
      "Epoch 60: training loss = 0.2719, validation loss = 0.6784. Learning rate = 0.000125. Patience = 37\n",
      "Epoch 61: training loss = 0.2499, validation loss = 0.6560. Learning rate = 0.000125. Patience = 38\n",
      "Epoch 62: training loss = 0.2597, validation loss = 0.6553. Learning rate = 0.000125. Patience = 39\n",
      "Epoch 63: training loss = 0.2526, validation loss = 0.6531. Learning rate = 0.000125. Patience = 40\n",
      "Epoch 64: training loss = 0.2438, validation loss = 0.6542. Learning rate = 0.000125. Patience = 41\n",
      "Epoch 65: training loss = 0.2510, validation loss = 0.6619. Learning rate = 0.000125. Patience = 42\n",
      "Epoch 66: training loss = 0.2608, validation loss = 0.6601. Learning rate = 0.000125. Patience = 43\n",
      "Epoch 67: training loss = 0.2746, validation loss = 0.6561. Learning rate = 6.25e-05. Patience = 44\n",
      "Epoch 68: training loss = 0.2537, validation loss = 0.6598. Learning rate = 6.25e-05. Patience = 45\n",
      "Epoch 69: training loss = 0.2533, validation loss = 0.6583. Learning rate = 6.25e-05. Patience = 46\n",
      "Epoch 70: training loss = 0.2458, validation loss = 0.6549. Learning rate = 6.25e-05. Patience = 47\n",
      "Epoch 71: training loss = 0.2573, validation loss = 0.6544. Learning rate = 6.25e-05. Patience = 48\n",
      "Epoch 72: training loss = 0.2566, validation loss = 0.6549. Learning rate = 6.25e-05. Patience = 49\n",
      "Epoch 73: training loss = 0.2480, validation loss = 0.6591. Learning rate = 6.25e-05. Patience = 50\n",
      "Training stopped at epoch 73; restoring weights from epoch 23\n",
      "Epoch 1: training loss = 1.6694, validation loss = 3.7295. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 1.4283, validation loss = 4.1552. Learning rate = 0.001. Patience = 1\n",
      "Epoch 3: training loss = 1.4348, validation loss = 4.9291. Learning rate = 0.001. Patience = 2\n",
      "Epoch 4: training loss = 1.3248, validation loss = 3.9431. Learning rate = 0.001. Patience = 3\n",
      "Epoch 5: training loss = 1.3169, validation loss = 3.9542. Learning rate = 0.001. Patience = 4\n",
      "Epoch 6: training loss = 1.3386, validation loss = 3.9267. Learning rate = 0.001. Patience = 5\n",
      "Epoch 7: training loss = 1.3441, validation loss = 3.9322. Learning rate = 0.001. Patience = 6\n",
      "Epoch 8: training loss = 1.3591, validation loss = 3.9633. Learning rate = 0.001. Patience = 7\n",
      "Epoch 9: training loss = 1.3260, validation loss = 4.0625. Learning rate = 0.001. Patience = 8\n",
      "Epoch 10: training loss = 1.2818, validation loss = 4.1253. Learning rate = 0.001. Patience = 9\n",
      "Epoch 11: training loss = 1.2826, validation loss = 4.4457. Learning rate = 0.001. Patience = 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: training loss = 2.0981, validation loss = 4.3449. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 13: training loss = 1.2506, validation loss = 4.9185. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 14: training loss = 1.2310, validation loss = 4.5799. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 15: training loss = 1.2450, validation loss = 4.6095. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 16: training loss = 2.0206, validation loss = 4.9123. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 17: training loss = 1.2217, validation loss = 5.8826. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 18: training loss = 1.2321, validation loss = 5.1921. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 19: training loss = 1.2546, validation loss = 5.1762. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 20: training loss = 1.1765, validation loss = 5.2705. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 21: training loss = 1.1720, validation loss = 5.6132. Learning rate = 0.0005. Patience = 20\n",
      "Epoch 22: training loss = 1.1997, validation loss = 5.6392. Learning rate = 0.0005. Patience = 21\n",
      "Epoch 23: training loss = 1.1469, validation loss = 5.6453. Learning rate = 0.00025. Patience = 22\n",
      "Epoch 24: training loss = 1.1605, validation loss = 5.5706. Learning rate = 0.00025. Patience = 23\n",
      "Epoch 25: training loss = 1.1640, validation loss = 5.6454. Learning rate = 0.00025. Patience = 24\n",
      "Epoch 26: training loss = 1.1184, validation loss = 5.7027. Learning rate = 0.00025. Patience = 25\n",
      "Epoch 27: training loss = 1.2127, validation loss = 5.8279. Learning rate = 0.00025. Patience = 26\n",
      "Epoch 28: training loss = 1.1243, validation loss = 5.8970. Learning rate = 0.00025. Patience = 27\n",
      "Epoch 29: training loss = 1.1494, validation loss = 6.0518. Learning rate = 0.00025. Patience = 28\n",
      "Epoch 30: training loss = 1.1529, validation loss = 5.9230. Learning rate = 0.00025. Patience = 29\n",
      "Epoch 31: training loss = 1.1176, validation loss = 6.1135. Learning rate = 0.00025. Patience = 30\n",
      "Epoch 32: training loss = 1.1165, validation loss = 6.1112. Learning rate = 0.00025. Patience = 31\n",
      "Epoch 33: training loss = 1.1314, validation loss = 6.1747. Learning rate = 0.00025. Patience = 32\n",
      "Epoch 34: training loss = 1.1114, validation loss = 6.3772. Learning rate = 0.000125. Patience = 33\n",
      "Epoch 35: training loss = 1.0828, validation loss = 6.4772. Learning rate = 0.000125. Patience = 34\n",
      "Epoch 36: training loss = 1.0705, validation loss = 6.6271. Learning rate = 0.000125. Patience = 35\n",
      "Epoch 37: training loss = 1.0842, validation loss = 6.4874. Learning rate = 0.000125. Patience = 36\n",
      "Epoch 38: training loss = 1.0934, validation loss = 6.5687. Learning rate = 0.000125. Patience = 37\n",
      "Epoch 39: training loss = 1.1386, validation loss = 6.5213. Learning rate = 0.000125. Patience = 38\n",
      "Epoch 40: training loss = 1.1500, validation loss = 6.4765. Learning rate = 0.000125. Patience = 39\n",
      "Epoch 41: training loss = 1.0878, validation loss = 6.6379. Learning rate = 0.000125. Patience = 40\n",
      "Epoch 42: training loss = 1.7650, validation loss = 6.7712. Learning rate = 0.000125. Patience = 41\n",
      "Epoch 43: training loss = 1.0724, validation loss = 7.4315. Learning rate = 0.000125. Patience = 42\n",
      "Epoch 44: training loss = 1.0784, validation loss = 7.4481. Learning rate = 0.000125. Patience = 43\n",
      "Epoch 45: training loss = 1.0526, validation loss = 7.1090. Learning rate = 6.25e-05. Patience = 44\n",
      "Epoch 46: training loss = 1.6557, validation loss = 7.0930. Learning rate = 6.25e-05. Patience = 45\n",
      "Epoch 47: training loss = 1.0675, validation loss = 7.3731. Learning rate = 6.25e-05. Patience = 46\n",
      "Epoch 48: training loss = 1.1145, validation loss = 7.4383. Learning rate = 6.25e-05. Patience = 47\n",
      "Epoch 49: training loss = 1.0831, validation loss = 7.4299. Learning rate = 6.25e-05. Patience = 48\n",
      "Epoch 50: training loss = 1.1324, validation loss = 7.4218. Learning rate = 6.25e-05. Patience = 49\n",
      "Epoch 51: training loss = 1.0920, validation loss = 7.4594. Learning rate = 6.25e-05. Patience = 50\n",
      "Training stopped at epoch 51; restoring weights from epoch 1\n",
      "Epoch 1: training loss = 0.8583, validation loss = 4.7469. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.6074, validation loss = 2.7327. Learning rate = 0.001. Patience = 0\n",
      "Epoch 3: training loss = 0.4691, validation loss = 1.6608. Learning rate = 0.001. Patience = 0\n",
      "Epoch 4: training loss = 0.4259, validation loss = 1.9034. Learning rate = 0.001. Patience = 1\n",
      "Epoch 5: training loss = 0.4105, validation loss = 2.2748. Learning rate = 0.001. Patience = 2\n",
      "Epoch 6: training loss = 0.3951, validation loss = 1.8784. Learning rate = 0.001. Patience = 3\n",
      "Epoch 7: training loss = 0.3905, validation loss = 1.9001. Learning rate = 0.001. Patience = 4\n",
      "Epoch 8: training loss = 0.3956, validation loss = 1.6413. Learning rate = 0.001. Patience = 0\n",
      "Epoch 9: training loss = 0.3873, validation loss = 1.9351. Learning rate = 0.001. Patience = 1\n",
      "Epoch 10: training loss = 0.3821, validation loss = 1.7035. Learning rate = 0.001. Patience = 2\n",
      "Epoch 11: training loss = 0.3889, validation loss = 2.1928. Learning rate = 0.001. Patience = 3\n",
      "Epoch 12: training loss = 0.3692, validation loss = 1.8436. Learning rate = 0.001. Patience = 4\n",
      "Epoch 13: training loss = 0.3713, validation loss = 2.0643. Learning rate = 0.001. Patience = 5\n",
      "Epoch 14: training loss = 0.3544, validation loss = 1.8650. Learning rate = 0.001. Patience = 6\n",
      "Epoch 15: training loss = 0.3455, validation loss = 2.2426. Learning rate = 0.001. Patience = 7\n",
      "Epoch 16: training loss = 0.3650, validation loss = 2.1372. Learning rate = 0.001. Patience = 8\n",
      "Epoch 17: training loss = 0.3597, validation loss = 2.5610. Learning rate = 0.001. Patience = 9\n",
      "Epoch 18: training loss = 0.3470, validation loss = 1.7755. Learning rate = 0.001. Patience = 10\n",
      "Epoch 19: training loss = 0.3520, validation loss = 2.5939. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 20: training loss = 0.3484, validation loss = 2.1019. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 21: training loss = 0.3727, validation loss = 2.2956. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 22: training loss = 0.3281, validation loss = 1.9982. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 23: training loss = 0.3478, validation loss = 2.2431. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 24: training loss = 0.3309, validation loss = 2.3784. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 25: training loss = 0.3359, validation loss = 2.2592. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 26: training loss = 0.3333, validation loss = 2.2003. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 27: training loss = 0.3354, validation loss = 2.2811. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 28: training loss = 0.3204, validation loss = 2.4502. Learning rate = 0.0005. Patience = 20\n",
      "Epoch 29: training loss = 0.3402, validation loss = 2.3826. Learning rate = 0.0005. Patience = 21\n",
      "Epoch 30: training loss = 0.3331, validation loss = 2.3708. Learning rate = 0.00025. Patience = 22\n",
      "Epoch 31: training loss = 0.3428, validation loss = 2.4589. Learning rate = 0.00025. Patience = 23\n",
      "Epoch 32: training loss = 0.3314, validation loss = 2.3582. Learning rate = 0.00025. Patience = 24\n",
      "Epoch 33: training loss = 0.3275, validation loss = 2.5735. Learning rate = 0.00025. Patience = 25\n",
      "Epoch 34: training loss = 0.3340, validation loss = 2.6040. Learning rate = 0.00025. Patience = 26\n",
      "Epoch 35: training loss = 0.3185, validation loss = 2.4073. Learning rate = 0.00025. Patience = 27\n",
      "Epoch 36: training loss = 0.3280, validation loss = 2.3676. Learning rate = 0.00025. Patience = 28\n",
      "Epoch 37: training loss = 0.3169, validation loss = 2.4015. Learning rate = 0.00025. Patience = 29\n",
      "Epoch 38: training loss = 0.3149, validation loss = 2.4276. Learning rate = 0.00025. Patience = 30\n",
      "Epoch 39: training loss = 0.3157, validation loss = 2.3260. Learning rate = 0.00025. Patience = 31\n",
      "Epoch 40: training loss = 0.3422, validation loss = 2.3212. Learning rate = 0.00025. Patience = 32\n",
      "Epoch 41: training loss = 0.3118, validation loss = 2.5635. Learning rate = 0.000125. Patience = 33\n",
      "Epoch 42: training loss = 0.3253, validation loss = 2.6758. Learning rate = 0.000125. Patience = 34\n",
      "Epoch 43: training loss = 0.3079, validation loss = 2.5131. Learning rate = 0.000125. Patience = 35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: training loss = 0.3321, validation loss = 2.4544. Learning rate = 0.000125. Patience = 36\n",
      "Epoch 45: training loss = 0.3246, validation loss = 2.3790. Learning rate = 0.000125. Patience = 37\n",
      "Epoch 46: training loss = 0.3184, validation loss = 2.4105. Learning rate = 0.000125. Patience = 38\n",
      "Epoch 47: training loss = 0.3094, validation loss = 2.4041. Learning rate = 0.000125. Patience = 39\n",
      "Epoch 48: training loss = 0.3147, validation loss = 2.4735. Learning rate = 0.000125. Patience = 40\n",
      "Epoch 49: training loss = 0.3149, validation loss = 2.4181. Learning rate = 0.000125. Patience = 41\n",
      "Epoch 50: training loss = 0.3055, validation loss = 2.4266. Learning rate = 0.000125. Patience = 42\n",
      "Epoch 51: training loss = 0.3196, validation loss = 2.5606. Learning rate = 0.000125. Patience = 43\n",
      "Epoch 52: training loss = 0.3063, validation loss = 2.5111. Learning rate = 6.25e-05. Patience = 44\n",
      "Epoch 53: training loss = 0.3114, validation loss = 2.4902. Learning rate = 6.25e-05. Patience = 45\n",
      "Epoch 54: training loss = 0.3078, validation loss = 2.4920. Learning rate = 6.25e-05. Patience = 46\n",
      "Epoch 55: training loss = 0.3267, validation loss = 2.4406. Learning rate = 6.25e-05. Patience = 47\n",
      "Epoch 56: training loss = 0.3264, validation loss = 2.4883. Learning rate = 6.25e-05. Patience = 48\n",
      "Epoch 57: training loss = 0.3127, validation loss = 2.4687. Learning rate = 6.25e-05. Patience = 49\n",
      "Epoch 58: training loss = 0.3244, validation loss = 2.4769. Learning rate = 6.25e-05. Patience = 50\n",
      "Training stopped at epoch 58; restoring weights from epoch 8\n",
      "Epoch 1: training loss = 0.8236, validation loss = 0.9139. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.4699, validation loss = 0.6526. Learning rate = 0.001. Patience = 0\n",
      "Epoch 3: training loss = 0.3277, validation loss = 0.5478. Learning rate = 0.001. Patience = 0\n",
      "Epoch 4: training loss = 0.3101, validation loss = 0.4861. Learning rate = 0.001. Patience = 0\n",
      "Epoch 5: training loss = 0.2716, validation loss = 0.4730. Learning rate = 0.001. Patience = 0\n",
      "Epoch 6: training loss = 0.2878, validation loss = 0.4218. Learning rate = 0.001. Patience = 0\n",
      "Epoch 7: training loss = 0.2753, validation loss = 0.4653. Learning rate = 0.001. Patience = 1\n",
      "Epoch 8: training loss = 0.2497, validation loss = 0.4178. Learning rate = 0.001. Patience = 0\n",
      "Epoch 9: training loss = 0.2589, validation loss = 0.4139. Learning rate = 0.001. Patience = 0\n",
      "Epoch 10: training loss = 0.2440, validation loss = 0.4483. Learning rate = 0.001. Patience = 1\n",
      "Epoch 11: training loss = 0.2401, validation loss = 0.4028. Learning rate = 0.001. Patience = 0\n",
      "Epoch 12: training loss = 0.2643, validation loss = 0.4477. Learning rate = 0.001. Patience = 1\n",
      "Epoch 13: training loss = 0.2413, validation loss = 0.4245. Learning rate = 0.001. Patience = 2\n",
      "Epoch 14: training loss = 0.2597, validation loss = 0.4264. Learning rate = 0.001. Patience = 3\n",
      "Epoch 15: training loss = 0.2447, validation loss = 0.4444. Learning rate = 0.001. Patience = 4\n",
      "Epoch 16: training loss = 0.2455, validation loss = 0.4116. Learning rate = 0.001. Patience = 5\n",
      "Epoch 17: training loss = 0.2446, validation loss = 0.4225. Learning rate = 0.001. Patience = 6\n",
      "Epoch 18: training loss = 0.2506, validation loss = 0.4083. Learning rate = 0.001. Patience = 7\n",
      "Epoch 19: training loss = 0.2509, validation loss = 0.4393. Learning rate = 0.001. Patience = 8\n",
      "Epoch 20: training loss = 0.2442, validation loss = 0.4028. Learning rate = 0.001. Patience = 0\n",
      "Epoch 21: training loss = 0.2220, validation loss = 0.4039. Learning rate = 0.001. Patience = 1\n",
      "Epoch 22: training loss = 0.2295, validation loss = 0.4774. Learning rate = 0.001. Patience = 2\n",
      "Epoch 23: training loss = 0.2514, validation loss = 0.3927. Learning rate = 0.001. Patience = 0\n",
      "Epoch 24: training loss = 0.2214, validation loss = 0.5196. Learning rate = 0.001. Patience = 1\n",
      "Epoch 25: training loss = 0.2414, validation loss = 0.3926. Learning rate = 0.001. Patience = 0\n",
      "Epoch 26: training loss = 0.2206, validation loss = 0.4696. Learning rate = 0.001. Patience = 1\n",
      "Epoch 27: training loss = 0.2264, validation loss = 0.4004. Learning rate = 0.001. Patience = 2\n",
      "Epoch 28: training loss = 0.2321, validation loss = 0.4346. Learning rate = 0.001. Patience = 3\n",
      "Epoch 29: training loss = 0.2383, validation loss = 0.4314. Learning rate = 0.001. Patience = 4\n",
      "Epoch 30: training loss = 0.2317, validation loss = 0.4463. Learning rate = 0.001. Patience = 5\n",
      "Epoch 31: training loss = 0.2193, validation loss = 0.3761. Learning rate = 0.001. Patience = 0\n",
      "Epoch 32: training loss = 0.2512, validation loss = 0.4286. Learning rate = 0.001. Patience = 1\n",
      "Epoch 33: training loss = 0.2100, validation loss = 0.4410. Learning rate = 0.001. Patience = 2\n",
      "Epoch 34: training loss = 0.2060, validation loss = 0.4093. Learning rate = 0.001. Patience = 3\n",
      "Epoch 35: training loss = 0.2126, validation loss = 0.3845. Learning rate = 0.001. Patience = 4\n",
      "Epoch 36: training loss = 0.2360, validation loss = 0.4683. Learning rate = 0.001. Patience = 5\n",
      "Epoch 37: training loss = 0.2302, validation loss = 0.4093. Learning rate = 0.001. Patience = 6\n",
      "Epoch 38: training loss = 0.2029, validation loss = 0.4463. Learning rate = 0.001. Patience = 7\n",
      "Epoch 39: training loss = 0.2014, validation loss = 0.4454. Learning rate = 0.001. Patience = 8\n",
      "Epoch 40: training loss = 0.2210, validation loss = 0.3800. Learning rate = 0.001. Patience = 9\n",
      "Epoch 41: training loss = 0.1995, validation loss = 0.4787. Learning rate = 0.001. Patience = 10\n",
      "Epoch 42: training loss = 0.2347, validation loss = 0.4384. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 43: training loss = 0.1997, validation loss = 0.4629. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 44: training loss = 0.2190, validation loss = 0.4051. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 45: training loss = 0.2015, validation loss = 0.4464. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 46: training loss = 0.2070, validation loss = 0.4438. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 47: training loss = 0.1992, validation loss = 0.4471. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 48: training loss = 0.2010, validation loss = 0.4289. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 49: training loss = 0.2002, validation loss = 0.4413. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 50: training loss = 0.2084, validation loss = 0.4564. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 51: training loss = 0.2106, validation loss = 0.4288. Learning rate = 0.0005. Patience = 20\n",
      "Epoch 52: training loss = 0.2093, validation loss = 0.4483. Learning rate = 0.0005. Patience = 21\n",
      "Epoch 53: training loss = 0.1923, validation loss = 0.4358. Learning rate = 0.00025. Patience = 22\n",
      "Epoch 54: training loss = 0.1979, validation loss = 0.4324. Learning rate = 0.00025. Patience = 23\n",
      "Epoch 55: training loss = 0.2030, validation loss = 0.4322. Learning rate = 0.00025. Patience = 24\n",
      "Epoch 56: training loss = 0.2086, validation loss = 0.4431. Learning rate = 0.00025. Patience = 25\n",
      "Epoch 57: training loss = 0.1923, validation loss = 0.4329. Learning rate = 0.00025. Patience = 26\n",
      "Epoch 58: training loss = 0.2032, validation loss = 0.4151. Learning rate = 0.00025. Patience = 27\n",
      "Epoch 59: training loss = 0.2040, validation loss = 0.4168. Learning rate = 0.00025. Patience = 28\n",
      "Epoch 60: training loss = 0.2004, validation loss = 0.4291. Learning rate = 0.00025. Patience = 29\n",
      "Epoch 61: training loss = 0.1881, validation loss = 0.4233. Learning rate = 0.00025. Patience = 30\n",
      "Epoch 62: training loss = 0.1975, validation loss = 0.4287. Learning rate = 0.00025. Patience = 31\n",
      "Epoch 63: training loss = 0.2020, validation loss = 0.4453. Learning rate = 0.00025. Patience = 32\n",
      "Epoch 64: training loss = 0.1939, validation loss = 0.4456. Learning rate = 0.000125. Patience = 33\n",
      "Epoch 65: training loss = 0.1951, validation loss = 0.4262. Learning rate = 0.000125. Patience = 34\n",
      "Epoch 66: training loss = 0.2027, validation loss = 0.4328. Learning rate = 0.000125. Patience = 35\n",
      "Epoch 67: training loss = 0.1892, validation loss = 0.4167. Learning rate = 0.000125. Patience = 36\n",
      "Epoch 68: training loss = 0.1903, validation loss = 0.4346. Learning rate = 0.000125. Patience = 37\n",
      "Epoch 69: training loss = 0.1956, validation loss = 0.4325. Learning rate = 0.000125. Patience = 38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70: training loss = 0.1934, validation loss = 0.4303. Learning rate = 0.000125. Patience = 39\n",
      "Epoch 71: training loss = 0.1978, validation loss = 0.4320. Learning rate = 0.000125. Patience = 40\n",
      "Epoch 72: training loss = 0.2000, validation loss = 0.4345. Learning rate = 0.000125. Patience = 41\n",
      "Epoch 73: training loss = 0.1868, validation loss = 0.4285. Learning rate = 0.000125. Patience = 42\n",
      "Epoch 74: training loss = 0.1999, validation loss = 0.4382. Learning rate = 0.000125. Patience = 43\n",
      "Epoch 75: training loss = 0.1912, validation loss = 0.4523. Learning rate = 6.25e-05. Patience = 44\n",
      "Epoch 76: training loss = 0.2046, validation loss = 0.4478. Learning rate = 6.25e-05. Patience = 45\n",
      "Epoch 77: training loss = 0.1925, validation loss = 0.4349. Learning rate = 6.25e-05. Patience = 46\n",
      "Epoch 78: training loss = 0.2043, validation loss = 0.4317. Learning rate = 6.25e-05. Patience = 47\n",
      "Epoch 79: training loss = 0.2197, validation loss = 0.4368. Learning rate = 6.25e-05. Patience = 48\n",
      "Epoch 80: training loss = 0.1845, validation loss = 0.4255. Learning rate = 6.25e-05. Patience = 49\n",
      "Epoch 81: training loss = 0.1958, validation loss = 0.4280. Learning rate = 6.25e-05. Patience = 50\n",
      "Training stopped at epoch 81; restoring weights from epoch 31\n",
      "Epoch 1: training loss = 0.0178, validation loss = 0.0001. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.0037, validation loss = 0.0010. Learning rate = 0.001. Patience = 1\n",
      "Epoch 3: training loss = 0.0013, validation loss = 0.0010. Learning rate = 0.001. Patience = 2\n",
      "Epoch 4: training loss = 0.0007, validation loss = 0.0005. Learning rate = 0.001. Patience = 3\n",
      "Epoch 5: training loss = 0.0005, validation loss = 0.0001. Learning rate = 0.001. Patience = 4\n",
      "Epoch 6: training loss = 0.0004, validation loss = 0.0000. Learning rate = 0.001. Patience = 0\n",
      "Epoch 7: training loss = 0.0003, validation loss = 0.0000. Learning rate = 0.001. Patience = 0\n",
      "Epoch 8: training loss = 0.0002, validation loss = 0.0000. Learning rate = 0.001. Patience = 1\n",
      "Epoch 9: training loss = 0.0002, validation loss = 0.0000. Learning rate = 0.001. Patience = 2\n",
      "Epoch 10: training loss = 0.0002, validation loss = 0.0000. Learning rate = 0.001. Patience = 0\n",
      "Epoch 11: training loss = 0.0002, validation loss = 0.0000. Learning rate = 0.001. Patience = 1\n",
      "Epoch 12: training loss = 0.0002, validation loss = 0.0000. Learning rate = 0.001. Patience = 2\n",
      "Epoch 13: training loss = 0.0002, validation loss = 0.0000. Learning rate = 0.001. Patience = 0\n",
      "Epoch 14: training loss = 0.0002, validation loss = 0.0000. Learning rate = 0.001. Patience = 1\n",
      "Epoch 15: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.001. Patience = 2\n",
      "Epoch 16: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.001. Patience = 3\n",
      "Epoch 17: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.001. Patience = 4\n",
      "Epoch 18: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.001. Patience = 5\n",
      "Epoch 19: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.001. Patience = 0\n",
      "Epoch 20: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.001. Patience = 1\n",
      "Epoch 21: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.001. Patience = 2\n",
      "Epoch 22: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.001. Patience = 3\n",
      "Epoch 23: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.001. Patience = 4\n",
      "Epoch 24: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.001. Patience = 5\n",
      "Epoch 25: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.001. Patience = 6\n",
      "Epoch 26: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.001. Patience = 0\n",
      "Epoch 27: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.001. Patience = 1\n",
      "Epoch 28: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.001. Patience = 2\n",
      "Epoch 29: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.001. Patience = 3\n",
      "Epoch 30: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.001. Patience = 4\n",
      "Epoch 31: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.001. Patience = 5\n",
      "Epoch 32: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.001. Patience = 6\n",
      "Epoch 33: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.001. Patience = 7\n",
      "Epoch 34: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.001. Patience = 8\n",
      "Epoch 35: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.001. Patience = 9\n",
      "Epoch 36: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.001. Patience = 10\n",
      "Epoch 37: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 38: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 39: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 40: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 41: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 42: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 43: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 44: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 45: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 46: training loss = 0.0001, validation loss = 0.0000. Learning rate = 0.0005. Patience = 20\n",
      "Epoch 47: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.0005. Patience = 0\n",
      "Epoch 48: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.0005. Patience = 1\n",
      "Epoch 49: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.0005. Patience = 2\n",
      "Epoch 50: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.0005. Patience = 3\n",
      "Epoch 51: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.0005. Patience = 4\n",
      "Epoch 52: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.0005. Patience = 5\n",
      "Epoch 53: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.0005. Patience = 6\n",
      "Epoch 54: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.0005. Patience = 7\n",
      "Epoch 55: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.0005. Patience = 0\n",
      "Epoch 56: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.0005. Patience = 1\n",
      "Epoch 57: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.0005. Patience = 2\n",
      "Epoch 58: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.0005. Patience = 3\n",
      "Epoch 59: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.0005. Patience = 4\n",
      "Epoch 60: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.0005. Patience = 5\n",
      "Epoch 61: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.0005. Patience = 6\n",
      "Epoch 62: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.0005. Patience = 7\n",
      "Epoch 63: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.0005. Patience = 8\n",
      "Epoch 64: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.0005. Patience = 9\n",
      "Epoch 65: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.0005. Patience = 10\n",
      "Epoch 66: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.00025. Patience = 11\n",
      "Epoch 67: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.00025. Patience = 12\n",
      "Epoch 68: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.00025. Patience = 13\n",
      "Epoch 69: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.00025. Patience = 14\n",
      "Epoch 70: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.00025. Patience = 15\n",
      "Epoch 71: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.00025. Patience = 16\n",
      "Epoch 72: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.00025. Patience = 17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.00025. Patience = 18\n",
      "Epoch 74: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.00025. Patience = 19\n",
      "Epoch 75: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.00025. Patience = 20\n",
      "Epoch 76: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.00025. Patience = 21\n",
      "Epoch 77: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.000125. Patience = 22\n",
      "Epoch 78: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.000125. Patience = 23\n",
      "Epoch 79: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.000125. Patience = 24\n",
      "Epoch 80: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.000125. Patience = 25\n",
      "Epoch 81: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.000125. Patience = 26\n",
      "Epoch 82: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.000125. Patience = 27\n",
      "Epoch 83: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.000125. Patience = 28\n",
      "Epoch 84: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.000125. Patience = 29\n",
      "Epoch 85: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.000125. Patience = 30\n",
      "Epoch 86: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.000125. Patience = 31\n",
      "Epoch 87: training loss = 0.0000, validation loss = 0.0000. Learning rate = 0.000125. Patience = 32\n",
      "Epoch 88: training loss = 0.0000, validation loss = 0.0000. Learning rate = 6.25e-05. Patience = 33\n",
      "Epoch 89: training loss = 0.0000, validation loss = 0.0000. Learning rate = 6.25e-05. Patience = 34\n",
      "Epoch 90: training loss = 0.0000, validation loss = 0.0000. Learning rate = 6.25e-05. Patience = 35\n",
      "Epoch 91: training loss = 0.0000, validation loss = 0.0000. Learning rate = 6.25e-05. Patience = 36\n",
      "Epoch 92: training loss = 0.0000, validation loss = 0.0000. Learning rate = 6.25e-05. Patience = 37\n",
      "Epoch 93: training loss = 0.0000, validation loss = 0.0000. Learning rate = 6.25e-05. Patience = 38\n",
      "Epoch 94: training loss = 0.0000, validation loss = 0.0000. Learning rate = 6.25e-05. Patience = 39\n",
      "Epoch 95: training loss = 0.0000, validation loss = 0.0000. Learning rate = 6.25e-05. Patience = 40\n",
      "Epoch 96: training loss = 0.0000, validation loss = 0.0000. Learning rate = 6.25e-05. Patience = 41\n",
      "Epoch 97: training loss = 0.0000, validation loss = 0.0000. Learning rate = 6.25e-05. Patience = 42\n",
      "Epoch 98: training loss = 0.0000, validation loss = 0.0000. Learning rate = 6.25e-05. Patience = 43\n",
      "Epoch 99: training loss = 0.0000, validation loss = 0.0000. Learning rate = 3.125e-05. Patience = 44\n",
      "Epoch 100: training loss = 0.0000, validation loss = 0.0000. Learning rate = 3.125e-05. Patience = 45\n",
      "Epoch 101: training loss = 0.0000, validation loss = 0.0000. Learning rate = 3.125e-05. Patience = 46\n",
      "Epoch 102: training loss = 0.0000, validation loss = 0.0000. Learning rate = 3.125e-05. Patience = 47\n",
      "Epoch 103: training loss = 0.0000, validation loss = 0.0000. Learning rate = 3.125e-05. Patience = 48\n",
      "Epoch 104: training loss = 0.0000, validation loss = 0.0000. Learning rate = 3.125e-05. Patience = 49\n",
      "Epoch 105: training loss = 0.0000, validation loss = 0.0000. Learning rate = 3.125e-05. Patience = 50\n",
      "Training stopped at epoch 105; restoring weights from epoch 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alled\\anaconda3\\envs\\trading\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\alled\\anaconda3\\envs\\trading\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\alled\\anaconda3\\envs\\trading\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\alled\\anaconda3\\envs\\trading\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training loss = 0.8555, validation loss = 1.0442. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.4521, validation loss = 0.6214. Learning rate = 0.001. Patience = 0\n",
      "Epoch 3: training loss = 0.3083, validation loss = 0.5129. Learning rate = 0.001. Patience = 0\n",
      "Epoch 4: training loss = 0.2619, validation loss = 0.3946. Learning rate = 0.001. Patience = 0\n",
      "Epoch 5: training loss = 0.2366, validation loss = 0.3659. Learning rate = 0.001. Patience = 0\n",
      "Epoch 6: training loss = 0.2352, validation loss = 0.3499. Learning rate = 0.001. Patience = 0\n",
      "Epoch 7: training loss = 0.2203, validation loss = 0.3469. Learning rate = 0.001. Patience = 0\n",
      "Epoch 8: training loss = 0.2247, validation loss = 0.3374. Learning rate = 0.001. Patience = 0\n",
      "Epoch 9: training loss = 0.2399, validation loss = 0.3097. Learning rate = 0.001. Patience = 0\n",
      "Epoch 10: training loss = 0.2153, validation loss = 0.3194. Learning rate = 0.001. Patience = 1\n",
      "Epoch 11: training loss = 0.2185, validation loss = 0.3094. Learning rate = 0.001. Patience = 0\n",
      "Epoch 12: training loss = 0.2050, validation loss = 0.2991. Learning rate = 0.001. Patience = 0\n",
      "Epoch 13: training loss = 0.2281, validation loss = 0.3043. Learning rate = 0.001. Patience = 1\n",
      "Epoch 14: training loss = 0.1942, validation loss = 0.3013. Learning rate = 0.001. Patience = 2\n",
      "Epoch 15: training loss = 0.1999, validation loss = 0.2814. Learning rate = 0.001. Patience = 0\n",
      "Epoch 16: training loss = 0.1978, validation loss = 0.2933. Learning rate = 0.001. Patience = 1\n",
      "Epoch 17: training loss = 0.1944, validation loss = 0.2939. Learning rate = 0.001. Patience = 2\n",
      "Epoch 18: training loss = 0.1896, validation loss = 0.2997. Learning rate = 0.001. Patience = 3\n",
      "Epoch 19: training loss = 0.1888, validation loss = 0.2722. Learning rate = 0.001. Patience = 0\n",
      "Epoch 20: training loss = 0.2095, validation loss = 0.2873. Learning rate = 0.001. Patience = 1\n",
      "Epoch 21: training loss = 0.1905, validation loss = 0.2659. Learning rate = 0.001. Patience = 0\n",
      "Epoch 22: training loss = 0.1895, validation loss = 0.3064. Learning rate = 0.001. Patience = 1\n",
      "Epoch 23: training loss = 0.1865, validation loss = 0.2660. Learning rate = 0.001. Patience = 2\n",
      "Epoch 24: training loss = 0.1937, validation loss = 0.2807. Learning rate = 0.001. Patience = 3\n",
      "Epoch 25: training loss = 0.1804, validation loss = 0.2660. Learning rate = 0.001. Patience = 4\n",
      "Epoch 26: training loss = 0.1860, validation loss = 0.2676. Learning rate = 0.001. Patience = 5\n",
      "Epoch 27: training loss = 0.1929, validation loss = 0.2734. Learning rate = 0.001. Patience = 6\n",
      "Epoch 28: training loss = 0.1971, validation loss = 0.2729. Learning rate = 0.001. Patience = 7\n",
      "Epoch 29: training loss = 0.1746, validation loss = 0.2823. Learning rate = 0.001. Patience = 8\n",
      "Epoch 30: training loss = 0.1732, validation loss = 0.2751. Learning rate = 0.001. Patience = 9\n",
      "Epoch 31: training loss = 0.1775, validation loss = 0.2670. Learning rate = 0.001. Patience = 10\n",
      "Epoch 32: training loss = 0.1764, validation loss = 0.2763. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 33: training loss = 0.1756, validation loss = 0.2835. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 34: training loss = 0.1850, validation loss = 0.2669. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 35: training loss = 0.1856, validation loss = 0.2729. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 36: training loss = 0.1688, validation loss = 0.2804. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 37: training loss = 0.1763, validation loss = 0.2675. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 38: training loss = 0.1881, validation loss = 0.2986. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 39: training loss = 0.1708, validation loss = 0.2665. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 40: training loss = 0.1765, validation loss = 0.2796. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 41: training loss = 0.1735, validation loss = 0.2603. Learning rate = 0.0005. Patience = 0\n",
      "Epoch 42: training loss = 0.1857, validation loss = 0.2852. Learning rate = 0.0005. Patience = 1\n",
      "Epoch 43: training loss = 0.1740, validation loss = 0.2937. Learning rate = 0.0005. Patience = 2\n",
      "Epoch 44: training loss = 0.1670, validation loss = 0.2769. Learning rate = 0.0005. Patience = 3\n",
      "Epoch 45: training loss = 0.1734, validation loss = 0.2651. Learning rate = 0.0005. Patience = 4\n",
      "Epoch 46: training loss = 0.1657, validation loss = 0.2900. Learning rate = 0.0005. Patience = 5\n",
      "Epoch 47: training loss = 0.1685, validation loss = 0.2700. Learning rate = 0.0005. Patience = 6\n",
      "Epoch 48: training loss = 0.1747, validation loss = 0.2807. Learning rate = 0.0005. Patience = 7\n",
      "Epoch 49: training loss = 0.1774, validation loss = 0.2802. Learning rate = 0.0005. Patience = 8\n",
      "Epoch 50: training loss = 0.1667, validation loss = 0.2711. Learning rate = 0.0005. Patience = 9\n",
      "Epoch 51: training loss = 0.1651, validation loss = 0.2816. Learning rate = 0.0005. Patience = 10\n",
      "Epoch 52: training loss = 0.1662, validation loss = 0.2660. Learning rate = 0.00025. Patience = 11\n",
      "Epoch 53: training loss = 0.1742, validation loss = 0.2761. Learning rate = 0.00025. Patience = 12\n",
      "Epoch 54: training loss = 0.1746, validation loss = 0.2824. Learning rate = 0.00025. Patience = 13\n",
      "Epoch 55: training loss = 0.1600, validation loss = 0.2744. Learning rate = 0.00025. Patience = 14\n",
      "Epoch 56: training loss = 0.1613, validation loss = 0.2719. Learning rate = 0.00025. Patience = 15\n",
      "Epoch 57: training loss = 0.1671, validation loss = 0.2834. Learning rate = 0.00025. Patience = 16\n",
      "Epoch 58: training loss = 0.1662, validation loss = 0.2701. Learning rate = 0.00025. Patience = 17\n",
      "Epoch 59: training loss = 0.1835, validation loss = 0.2764. Learning rate = 0.00025. Patience = 18\n",
      "Epoch 60: training loss = 0.1646, validation loss = 0.2684. Learning rate = 0.00025. Patience = 19\n",
      "Epoch 61: training loss = 0.1630, validation loss = 0.2781. Learning rate = 0.00025. Patience = 20\n",
      "Epoch 62: training loss = 0.1611, validation loss = 0.2743. Learning rate = 0.00025. Patience = 21\n",
      "Epoch 63: training loss = 0.1607, validation loss = 0.2756. Learning rate = 0.000125. Patience = 22\n",
      "Epoch 64: training loss = 0.1657, validation loss = 0.2785. Learning rate = 0.000125. Patience = 23\n",
      "Epoch 65: training loss = 0.1643, validation loss = 0.2717. Learning rate = 0.000125. Patience = 24\n",
      "Epoch 66: training loss = 0.1621, validation loss = 0.2810. Learning rate = 0.000125. Patience = 25\n",
      "Epoch 67: training loss = 0.1633, validation loss = 0.2721. Learning rate = 0.000125. Patience = 26\n",
      "Epoch 68: training loss = 0.1605, validation loss = 0.2715. Learning rate = 0.000125. Patience = 27\n",
      "Epoch 69: training loss = 0.1681, validation loss = 0.2843. Learning rate = 0.000125. Patience = 28\n",
      "Epoch 70: training loss = 0.1568, validation loss = 0.2823. Learning rate = 0.000125. Patience = 29\n",
      "Epoch 71: training loss = 0.1646, validation loss = 0.2701. Learning rate = 0.000125. Patience = 30\n",
      "Epoch 72: training loss = 0.1752, validation loss = 0.2702. Learning rate = 0.000125. Patience = 31\n",
      "Epoch 73: training loss = 0.1588, validation loss = 0.2739. Learning rate = 0.000125. Patience = 32\n",
      "Epoch 74: training loss = 0.1641, validation loss = 0.2766. Learning rate = 6.25e-05. Patience = 33\n",
      "Epoch 75: training loss = 0.1609, validation loss = 0.2729. Learning rate = 6.25e-05. Patience = 34\n",
      "Epoch 76: training loss = 0.1571, validation loss = 0.2751. Learning rate = 6.25e-05. Patience = 35\n",
      "Epoch 77: training loss = 0.1569, validation loss = 0.2771. Learning rate = 6.25e-05. Patience = 36\n",
      "Epoch 78: training loss = 0.1569, validation loss = 0.2810. Learning rate = 6.25e-05. Patience = 37\n",
      "Epoch 79: training loss = 0.1770, validation loss = 0.2786. Learning rate = 6.25e-05. Patience = 38\n",
      "Epoch 80: training loss = 0.1615, validation loss = 0.2692. Learning rate = 6.25e-05. Patience = 39\n",
      "Epoch 81: training loss = 0.1745, validation loss = 0.2747. Learning rate = 6.25e-05. Patience = 40\n",
      "Epoch 82: training loss = 0.1766, validation loss = 0.2759. Learning rate = 6.25e-05. Patience = 41\n",
      "Epoch 83: training loss = 0.1585, validation loss = 0.2752. Learning rate = 6.25e-05. Patience = 42\n",
      "Epoch 84: training loss = 0.1645, validation loss = 0.2781. Learning rate = 6.25e-05. Patience = 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85: training loss = 0.1624, validation loss = 0.2784. Learning rate = 3.125e-05. Patience = 44\n",
      "Epoch 86: training loss = 0.1654, validation loss = 0.2757. Learning rate = 3.125e-05. Patience = 45\n",
      "Epoch 87: training loss = 0.1686, validation loss = 0.2736. Learning rate = 3.125e-05. Patience = 46\n",
      "Epoch 88: training loss = 0.1596, validation loss = 0.2754. Learning rate = 3.125e-05. Patience = 47\n",
      "Epoch 89: training loss = 0.1651, validation loss = 0.2770. Learning rate = 3.125e-05. Patience = 48\n",
      "Epoch 90: training loss = 0.1653, validation loss = 0.2796. Learning rate = 3.125e-05. Patience = 49\n",
      "Epoch 91: training loss = 0.1578, validation loss = 0.2745. Learning rate = 3.125e-05. Patience = 50\n",
      "Training stopped at epoch 91; restoring weights from epoch 41\n",
      "Epoch 1: training loss = 0.6273, validation loss = 0.6060. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.4560, validation loss = 0.6149. Learning rate = 0.001. Patience = 1\n",
      "Epoch 3: training loss = 0.3607, validation loss = 0.4096. Learning rate = 0.001. Patience = 0\n",
      "Epoch 4: training loss = 0.3460, validation loss = 0.2882. Learning rate = 0.001. Patience = 0\n",
      "Epoch 5: training loss = 0.3511, validation loss = 0.3091. Learning rate = 0.001. Patience = 1\n",
      "Epoch 6: training loss = 0.3276, validation loss = 0.2469. Learning rate = 0.001. Patience = 0\n",
      "Epoch 7: training loss = 0.3166, validation loss = 0.2638. Learning rate = 0.001. Patience = 1\n",
      "Epoch 8: training loss = 0.3295, validation loss = 0.2406. Learning rate = 0.001. Patience = 0\n",
      "Epoch 9: training loss = 0.3080, validation loss = 0.2440. Learning rate = 0.001. Patience = 1\n",
      "Epoch 10: training loss = 0.2884, validation loss = 0.2481. Learning rate = 0.001. Patience = 2\n",
      "Epoch 11: training loss = 0.2884, validation loss = 0.2273. Learning rate = 0.001. Patience = 0\n",
      "Epoch 12: training loss = 0.3030, validation loss = 0.2089. Learning rate = 0.001. Patience = 0\n",
      "Epoch 13: training loss = 0.3032, validation loss = 0.2580. Learning rate = 0.001. Patience = 1\n",
      "Epoch 14: training loss = 0.2894, validation loss = 0.2137. Learning rate = 0.001. Patience = 2\n",
      "Epoch 15: training loss = 0.3259, validation loss = 0.2474. Learning rate = 0.001. Patience = 3\n",
      "Epoch 16: training loss = 0.3288, validation loss = 0.2089. Learning rate = 0.001. Patience = 4\n",
      "Epoch 17: training loss = 0.3098, validation loss = 0.2164. Learning rate = 0.001. Patience = 5\n",
      "Epoch 18: training loss = 0.2890, validation loss = 0.2068. Learning rate = 0.001. Patience = 0\n",
      "Epoch 19: training loss = 0.2706, validation loss = 0.1890. Learning rate = 0.001. Patience = 0\n",
      "Epoch 20: training loss = 0.2781, validation loss = 0.2793. Learning rate = 0.001. Patience = 1\n",
      "Epoch 21: training loss = 0.3103, validation loss = 0.1975. Learning rate = 0.001. Patience = 2\n",
      "Epoch 22: training loss = 0.2771, validation loss = 0.2283. Learning rate = 0.001. Patience = 3\n",
      "Epoch 23: training loss = 0.2893, validation loss = 0.2134. Learning rate = 0.001. Patience = 4\n",
      "Epoch 24: training loss = 0.2602, validation loss = 0.2161. Learning rate = 0.001. Patience = 5\n",
      "Epoch 25: training loss = 0.2513, validation loss = 0.2020. Learning rate = 0.001. Patience = 6\n",
      "Epoch 26: training loss = 0.2788, validation loss = 0.2200. Learning rate = 0.001. Patience = 7\n",
      "Epoch 27: training loss = 0.2484, validation loss = 0.2000. Learning rate = 0.001. Patience = 8\n",
      "Epoch 28: training loss = 0.2866, validation loss = 0.3106. Learning rate = 0.001. Patience = 9\n",
      "Epoch 29: training loss = 0.2654, validation loss = 0.2110. Learning rate = 0.001. Patience = 10\n",
      "Epoch 30: training loss = 0.2543, validation loss = 0.2520. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 31: training loss = 0.2750, validation loss = 0.2109. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 32: training loss = 0.2547, validation loss = 0.2388. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 33: training loss = 0.2730, validation loss = 0.2114. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 34: training loss = 0.2695, validation loss = 0.2173. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 35: training loss = 0.2700, validation loss = 0.2172. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 36: training loss = 0.2690, validation loss = 0.2187. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 37: training loss = 0.2670, validation loss = 0.2234. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 38: training loss = 0.2659, validation loss = 0.2350. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 39: training loss = 0.2531, validation loss = 0.2235. Learning rate = 0.0005. Patience = 20\n",
      "Epoch 40: training loss = 0.2727, validation loss = 0.2206. Learning rate = 0.0005. Patience = 21\n",
      "Epoch 41: training loss = 0.2532, validation loss = 0.2383. Learning rate = 0.00025. Patience = 22\n",
      "Epoch 42: training loss = 0.2578, validation loss = 0.2309. Learning rate = 0.00025. Patience = 23\n",
      "Epoch 43: training loss = 0.2652, validation loss = 0.2277. Learning rate = 0.00025. Patience = 24\n",
      "Epoch 44: training loss = 0.2379, validation loss = 0.2426. Learning rate = 0.00025. Patience = 25\n",
      "Epoch 45: training loss = 0.2541, validation loss = 0.2253. Learning rate = 0.00025. Patience = 26\n",
      "Epoch 46: training loss = 0.2520, validation loss = 0.2276. Learning rate = 0.00025. Patience = 27\n",
      "Epoch 47: training loss = 0.2592, validation loss = 0.2453. Learning rate = 0.00025. Patience = 28\n",
      "Epoch 48: training loss = 0.2335, validation loss = 0.2271. Learning rate = 0.00025. Patience = 29\n",
      "Epoch 49: training loss = 0.2436, validation loss = 0.2335. Learning rate = 0.00025. Patience = 30\n",
      "Epoch 50: training loss = 0.2683, validation loss = 0.2298. Learning rate = 0.00025. Patience = 31\n",
      "Epoch 51: training loss = 0.2462, validation loss = 0.2332. Learning rate = 0.00025. Patience = 32\n",
      "Epoch 52: training loss = 0.2346, validation loss = 0.2395. Learning rate = 0.000125. Patience = 33\n",
      "Epoch 53: training loss = 0.2878, validation loss = 0.2337. Learning rate = 0.000125. Patience = 34\n",
      "Epoch 54: training loss = 0.2444, validation loss = 0.2342. Learning rate = 0.000125. Patience = 35\n",
      "Epoch 55: training loss = 0.2509, validation loss = 0.2314. Learning rate = 0.000125. Patience = 36\n",
      "Epoch 56: training loss = 0.2546, validation loss = 0.2334. Learning rate = 0.000125. Patience = 37\n",
      "Epoch 57: training loss = 0.2528, validation loss = 0.2471. Learning rate = 0.000125. Patience = 38\n",
      "Epoch 58: training loss = 0.2372, validation loss = 0.2425. Learning rate = 0.000125. Patience = 39\n",
      "Epoch 59: training loss = 0.2590, validation loss = 0.2332. Learning rate = 0.000125. Patience = 40\n",
      "Epoch 60: training loss = 0.2804, validation loss = 0.2443. Learning rate = 0.000125. Patience = 41\n",
      "Epoch 61: training loss = 0.2555, validation loss = 0.2482. Learning rate = 0.000125. Patience = 42\n",
      "Epoch 62: training loss = 0.2619, validation loss = 0.2398. Learning rate = 0.000125. Patience = 43\n",
      "Epoch 63: training loss = 0.2475, validation loss = 0.2336. Learning rate = 6.25e-05. Patience = 44\n",
      "Epoch 64: training loss = 0.2653, validation loss = 0.2343. Learning rate = 6.25e-05. Patience = 45\n",
      "Epoch 65: training loss = 0.2443, validation loss = 0.2371. Learning rate = 6.25e-05. Patience = 46\n",
      "Epoch 66: training loss = 0.2721, validation loss = 0.2431. Learning rate = 6.25e-05. Patience = 47\n",
      "Epoch 67: training loss = 0.2479, validation loss = 0.2395. Learning rate = 6.25e-05. Patience = 48\n",
      "Epoch 68: training loss = 0.2560, validation loss = 0.2375. Learning rate = 6.25e-05. Patience = 49\n",
      "Epoch 69: training loss = 0.2422, validation loss = 0.2377. Learning rate = 6.25e-05. Patience = 50\n",
      "Training stopped at epoch 69; restoring weights from epoch 19\n",
      "Epoch 1: training loss = 0.8618, validation loss = 1.0204. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.5801, validation loss = 0.6517. Learning rate = 0.001. Patience = 0\n",
      "Epoch 3: training loss = 0.4955, validation loss = 0.6406. Learning rate = 0.001. Patience = 0\n",
      "Epoch 4: training loss = 0.4570, validation loss = 0.5977. Learning rate = 0.001. Patience = 0\n",
      "Epoch 5: training loss = 0.4340, validation loss = 0.5660. Learning rate = 0.001. Patience = 0\n",
      "Epoch 6: training loss = 0.4377, validation loss = 0.5698. Learning rate = 0.001. Patience = 1\n",
      "Epoch 7: training loss = 0.3999, validation loss = 0.5633. Learning rate = 0.001. Patience = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: training loss = 0.4006, validation loss = 0.5465. Learning rate = 0.001. Patience = 0\n",
      "Epoch 9: training loss = 0.3646, validation loss = 0.5557. Learning rate = 0.001. Patience = 1\n",
      "Epoch 10: training loss = 0.3770, validation loss = 0.5167. Learning rate = 0.001. Patience = 0\n",
      "Epoch 11: training loss = 0.3835, validation loss = 0.5025. Learning rate = 0.001. Patience = 0\n",
      "Epoch 12: training loss = 0.3407, validation loss = 0.5961. Learning rate = 0.001. Patience = 1\n",
      "Epoch 13: training loss = 0.3430, validation loss = 0.5022. Learning rate = 0.001. Patience = 0\n",
      "Epoch 14: training loss = 0.3333, validation loss = 0.4964. Learning rate = 0.001. Patience = 0\n",
      "Epoch 15: training loss = 0.3567, validation loss = 0.5075. Learning rate = 0.001. Patience = 1\n",
      "Epoch 16: training loss = 0.3165, validation loss = 0.4967. Learning rate = 0.001. Patience = 2\n",
      "Epoch 17: training loss = 0.3052, validation loss = 0.5215. Learning rate = 0.001. Patience = 3\n",
      "Epoch 18: training loss = 0.3101, validation loss = 0.5059. Learning rate = 0.001. Patience = 4\n",
      "Epoch 19: training loss = 0.3104, validation loss = 0.5068. Learning rate = 0.001. Patience = 5\n",
      "Epoch 20: training loss = 0.3124, validation loss = 0.5033. Learning rate = 0.001. Patience = 6\n",
      "Epoch 21: training loss = 0.2983, validation loss = 0.4924. Learning rate = 0.001. Patience = 0\n",
      "Epoch 22: training loss = 0.3031, validation loss = 0.5373. Learning rate = 0.001. Patience = 1\n",
      "Epoch 23: training loss = 0.3016, validation loss = 0.4897. Learning rate = 0.001. Patience = 0\n",
      "Epoch 24: training loss = 0.2925, validation loss = 0.4989. Learning rate = 0.001. Patience = 1\n",
      "Epoch 25: training loss = 0.2903, validation loss = 0.4880. Learning rate = 0.001. Patience = 0\n",
      "Epoch 26: training loss = 0.2985, validation loss = 0.4985. Learning rate = 0.001. Patience = 1\n",
      "Epoch 27: training loss = 0.2824, validation loss = 0.4995. Learning rate = 0.001. Patience = 2\n",
      "Epoch 28: training loss = 0.2869, validation loss = 0.4742. Learning rate = 0.001. Patience = 0\n",
      "Epoch 29: training loss = 0.2856, validation loss = 0.5034. Learning rate = 0.001. Patience = 1\n",
      "Epoch 30: training loss = 0.2752, validation loss = 0.4957. Learning rate = 0.001. Patience = 2\n",
      "Epoch 31: training loss = 0.2723, validation loss = 0.4994. Learning rate = 0.001. Patience = 3\n",
      "Epoch 32: training loss = 0.2924, validation loss = 0.5413. Learning rate = 0.001. Patience = 4\n",
      "Epoch 33: training loss = 0.2718, validation loss = 0.5028. Learning rate = 0.001. Patience = 5\n",
      "Epoch 34: training loss = 0.2640, validation loss = 0.5239. Learning rate = 0.001. Patience = 6\n",
      "Epoch 35: training loss = 0.2671, validation loss = 0.4828. Learning rate = 0.001. Patience = 7\n",
      "Epoch 36: training loss = 0.2718, validation loss = 0.5017. Learning rate = 0.001. Patience = 8\n",
      "Epoch 37: training loss = 0.2649, validation loss = 0.4903. Learning rate = 0.001. Patience = 9\n",
      "Epoch 38: training loss = 0.2549, validation loss = 0.4839. Learning rate = 0.001. Patience = 10\n",
      "Epoch 39: training loss = 0.2560, validation loss = 0.4874. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 40: training loss = 0.2475, validation loss = 0.5276. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 41: training loss = 0.2574, validation loss = 0.4943. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 42: training loss = 0.2416, validation loss = 0.5014. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 43: training loss = 0.2383, validation loss = 0.4881. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 44: training loss = 0.2410, validation loss = 0.4969. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 45: training loss = 0.2600, validation loss = 0.4980. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 46: training loss = 0.2410, validation loss = 0.4892. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 47: training loss = 0.2389, validation loss = 0.5059. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 48: training loss = 0.2563, validation loss = 0.4942. Learning rate = 0.0005. Patience = 20\n",
      "Epoch 49: training loss = 0.2411, validation loss = 0.4986. Learning rate = 0.0005. Patience = 21\n",
      "Epoch 50: training loss = 0.2465, validation loss = 0.5128. Learning rate = 0.00025. Patience = 22\n",
      "Epoch 51: training loss = 0.2558, validation loss = 0.4908. Learning rate = 0.00025. Patience = 23\n",
      "Epoch 52: training loss = 0.2325, validation loss = 0.4995. Learning rate = 0.00025. Patience = 24\n",
      "Epoch 53: training loss = 0.2564, validation loss = 0.4939. Learning rate = 0.00025. Patience = 25\n",
      "Epoch 54: training loss = 0.2371, validation loss = 0.5082. Learning rate = 0.00025. Patience = 26\n",
      "Epoch 55: training loss = 0.2316, validation loss = 0.4994. Learning rate = 0.00025. Patience = 27\n",
      "Epoch 56: training loss = 0.2434, validation loss = 0.4934. Learning rate = 0.00025. Patience = 28\n",
      "Epoch 57: training loss = 0.2279, validation loss = 0.5110. Learning rate = 0.00025. Patience = 29\n",
      "Epoch 58: training loss = 0.2327, validation loss = 0.4961. Learning rate = 0.00025. Patience = 30\n",
      "Epoch 59: training loss = 0.2361, validation loss = 0.5101. Learning rate = 0.00025. Patience = 31\n",
      "Epoch 60: training loss = 0.2413, validation loss = 0.4981. Learning rate = 0.00025. Patience = 32\n",
      "Epoch 61: training loss = 0.2313, validation loss = 0.5110. Learning rate = 0.000125. Patience = 33\n",
      "Epoch 62: training loss = 0.2340, validation loss = 0.5114. Learning rate = 0.000125. Patience = 34\n",
      "Epoch 63: training loss = 0.2327, validation loss = 0.5040. Learning rate = 0.000125. Patience = 35\n",
      "Epoch 64: training loss = 0.2269, validation loss = 0.5024. Learning rate = 0.000125. Patience = 36\n",
      "Epoch 65: training loss = 0.2396, validation loss = 0.5030. Learning rate = 0.000125. Patience = 37\n",
      "Epoch 66: training loss = 0.2298, validation loss = 0.5070. Learning rate = 0.000125. Patience = 38\n",
      "Epoch 67: training loss = 0.2322, validation loss = 0.4979. Learning rate = 0.000125. Patience = 39\n",
      "Epoch 68: training loss = 0.2232, validation loss = 0.4997. Learning rate = 0.000125. Patience = 40\n",
      "Epoch 69: training loss = 0.2381, validation loss = 0.5040. Learning rate = 0.000125. Patience = 41\n",
      "Epoch 70: training loss = 0.2251, validation loss = 0.5151. Learning rate = 0.000125. Patience = 42\n",
      "Epoch 71: training loss = 0.2272, validation loss = 0.5009. Learning rate = 0.000125. Patience = 43\n",
      "Epoch 72: training loss = 0.2339, validation loss = 0.5021. Learning rate = 6.25e-05. Patience = 44\n",
      "Epoch 73: training loss = 0.2344, validation loss = 0.5019. Learning rate = 6.25e-05. Patience = 45\n",
      "Epoch 74: training loss = 0.2416, validation loss = 0.5033. Learning rate = 6.25e-05. Patience = 46\n",
      "Epoch 75: training loss = 0.2387, validation loss = 0.5035. Learning rate = 6.25e-05. Patience = 47\n",
      "Epoch 76: training loss = 0.2284, validation loss = 0.5014. Learning rate = 6.25e-05. Patience = 48\n",
      "Epoch 77: training loss = 0.2292, validation loss = 0.5007. Learning rate = 6.25e-05. Patience = 49\n",
      "Epoch 78: training loss = 0.2162, validation loss = 0.5037. Learning rate = 6.25e-05. Patience = 50\n",
      "Training stopped at epoch 78; restoring weights from epoch 28\n",
      "Epoch 1: training loss = 0.7752, validation loss = 0.8934. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.3572, validation loss = 0.4364. Learning rate = 0.001. Patience = 0\n",
      "Epoch 3: training loss = 0.2324, validation loss = 0.4310. Learning rate = 0.001. Patience = 0\n",
      "Epoch 4: training loss = 0.1952, validation loss = 0.2702. Learning rate = 0.001. Patience = 0\n",
      "Epoch 5: training loss = 0.1898, validation loss = 0.3371. Learning rate = 0.001. Patience = 1\n",
      "Epoch 6: training loss = 0.1843, validation loss = 0.3387. Learning rate = 0.001. Patience = 2\n",
      "Epoch 7: training loss = 0.1792, validation loss = 0.3125. Learning rate = 0.001. Patience = 3\n",
      "Epoch 8: training loss = 0.1628, validation loss = 0.3462. Learning rate = 0.001. Patience = 4\n",
      "Epoch 9: training loss = 0.1654, validation loss = 0.3168. Learning rate = 0.001. Patience = 5\n",
      "Epoch 10: training loss = 0.1572, validation loss = 0.3539. Learning rate = 0.001. Patience = 6\n",
      "Epoch 11: training loss = 0.1646, validation loss = 0.3201. Learning rate = 0.001. Patience = 7\n",
      "Epoch 12: training loss = 0.1602, validation loss = 0.3518. Learning rate = 0.001. Patience = 8\n",
      "Epoch 13: training loss = 0.1623, validation loss = 0.3447. Learning rate = 0.001. Patience = 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: training loss = 0.1565, validation loss = 0.3372. Learning rate = 0.001. Patience = 10\n",
      "Epoch 15: training loss = 0.1553, validation loss = 0.3491. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 16: training loss = 0.1499, validation loss = 0.3314. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 17: training loss = 0.1518, validation loss = 0.3525. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 18: training loss = 0.1457, validation loss = 0.3387. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 19: training loss = 0.1490, validation loss = 0.3345. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 20: training loss = 0.1543, validation loss = 0.3475. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 21: training loss = 0.1536, validation loss = 0.3212. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 22: training loss = 0.1521, validation loss = 0.3381. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 23: training loss = 0.1442, validation loss = 0.3297. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 24: training loss = 0.1471, validation loss = 0.3280. Learning rate = 0.0005. Patience = 20\n",
      "Epoch 25: training loss = 0.1461, validation loss = 0.3374. Learning rate = 0.0005. Patience = 21\n",
      "Epoch 26: training loss = 0.1486, validation loss = 0.3304. Learning rate = 0.00025. Patience = 22\n",
      "Epoch 27: training loss = 0.1481, validation loss = 0.3506. Learning rate = 0.00025. Patience = 23\n",
      "Epoch 28: training loss = 0.1520, validation loss = 0.3493. Learning rate = 0.00025. Patience = 24\n",
      "Epoch 29: training loss = 0.1428, validation loss = 0.3333. Learning rate = 0.00025. Patience = 25\n",
      "Epoch 30: training loss = 0.1442, validation loss = 0.3457. Learning rate = 0.00025. Patience = 26\n",
      "Epoch 31: training loss = 0.1443, validation loss = 0.3290. Learning rate = 0.00025. Patience = 27\n",
      "Epoch 32: training loss = 0.1494, validation loss = 0.3473. Learning rate = 0.00025. Patience = 28\n",
      "Epoch 33: training loss = 0.1383, validation loss = 0.3398. Learning rate = 0.00025. Patience = 29\n",
      "Epoch 34: training loss = 0.1505, validation loss = 0.3287. Learning rate = 0.00025. Patience = 30\n",
      "Epoch 35: training loss = 0.1382, validation loss = 0.3391. Learning rate = 0.00025. Patience = 31\n",
      "Epoch 36: training loss = 0.1471, validation loss = 0.3392. Learning rate = 0.00025. Patience = 32\n",
      "Epoch 37: training loss = 0.1450, validation loss = 0.3449. Learning rate = 0.000125. Patience = 33\n",
      "Epoch 38: training loss = 0.1426, validation loss = 0.3425. Learning rate = 0.000125. Patience = 34\n",
      "Epoch 39: training loss = 0.1555, validation loss = 0.3333. Learning rate = 0.000125. Patience = 35\n",
      "Epoch 40: training loss = 0.1496, validation loss = 0.3300. Learning rate = 0.000125. Patience = 36\n",
      "Epoch 41: training loss = 0.1376, validation loss = 0.3350. Learning rate = 0.000125. Patience = 37\n",
      "Epoch 42: training loss = 0.1435, validation loss = 0.3327. Learning rate = 0.000125. Patience = 38\n",
      "Epoch 43: training loss = 0.1456, validation loss = 0.3300. Learning rate = 0.000125. Patience = 39\n",
      "Epoch 44: training loss = 0.1379, validation loss = 0.3343. Learning rate = 0.000125. Patience = 40\n",
      "Epoch 45: training loss = 0.1407, validation loss = 0.3504. Learning rate = 0.000125. Patience = 41\n",
      "Epoch 46: training loss = 0.1487, validation loss = 0.3323. Learning rate = 0.000125. Patience = 42\n",
      "Epoch 47: training loss = 0.1421, validation loss = 0.3207. Learning rate = 0.000125. Patience = 43\n",
      "Epoch 48: training loss = 0.1452, validation loss = 0.3290. Learning rate = 6.25e-05. Patience = 44\n",
      "Epoch 49: training loss = 0.1377, validation loss = 0.3373. Learning rate = 6.25e-05. Patience = 45\n",
      "Epoch 50: training loss = 0.1457, validation loss = 0.3422. Learning rate = 6.25e-05. Patience = 46\n",
      "Epoch 51: training loss = 0.1380, validation loss = 0.3327. Learning rate = 6.25e-05. Patience = 47\n",
      "Epoch 52: training loss = 0.1407, validation loss = 0.3320. Learning rate = 6.25e-05. Patience = 48\n",
      "Epoch 53: training loss = 0.1405, validation loss = 0.3345. Learning rate = 6.25e-05. Patience = 49\n",
      "Epoch 54: training loss = 0.1421, validation loss = 0.3378. Learning rate = 6.25e-05. Patience = 50\n",
      "Training stopped at epoch 54; restoring weights from epoch 4\n",
      "Epoch 1: training loss = 0.8657, validation loss = 0.8234. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.5047, validation loss = 0.5126. Learning rate = 0.001. Patience = 0\n",
      "Epoch 3: training loss = 0.3507, validation loss = 0.4515. Learning rate = 0.001. Patience = 0\n",
      "Epoch 4: training loss = 0.3203, validation loss = 0.4133. Learning rate = 0.001. Patience = 0\n",
      "Epoch 5: training loss = 0.2979, validation loss = 0.3928. Learning rate = 0.001. Patience = 0\n",
      "Epoch 6: training loss = 0.2664, validation loss = 0.3735. Learning rate = 0.001. Patience = 0\n",
      "Epoch 7: training loss = 0.2908, validation loss = 0.3765. Learning rate = 0.001. Patience = 1\n",
      "Epoch 8: training loss = 0.2634, validation loss = 0.3733. Learning rate = 0.001. Patience = 0\n",
      "Epoch 9: training loss = 0.2671, validation loss = 0.3645. Learning rate = 0.001. Patience = 0\n",
      "Epoch 10: training loss = 0.2594, validation loss = 0.3748. Learning rate = 0.001. Patience = 1\n",
      "Epoch 11: training loss = 0.2643, validation loss = 0.3650. Learning rate = 0.001. Patience = 2\n",
      "Epoch 12: training loss = 0.2578, validation loss = 0.3821. Learning rate = 0.001. Patience = 3\n",
      "Epoch 13: training loss = 0.2567, validation loss = 0.3587. Learning rate = 0.001. Patience = 0\n",
      "Epoch 14: training loss = 0.2443, validation loss = 0.3651. Learning rate = 0.001. Patience = 1\n",
      "Epoch 15: training loss = 0.2515, validation loss = 0.3628. Learning rate = 0.001. Patience = 2\n",
      "Epoch 16: training loss = 0.2522, validation loss = 0.3647. Learning rate = 0.001. Patience = 3\n",
      "Epoch 17: training loss = 0.2418, validation loss = 0.3627. Learning rate = 0.001. Patience = 4\n",
      "Epoch 18: training loss = 0.2350, validation loss = 0.3529. Learning rate = 0.001. Patience = 0\n",
      "Epoch 19: training loss = 0.2310, validation loss = 0.3658. Learning rate = 0.001. Patience = 1\n",
      "Epoch 20: training loss = 0.2307, validation loss = 0.3548. Learning rate = 0.001. Patience = 2\n",
      "Epoch 21: training loss = 0.2275, validation loss = 0.3563. Learning rate = 0.001. Patience = 3\n",
      "Epoch 22: training loss = 0.2452, validation loss = 0.3681. Learning rate = 0.001. Patience = 4\n",
      "Epoch 23: training loss = 0.2481, validation loss = 0.3578. Learning rate = 0.001. Patience = 5\n",
      "Epoch 24: training loss = 0.2236, validation loss = 0.3560. Learning rate = 0.001. Patience = 6\n",
      "Epoch 25: training loss = 0.2187, validation loss = 0.3494. Learning rate = 0.001. Patience = 0\n",
      "Epoch 26: training loss = 0.2342, validation loss = 0.3516. Learning rate = 0.001. Patience = 1\n",
      "Epoch 27: training loss = 0.2214, validation loss = 0.3586. Learning rate = 0.001. Patience = 2\n",
      "Epoch 28: training loss = 0.2091, validation loss = 0.3626. Learning rate = 0.001. Patience = 3\n",
      "Epoch 29: training loss = 0.2149, validation loss = 0.3518. Learning rate = 0.001. Patience = 4\n",
      "Epoch 30: training loss = 0.2104, validation loss = 0.3665. Learning rate = 0.001. Patience = 5\n",
      "Epoch 31: training loss = 0.2082, validation loss = 0.3600. Learning rate = 0.001. Patience = 6\n",
      "Epoch 32: training loss = 0.1997, validation loss = 0.3639. Learning rate = 0.001. Patience = 7\n",
      "Epoch 33: training loss = 0.2177, validation loss = 0.3540. Learning rate = 0.001. Patience = 8\n",
      "Epoch 34: training loss = 0.2157, validation loss = 0.3597. Learning rate = 0.001. Patience = 9\n",
      "Epoch 35: training loss = 0.2084, validation loss = 0.3564. Learning rate = 0.001. Patience = 10\n",
      "Epoch 36: training loss = 0.1996, validation loss = 0.3517. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 37: training loss = 0.2141, validation loss = 0.3533. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 38: training loss = 0.2030, validation loss = 0.3541. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 39: training loss = 0.1891, validation loss = 0.3498. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 40: training loss = 0.1889, validation loss = 0.3515. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 41: training loss = 0.1901, validation loss = 0.3510. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 42: training loss = 0.1932, validation loss = 0.3508. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 43: training loss = 0.1900, validation loss = 0.3529. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 44: training loss = 0.1926, validation loss = 0.3507. Learning rate = 0.0005. Patience = 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: training loss = 0.1867, validation loss = 0.3488. Learning rate = 0.0005. Patience = 0\n",
      "Epoch 46: training loss = 0.1911, validation loss = 0.3495. Learning rate = 0.0005. Patience = 1\n",
      "Epoch 47: training loss = 0.1908, validation loss = 0.3583. Learning rate = 0.0005. Patience = 2\n",
      "Epoch 48: training loss = 0.1831, validation loss = 0.3536. Learning rate = 0.0005. Patience = 3\n",
      "Epoch 49: training loss = 0.1891, validation loss = 0.3541. Learning rate = 0.0005. Patience = 4\n",
      "Epoch 50: training loss = 0.1890, validation loss = 0.3513. Learning rate = 0.0005. Patience = 5\n",
      "Epoch 51: training loss = 0.1940, validation loss = 0.3646. Learning rate = 0.0005. Patience = 6\n",
      "Epoch 52: training loss = 0.1823, validation loss = 0.3596. Learning rate = 0.0005. Patience = 7\n",
      "Epoch 53: training loss = 0.1792, validation loss = 0.3664. Learning rate = 0.0005. Patience = 8\n",
      "Epoch 54: training loss = 0.1823, validation loss = 0.3603. Learning rate = 0.0005. Patience = 9\n",
      "Epoch 55: training loss = 0.1774, validation loss = 0.3584. Learning rate = 0.0005. Patience = 10\n",
      "Epoch 56: training loss = 0.1866, validation loss = 0.3653. Learning rate = 0.00025. Patience = 11\n",
      "Epoch 57: training loss = 0.1752, validation loss = 0.3602. Learning rate = 0.00025. Patience = 12\n",
      "Epoch 58: training loss = 0.1836, validation loss = 0.3627. Learning rate = 0.00025. Patience = 13\n",
      "Epoch 59: training loss = 0.1775, validation loss = 0.3630. Learning rate = 0.00025. Patience = 14\n",
      "Epoch 60: training loss = 0.1801, validation loss = 0.3621. Learning rate = 0.00025. Patience = 15\n",
      "Epoch 61: training loss = 0.1877, validation loss = 0.3640. Learning rate = 0.00025. Patience = 16\n",
      "Epoch 62: training loss = 0.1716, validation loss = 0.3612. Learning rate = 0.00025. Patience = 17\n",
      "Epoch 63: training loss = 0.1910, validation loss = 0.3643. Learning rate = 0.00025. Patience = 18\n",
      "Epoch 64: training loss = 0.1733, validation loss = 0.3595. Learning rate = 0.00025. Patience = 19\n",
      "Epoch 65: training loss = 0.1820, validation loss = 0.3633. Learning rate = 0.00025. Patience = 20\n",
      "Epoch 66: training loss = 0.1759, validation loss = 0.3650. Learning rate = 0.00025. Patience = 21\n",
      "Epoch 67: training loss = 0.1734, validation loss = 0.3649. Learning rate = 0.000125. Patience = 22\n",
      "Epoch 68: training loss = 0.1700, validation loss = 0.3641. Learning rate = 0.000125. Patience = 23\n",
      "Epoch 69: training loss = 0.1745, validation loss = 0.3637. Learning rate = 0.000125. Patience = 24\n",
      "Epoch 70: training loss = 0.1764, validation loss = 0.3646. Learning rate = 0.000125. Patience = 25\n",
      "Epoch 71: training loss = 0.1787, validation loss = 0.3641. Learning rate = 0.000125. Patience = 26\n",
      "Epoch 72: training loss = 0.1745, validation loss = 0.3671. Learning rate = 0.000125. Patience = 27\n",
      "Epoch 73: training loss = 0.1708, validation loss = 0.3630. Learning rate = 0.000125. Patience = 28\n",
      "Epoch 74: training loss = 0.1780, validation loss = 0.3634. Learning rate = 0.000125. Patience = 29\n",
      "Epoch 75: training loss = 0.1788, validation loss = 0.3633. Learning rate = 0.000125. Patience = 30\n",
      "Epoch 76: training loss = 0.1694, validation loss = 0.3611. Learning rate = 0.000125. Patience = 31\n",
      "Epoch 77: training loss = 0.1837, validation loss = 0.3617. Learning rate = 0.000125. Patience = 32\n",
      "Epoch 78: training loss = 0.1698, validation loss = 0.3681. Learning rate = 6.25e-05. Patience = 33\n",
      "Epoch 79: training loss = 0.1696, validation loss = 0.3625. Learning rate = 6.25e-05. Patience = 34\n",
      "Epoch 80: training loss = 0.1753, validation loss = 0.3606. Learning rate = 6.25e-05. Patience = 35\n",
      "Epoch 81: training loss = 0.1760, validation loss = 0.3625. Learning rate = 6.25e-05. Patience = 36\n",
      "Epoch 82: training loss = 0.1747, validation loss = 0.3630. Learning rate = 6.25e-05. Patience = 37\n",
      "Epoch 83: training loss = 0.1823, validation loss = 0.3624. Learning rate = 6.25e-05. Patience = 38\n",
      "Epoch 84: training loss = 0.1704, validation loss = 0.3638. Learning rate = 6.25e-05. Patience = 39\n",
      "Epoch 85: training loss = 0.1742, validation loss = 0.3630. Learning rate = 6.25e-05. Patience = 40\n",
      "Epoch 86: training loss = 0.1918, validation loss = 0.3607. Learning rate = 6.25e-05. Patience = 41\n",
      "Epoch 87: training loss = 0.1805, validation loss = 0.3622. Learning rate = 6.25e-05. Patience = 42\n",
      "Epoch 88: training loss = 0.1784, validation loss = 0.3647. Learning rate = 6.25e-05. Patience = 43\n",
      "Epoch 89: training loss = 0.1703, validation loss = 0.3640. Learning rate = 3.125e-05. Patience = 44\n",
      "Epoch 90: training loss = 0.1745, validation loss = 0.3645. Learning rate = 3.125e-05. Patience = 45\n",
      "Epoch 91: training loss = 0.1703, validation loss = 0.3646. Learning rate = 3.125e-05. Patience = 46\n",
      "Epoch 92: training loss = 0.1735, validation loss = 0.3645. Learning rate = 3.125e-05. Patience = 47\n",
      "Epoch 93: training loss = 0.1658, validation loss = 0.3644. Learning rate = 3.125e-05. Patience = 48\n",
      "Epoch 94: training loss = 0.1705, validation loss = 0.3649. Learning rate = 3.125e-05. Patience = 49\n",
      "Epoch 95: training loss = 0.1710, validation loss = 0.3648. Learning rate = 3.125e-05. Patience = 50\n",
      "Training stopped at epoch 95; restoring weights from epoch 45\n",
      "Epoch 1: training loss = 0.7919, validation loss = 0.5629. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.4089, validation loss = 0.3272. Learning rate = 0.001. Patience = 0\n",
      "Epoch 3: training loss = 0.3097, validation loss = 0.2730. Learning rate = 0.001. Patience = 0\n",
      "Epoch 4: training loss = 0.2810, validation loss = 0.2698. Learning rate = 0.001. Patience = 0\n",
      "Epoch 5: training loss = 0.2721, validation loss = 0.2639. Learning rate = 0.001. Patience = 0\n",
      "Epoch 6: training loss = 0.2452, validation loss = 0.2599. Learning rate = 0.001. Patience = 0\n",
      "Epoch 7: training loss = 0.2646, validation loss = 0.2560. Learning rate = 0.001. Patience = 0\n",
      "Epoch 8: training loss = 0.2499, validation loss = 0.2771. Learning rate = 0.001. Patience = 1\n",
      "Epoch 9: training loss = 0.2403, validation loss = 0.2547. Learning rate = 0.001. Patience = 0\n",
      "Epoch 10: training loss = 0.2503, validation loss = 0.2549. Learning rate = 0.001. Patience = 1\n",
      "Epoch 11: training loss = 0.2353, validation loss = 0.2523. Learning rate = 0.001. Patience = 0\n",
      "Epoch 12: training loss = 0.2233, validation loss = 0.2567. Learning rate = 0.001. Patience = 1\n",
      "Epoch 13: training loss = 0.2365, validation loss = 0.2717. Learning rate = 0.001. Patience = 2\n",
      "Epoch 14: training loss = 0.2276, validation loss = 0.2529. Learning rate = 0.001. Patience = 3\n",
      "Epoch 15: training loss = 0.2295, validation loss = 0.2445. Learning rate = 0.001. Patience = 0\n",
      "Epoch 16: training loss = 0.2312, validation loss = 0.2524. Learning rate = 0.001. Patience = 1\n",
      "Epoch 17: training loss = 0.2291, validation loss = 0.2588. Learning rate = 0.001. Patience = 2\n",
      "Epoch 18: training loss = 0.2200, validation loss = 0.2707. Learning rate = 0.001. Patience = 3\n",
      "Epoch 19: training loss = 0.2225, validation loss = 0.2603. Learning rate = 0.001. Patience = 4\n",
      "Epoch 20: training loss = 0.2222, validation loss = 0.2552. Learning rate = 0.001. Patience = 5\n",
      "Epoch 21: training loss = 0.2203, validation loss = 0.2653. Learning rate = 0.001. Patience = 6\n",
      "Epoch 22: training loss = 0.2128, validation loss = 0.2612. Learning rate = 0.001. Patience = 7\n",
      "Epoch 23: training loss = 0.2212, validation loss = 0.2495. Learning rate = 0.001. Patience = 8\n",
      "Epoch 24: training loss = 0.2124, validation loss = 0.2608. Learning rate = 0.001. Patience = 9\n",
      "Epoch 25: training loss = 0.2091, validation loss = 0.2643. Learning rate = 0.001. Patience = 10\n",
      "Epoch 26: training loss = 0.2105, validation loss = 0.2604. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 27: training loss = 0.2116, validation loss = 0.2617. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 28: training loss = 0.2079, validation loss = 0.2588. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 29: training loss = 0.1993, validation loss = 0.2642. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 30: training loss = 0.2077, validation loss = 0.2560. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 31: training loss = 0.2010, validation loss = 0.2628. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 32: training loss = 0.2012, validation loss = 0.2630. Learning rate = 0.0005. Patience = 17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: training loss = 0.1999, validation loss = 0.2602. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 34: training loss = 0.2002, validation loss = 0.2627. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 35: training loss = 0.1971, validation loss = 0.2656. Learning rate = 0.0005. Patience = 20\n",
      "Epoch 36: training loss = 0.2063, validation loss = 0.2584. Learning rate = 0.0005. Patience = 21\n",
      "Epoch 37: training loss = 0.2004, validation loss = 0.2570. Learning rate = 0.00025. Patience = 22\n",
      "Epoch 38: training loss = 0.1979, validation loss = 0.2559. Learning rate = 0.00025. Patience = 23\n",
      "Epoch 39: training loss = 0.2001, validation loss = 0.2633. Learning rate = 0.00025. Patience = 24\n",
      "Epoch 40: training loss = 0.2007, validation loss = 0.2623. Learning rate = 0.00025. Patience = 25\n",
      "Epoch 41: training loss = 0.1987, validation loss = 0.2597. Learning rate = 0.00025. Patience = 26\n",
      "Epoch 42: training loss = 0.1905, validation loss = 0.2625. Learning rate = 0.00025. Patience = 27\n",
      "Epoch 43: training loss = 0.1938, validation loss = 0.2606. Learning rate = 0.00025. Patience = 28\n",
      "Epoch 44: training loss = 0.2051, validation loss = 0.2643. Learning rate = 0.00025. Patience = 29\n",
      "Epoch 45: training loss = 0.1922, validation loss = 0.2599. Learning rate = 0.00025. Patience = 30\n",
      "Epoch 46: training loss = 0.2047, validation loss = 0.2613. Learning rate = 0.00025. Patience = 31\n",
      "Epoch 47: training loss = 0.1906, validation loss = 0.2643. Learning rate = 0.00025. Patience = 32\n",
      "Epoch 48: training loss = 0.1958, validation loss = 0.2642. Learning rate = 0.000125. Patience = 33\n",
      "Epoch 49: training loss = 0.2017, validation loss = 0.2613. Learning rate = 0.000125. Patience = 34\n",
      "Epoch 50: training loss = 0.1975, validation loss = 0.2597. Learning rate = 0.000125. Patience = 35\n",
      "Epoch 51: training loss = 0.1962, validation loss = 0.2582. Learning rate = 0.000125. Patience = 36\n",
      "Epoch 52: training loss = 0.2004, validation loss = 0.2607. Learning rate = 0.000125. Patience = 37\n",
      "Epoch 53: training loss = 0.1932, validation loss = 0.2668. Learning rate = 0.000125. Patience = 38\n",
      "Epoch 54: training loss = 0.1872, validation loss = 0.2633. Learning rate = 0.000125. Patience = 39\n",
      "Epoch 55: training loss = 0.1868, validation loss = 0.2603. Learning rate = 0.000125. Patience = 40\n",
      "Epoch 56: training loss = 0.1985, validation loss = 0.2640. Learning rate = 0.000125. Patience = 41\n",
      "Epoch 57: training loss = 0.1883, validation loss = 0.2704. Learning rate = 0.000125. Patience = 42\n",
      "Epoch 58: training loss = 0.2076, validation loss = 0.2692. Learning rate = 0.000125. Patience = 43\n",
      "Epoch 59: training loss = 0.1949, validation loss = 0.2621. Learning rate = 6.25e-05. Patience = 44\n",
      "Epoch 60: training loss = 0.1858, validation loss = 0.2627. Learning rate = 6.25e-05. Patience = 45\n",
      "Epoch 61: training loss = 0.2032, validation loss = 0.2631. Learning rate = 6.25e-05. Patience = 46\n",
      "Epoch 62: training loss = 0.1919, validation loss = 0.2635. Learning rate = 6.25e-05. Patience = 47\n",
      "Epoch 63: training loss = 0.1924, validation loss = 0.2636. Learning rate = 6.25e-05. Patience = 48\n",
      "Epoch 64: training loss = 0.1940, validation loss = 0.2635. Learning rate = 6.25e-05. Patience = 49\n",
      "Epoch 65: training loss = 0.1914, validation loss = 0.2638. Learning rate = 6.25e-05. Patience = 50\n",
      "Training stopped at epoch 65; restoring weights from epoch 15\n",
      "Epoch 1: training loss = 0.8689, validation loss = 1.2649. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.7019, validation loss = 1.1096. Learning rate = 0.001. Patience = 0\n",
      "Epoch 3: training loss = 0.5133, validation loss = 1.0379. Learning rate = 0.001. Patience = 0\n",
      "Epoch 4: training loss = 0.5075, validation loss = 0.9724. Learning rate = 0.001. Patience = 0\n",
      "Epoch 5: training loss = 0.4480, validation loss = 0.9611. Learning rate = 0.001. Patience = 0\n",
      "Epoch 6: training loss = 0.4254, validation loss = 0.8845. Learning rate = 0.001. Patience = 0\n",
      "Epoch 7: training loss = 0.4513, validation loss = 0.8393. Learning rate = 0.001. Patience = 0\n",
      "Epoch 8: training loss = 0.3849, validation loss = 0.9459. Learning rate = 0.001. Patience = 1\n",
      "Epoch 9: training loss = 0.3480, validation loss = 0.7164. Learning rate = 0.001. Patience = 0\n",
      "Epoch 10: training loss = 0.3073, validation loss = 0.7779. Learning rate = 0.001. Patience = 1\n",
      "Epoch 11: training loss = 0.2851, validation loss = 0.6666. Learning rate = 0.001. Patience = 0\n",
      "Epoch 12: training loss = 0.2520, validation loss = 0.6685. Learning rate = 0.001. Patience = 1\n",
      "Epoch 13: training loss = 0.2238, validation loss = 0.6904. Learning rate = 0.001. Patience = 2\n",
      "Epoch 14: training loss = 0.2051, validation loss = 0.7555. Learning rate = 0.001. Patience = 3\n",
      "Epoch 15: training loss = 0.1719, validation loss = 0.7801. Learning rate = 0.001. Patience = 4\n",
      "Epoch 16: training loss = 0.1689, validation loss = 0.6734. Learning rate = 0.001. Patience = 5\n",
      "Epoch 17: training loss = 0.1656, validation loss = 0.6867. Learning rate = 0.001. Patience = 6\n",
      "Epoch 18: training loss = 0.1457, validation loss = 0.8423. Learning rate = 0.001. Patience = 7\n",
      "Epoch 19: training loss = 0.1660, validation loss = 0.6852. Learning rate = 0.001. Patience = 8\n",
      "Epoch 20: training loss = 0.1484, validation loss = 0.5763. Learning rate = 0.001. Patience = 0\n",
      "Epoch 21: training loss = 0.1457, validation loss = 0.7557. Learning rate = 0.001. Patience = 1\n",
      "Epoch 22: training loss = 0.1228, validation loss = 0.5880. Learning rate = 0.001. Patience = 2\n",
      "Epoch 23: training loss = 0.1123, validation loss = 0.6649. Learning rate = 0.001. Patience = 3\n",
      "Epoch 24: training loss = 0.1138, validation loss = 0.6127. Learning rate = 0.001. Patience = 4\n",
      "Epoch 25: training loss = 0.1191, validation loss = 0.8065. Learning rate = 0.001. Patience = 5\n",
      "Epoch 26: training loss = 0.0988, validation loss = 0.5099. Learning rate = 0.001. Patience = 0\n",
      "Epoch 27: training loss = 0.1218, validation loss = 0.8030. Learning rate = 0.001. Patience = 1\n",
      "Epoch 28: training loss = 0.1059, validation loss = 0.6543. Learning rate = 0.001. Patience = 2\n",
      "Epoch 29: training loss = 0.1009, validation loss = 0.5965. Learning rate = 0.001. Patience = 3\n",
      "Epoch 30: training loss = 0.0947, validation loss = 0.8207. Learning rate = 0.001. Patience = 4\n",
      "Epoch 31: training loss = 0.0934, validation loss = 0.6536. Learning rate = 0.001. Patience = 5\n",
      "Epoch 32: training loss = 0.0841, validation loss = 0.7268. Learning rate = 0.001. Patience = 6\n",
      "Epoch 33: training loss = 0.0939, validation loss = 0.7289. Learning rate = 0.001. Patience = 7\n",
      "Epoch 34: training loss = 0.0806, validation loss = 0.7744. Learning rate = 0.001. Patience = 8\n",
      "Epoch 35: training loss = 0.0993, validation loss = 0.6947. Learning rate = 0.001. Patience = 9\n",
      "Epoch 36: training loss = 0.0923, validation loss = 0.7244. Learning rate = 0.001. Patience = 10\n",
      "Epoch 37: training loss = 0.0929, validation loss = 0.7375. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 38: training loss = 0.0744, validation loss = 0.6821. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 39: training loss = 0.0798, validation loss = 0.7334. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 40: training loss = 0.0728, validation loss = 0.7559. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 41: training loss = 0.0691, validation loss = 0.7138. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 42: training loss = 0.0683, validation loss = 0.7073. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 43: training loss = 0.0756, validation loss = 0.7385. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 44: training loss = 0.0760, validation loss = 0.7443. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 45: training loss = 0.0611, validation loss = 0.6660. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 46: training loss = 0.0610, validation loss = 0.7209. Learning rate = 0.0005. Patience = 20\n",
      "Epoch 47: training loss = 0.0662, validation loss = 0.7448. Learning rate = 0.0005. Patience = 21\n",
      "Epoch 48: training loss = 0.0759, validation loss = 0.7618. Learning rate = 0.00025. Patience = 22\n",
      "Epoch 49: training loss = 0.0691, validation loss = 0.7345. Learning rate = 0.00025. Patience = 23\n",
      "Epoch 50: training loss = 0.0712, validation loss = 0.7093. Learning rate = 0.00025. Patience = 24\n",
      "Epoch 51: training loss = 0.0624, validation loss = 0.7722. Learning rate = 0.00025. Patience = 25\n",
      "Epoch 52: training loss = 0.0666, validation loss = 0.7156. Learning rate = 0.00025. Patience = 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: training loss = 0.0610, validation loss = 0.6806. Learning rate = 0.00025. Patience = 27\n",
      "Epoch 54: training loss = 0.0783, validation loss = 0.6986. Learning rate = 0.00025. Patience = 28\n",
      "Epoch 55: training loss = 0.0628, validation loss = 0.7330. Learning rate = 0.00025. Patience = 29\n",
      "Epoch 56: training loss = 0.0692, validation loss = 0.7419. Learning rate = 0.00025. Patience = 30\n",
      "Epoch 57: training loss = 0.0771, validation loss = 0.7280. Learning rate = 0.00025. Patience = 31\n",
      "Epoch 58: training loss = 0.0584, validation loss = 0.7138. Learning rate = 0.00025. Patience = 32\n",
      "Epoch 59: training loss = 0.0632, validation loss = 0.7664. Learning rate = 0.000125. Patience = 33\n",
      "Epoch 60: training loss = 0.0630, validation loss = 0.7530. Learning rate = 0.000125. Patience = 34\n",
      "Epoch 61: training loss = 0.0677, validation loss = 0.7454. Learning rate = 0.000125. Patience = 35\n",
      "Epoch 62: training loss = 0.0637, validation loss = 0.7367. Learning rate = 0.000125. Patience = 36\n",
      "Epoch 63: training loss = 0.0572, validation loss = 0.7315. Learning rate = 0.000125. Patience = 37\n",
      "Epoch 64: training loss = 0.0683, validation loss = 0.7540. Learning rate = 0.000125. Patience = 38\n",
      "Epoch 65: training loss = 0.0622, validation loss = 0.7129. Learning rate = 0.000125. Patience = 39\n",
      "Epoch 66: training loss = 0.0627, validation loss = 0.7097. Learning rate = 0.000125. Patience = 40\n",
      "Epoch 67: training loss = 0.0655, validation loss = 0.7161. Learning rate = 0.000125. Patience = 41\n",
      "Epoch 68: training loss = 0.0613, validation loss = 0.7070. Learning rate = 0.000125. Patience = 42\n",
      "Epoch 69: training loss = 0.0742, validation loss = 0.7172. Learning rate = 0.000125. Patience = 43\n",
      "Epoch 70: training loss = 0.0660, validation loss = 0.7134. Learning rate = 6.25e-05. Patience = 44\n",
      "Epoch 71: training loss = 0.0677, validation loss = 0.7338. Learning rate = 6.25e-05. Patience = 45\n",
      "Epoch 72: training loss = 0.0559, validation loss = 0.7404. Learning rate = 6.25e-05. Patience = 46\n",
      "Epoch 73: training loss = 0.0720, validation loss = 0.7378. Learning rate = 6.25e-05. Patience = 47\n",
      "Epoch 74: training loss = 0.0576, validation loss = 0.7448. Learning rate = 6.25e-05. Patience = 48\n",
      "Epoch 75: training loss = 0.0618, validation loss = 0.7532. Learning rate = 6.25e-05. Patience = 49\n",
      "Epoch 76: training loss = 0.0576, validation loss = 0.7469. Learning rate = 6.25e-05. Patience = 50\n",
      "Training stopped at epoch 76; restoring weights from epoch 26\n",
      "Epoch 1: training loss = 0.8769, validation loss = 1.8247. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.5722, validation loss = 1.2246. Learning rate = 0.001. Patience = 0\n",
      "Epoch 3: training loss = 0.4603, validation loss = 1.0230. Learning rate = 0.001. Patience = 0\n",
      "Epoch 4: training loss = 0.4165, validation loss = 1.0628. Learning rate = 0.001. Patience = 1\n",
      "Epoch 5: training loss = 0.4230, validation loss = 1.0989. Learning rate = 0.001. Patience = 2\n",
      "Epoch 6: training loss = 0.4007, validation loss = 1.0838. Learning rate = 0.001. Patience = 3\n",
      "Epoch 7: training loss = 0.3836, validation loss = 1.0555. Learning rate = 0.001. Patience = 4\n",
      "Epoch 8: training loss = 0.3701, validation loss = 1.0536. Learning rate = 0.001. Patience = 5\n",
      "Epoch 9: training loss = 0.3734, validation loss = 1.0670. Learning rate = 0.001. Patience = 6\n",
      "Epoch 10: training loss = 0.3698, validation loss = 1.0931. Learning rate = 0.001. Patience = 7\n",
      "Epoch 11: training loss = 0.3419, validation loss = 1.0618. Learning rate = 0.001. Patience = 8\n",
      "Epoch 12: training loss = 0.3550, validation loss = 1.0950. Learning rate = 0.001. Patience = 9\n",
      "Epoch 13: training loss = 0.3246, validation loss = 1.2400. Learning rate = 0.001. Patience = 10\n",
      "Epoch 14: training loss = 0.3156, validation loss = 1.1022. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 15: training loss = 0.3238, validation loss = 1.1209. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 16: training loss = 0.3083, validation loss = 1.1336. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 17: training loss = 0.3060, validation loss = 1.2157. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 18: training loss = 0.3063, validation loss = 1.2125. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 19: training loss = 0.2933, validation loss = 1.2755. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 20: training loss = 0.3017, validation loss = 1.1548. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 21: training loss = 0.2864, validation loss = 1.3415. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 22: training loss = 0.2872, validation loss = 1.2683. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 23: training loss = 0.2927, validation loss = 1.2700. Learning rate = 0.0005. Patience = 20\n",
      "Epoch 24: training loss = 0.2768, validation loss = 1.3686. Learning rate = 0.0005. Patience = 21\n",
      "Epoch 25: training loss = 0.2715, validation loss = 1.3508. Learning rate = 0.00025. Patience = 22\n",
      "Epoch 26: training loss = 0.2684, validation loss = 1.3338. Learning rate = 0.00025. Patience = 23\n",
      "Epoch 27: training loss = 0.2670, validation loss = 1.3151. Learning rate = 0.00025. Patience = 24\n",
      "Epoch 28: training loss = 0.2593, validation loss = 1.3565. Learning rate = 0.00025. Patience = 25\n",
      "Epoch 29: training loss = 0.2756, validation loss = 1.4778. Learning rate = 0.00025. Patience = 26\n",
      "Epoch 30: training loss = 0.2708, validation loss = 1.3519. Learning rate = 0.00025. Patience = 27\n",
      "Epoch 31: training loss = 0.2563, validation loss = 1.4149. Learning rate = 0.00025. Patience = 28\n",
      "Epoch 32: training loss = 0.2614, validation loss = 1.5034. Learning rate = 0.00025. Patience = 29\n",
      "Epoch 33: training loss = 0.2562, validation loss = 1.4234. Learning rate = 0.00025. Patience = 30\n",
      "Epoch 34: training loss = 0.2672, validation loss = 1.3562. Learning rate = 0.00025. Patience = 31\n",
      "Epoch 35: training loss = 0.2513, validation loss = 1.5641. Learning rate = 0.00025. Patience = 32\n",
      "Epoch 36: training loss = 0.2561, validation loss = 1.5348. Learning rate = 0.000125. Patience = 33\n",
      "Epoch 37: training loss = 0.2477, validation loss = 1.5291. Learning rate = 0.000125. Patience = 34\n",
      "Epoch 38: training loss = 0.2524, validation loss = 1.5205. Learning rate = 0.000125. Patience = 35\n",
      "Epoch 39: training loss = 0.2479, validation loss = 1.4956. Learning rate = 0.000125. Patience = 36\n",
      "Epoch 40: training loss = 0.2517, validation loss = 1.5197. Learning rate = 0.000125. Patience = 37\n",
      "Epoch 41: training loss = 0.2637, validation loss = 1.5252. Learning rate = 0.000125. Patience = 38\n",
      "Epoch 42: training loss = 0.2430, validation loss = 1.4799. Learning rate = 0.000125. Patience = 39\n",
      "Epoch 43: training loss = 0.2432, validation loss = 1.4994. Learning rate = 0.000125. Patience = 40\n",
      "Epoch 44: training loss = 0.2446, validation loss = 1.5247. Learning rate = 0.000125. Patience = 41\n",
      "Epoch 45: training loss = 0.2463, validation loss = 1.5203. Learning rate = 0.000125. Patience = 42\n",
      "Epoch 46: training loss = 0.2458, validation loss = 1.5942. Learning rate = 0.000125. Patience = 43\n",
      "Epoch 47: training loss = 0.2464, validation loss = 1.5832. Learning rate = 6.25e-05. Patience = 44\n",
      "Epoch 48: training loss = 0.2428, validation loss = 1.5862. Learning rate = 6.25e-05. Patience = 45\n",
      "Epoch 49: training loss = 0.2430, validation loss = 1.5775. Learning rate = 6.25e-05. Patience = 46\n",
      "Epoch 50: training loss = 0.2393, validation loss = 1.5424. Learning rate = 6.25e-05. Patience = 47\n",
      "Epoch 51: training loss = 0.2396, validation loss = 1.5470. Learning rate = 6.25e-05. Patience = 48\n",
      "Epoch 52: training loss = 0.2547, validation loss = 1.5494. Learning rate = 6.25e-05. Patience = 49\n",
      "Epoch 53: training loss = 0.2505, validation loss = 1.5675. Learning rate = 6.25e-05. Patience = 50\n",
      "Training stopped at epoch 53; restoring weights from epoch 3\n",
      "Epoch 1: training loss = 0.9470, validation loss = 0.9665. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.7030, validation loss = 0.8554. Learning rate = 0.001. Patience = 0\n",
      "Epoch 3: training loss = 0.6153, validation loss = 0.8422. Learning rate = 0.001. Patience = 0\n",
      "Epoch 4: training loss = 0.5533, validation loss = 0.8106. Learning rate = 0.001. Patience = 0\n",
      "Epoch 5: training loss = 0.5245, validation loss = 0.7830. Learning rate = 0.001. Patience = 0\n",
      "Epoch 6: training loss = 0.4716, validation loss = 0.7660. Learning rate = 0.001. Patience = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: training loss = 0.4399, validation loss = 0.7284. Learning rate = 0.001. Patience = 0\n",
      "Epoch 8: training loss = 0.4976, validation loss = 0.7325. Learning rate = 0.001. Patience = 1\n",
      "Epoch 9: training loss = 0.4142, validation loss = 0.7083. Learning rate = 0.001. Patience = 0\n",
      "Epoch 10: training loss = 0.4005, validation loss = 0.7060. Learning rate = 0.001. Patience = 0\n",
      "Epoch 11: training loss = 0.3847, validation loss = 0.6869. Learning rate = 0.001. Patience = 0\n",
      "Epoch 12: training loss = 0.3708, validation loss = 0.6946. Learning rate = 0.001. Patience = 1\n",
      "Epoch 13: training loss = 0.3660, validation loss = 0.6780. Learning rate = 0.001. Patience = 0\n",
      "Epoch 14: training loss = 0.3593, validation loss = 0.6830. Learning rate = 0.001. Patience = 1\n",
      "Epoch 15: training loss = 0.3892, validation loss = 0.6773. Learning rate = 0.001. Patience = 0\n",
      "Epoch 16: training loss = 0.3564, validation loss = 0.6559. Learning rate = 0.001. Patience = 0\n",
      "Epoch 17: training loss = 0.3542, validation loss = 0.6496. Learning rate = 0.001. Patience = 0\n",
      "Epoch 18: training loss = 0.3288, validation loss = 0.6360. Learning rate = 0.001. Patience = 0\n",
      "Epoch 19: training loss = 0.3217, validation loss = 0.6243. Learning rate = 0.001. Patience = 0\n",
      "Epoch 20: training loss = 0.3055, validation loss = 0.6186. Learning rate = 0.001. Patience = 0\n",
      "Epoch 21: training loss = 0.2883, validation loss = 0.6260. Learning rate = 0.001. Patience = 1\n",
      "Epoch 22: training loss = 0.3036, validation loss = 0.6066. Learning rate = 0.001. Patience = 0\n",
      "Epoch 23: training loss = 0.2993, validation loss = 0.6167. Learning rate = 0.001. Patience = 1\n",
      "Epoch 24: training loss = 0.3416, validation loss = 0.6350. Learning rate = 0.001. Patience = 2\n",
      "Epoch 25: training loss = 0.3032, validation loss = 0.6541. Learning rate = 0.001. Patience = 3\n",
      "Epoch 26: training loss = 0.3015, validation loss = 0.5866. Learning rate = 0.001. Patience = 0\n",
      "Epoch 27: training loss = 0.2865, validation loss = 0.5955. Learning rate = 0.001. Patience = 1\n",
      "Epoch 28: training loss = 0.2598, validation loss = 0.5939. Learning rate = 0.001. Patience = 2\n",
      "Epoch 29: training loss = 0.2608, validation loss = 0.5940. Learning rate = 0.001. Patience = 3\n",
      "Epoch 30: training loss = 0.2708, validation loss = 0.5962. Learning rate = 0.001. Patience = 4\n",
      "Epoch 31: training loss = 0.2648, validation loss = 0.5797. Learning rate = 0.001. Patience = 0\n",
      "Epoch 32: training loss = 0.2509, validation loss = 0.5841. Learning rate = 0.001. Patience = 1\n",
      "Epoch 33: training loss = 0.2656, validation loss = 0.5807. Learning rate = 0.001. Patience = 2\n",
      "Epoch 34: training loss = 0.2735, validation loss = 0.5931. Learning rate = 0.001. Patience = 3\n",
      "Epoch 35: training loss = 0.2930, validation loss = 0.5986. Learning rate = 0.001. Patience = 4\n",
      "Epoch 36: training loss = 0.2545, validation loss = 0.6021. Learning rate = 0.001. Patience = 5\n",
      "Epoch 37: training loss = 0.2782, validation loss = 0.5727. Learning rate = 0.001. Patience = 0\n",
      "Epoch 38: training loss = 0.2430, validation loss = 0.5690. Learning rate = 0.001. Patience = 0\n",
      "Epoch 39: training loss = 0.2475, validation loss = 0.5719. Learning rate = 0.001. Patience = 1\n",
      "Epoch 40: training loss = 0.2430, validation loss = 0.5706. Learning rate = 0.001. Patience = 2\n",
      "Epoch 41: training loss = 0.2339, validation loss = 0.5835. Learning rate = 0.001. Patience = 3\n",
      "Epoch 42: training loss = 0.2301, validation loss = 0.5722. Learning rate = 0.001. Patience = 4\n",
      "Epoch 43: training loss = 0.2465, validation loss = 0.5646. Learning rate = 0.001. Patience = 0\n",
      "Epoch 44: training loss = 0.2485, validation loss = 0.5836. Learning rate = 0.001. Patience = 1\n",
      "Epoch 45: training loss = 0.2307, validation loss = 0.5661. Learning rate = 0.001. Patience = 2\n",
      "Epoch 46: training loss = 0.2286, validation loss = 0.5691. Learning rate = 0.001. Patience = 3\n",
      "Epoch 47: training loss = 0.2312, validation loss = 0.5651. Learning rate = 0.001. Patience = 4\n",
      "Epoch 48: training loss = 0.2211, validation loss = 0.5699. Learning rate = 0.001. Patience = 5\n",
      "Epoch 49: training loss = 0.2257, validation loss = 0.5359. Learning rate = 0.001. Patience = 0\n",
      "Epoch 50: training loss = 0.2191, validation loss = 0.5521. Learning rate = 0.001. Patience = 1\n",
      "Epoch 51: training loss = 0.2101, validation loss = 0.5625. Learning rate = 0.001. Patience = 2\n",
      "Epoch 52: training loss = 0.2188, validation loss = 0.5397. Learning rate = 0.001. Patience = 3\n",
      "Epoch 53: training loss = 0.2093, validation loss = 0.5584. Learning rate = 0.001. Patience = 4\n",
      "Epoch 54: training loss = 0.2119, validation loss = 0.5716. Learning rate = 0.001. Patience = 5\n",
      "Epoch 55: training loss = 0.2512, validation loss = 0.5879. Learning rate = 0.001. Patience = 6\n",
      "Epoch 56: training loss = 0.1995, validation loss = 0.5473. Learning rate = 0.001. Patience = 7\n",
      "Epoch 57: training loss = 0.2163, validation loss = 0.5583. Learning rate = 0.001. Patience = 8\n",
      "Epoch 58: training loss = 0.2105, validation loss = 0.5460. Learning rate = 0.001. Patience = 9\n",
      "Epoch 59: training loss = 0.2096, validation loss = 0.5471. Learning rate = 0.001. Patience = 10\n",
      "Epoch 60: training loss = 0.2129, validation loss = 0.5787. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 61: training loss = 0.2472, validation loss = 0.5594. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 62: training loss = 0.2389, validation loss = 0.5584. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 63: training loss = 0.2013, validation loss = 0.5477. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 64: training loss = 0.2028, validation loss = 0.5394. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 65: training loss = 0.1957, validation loss = 0.5471. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 66: training loss = 0.2038, validation loss = 0.5556. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 67: training loss = 0.2042, validation loss = 0.5430. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 68: training loss = 0.2323, validation loss = 0.5486. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 69: training loss = 0.1988, validation loss = 0.5534. Learning rate = 0.0005. Patience = 20\n",
      "Epoch 70: training loss = 0.1980, validation loss = 0.5638. Learning rate = 0.0005. Patience = 21\n",
      "Epoch 71: training loss = 0.1972, validation loss = 0.5497. Learning rate = 0.00025. Patience = 22\n",
      "Epoch 72: training loss = 0.2002, validation loss = 0.5430. Learning rate = 0.00025. Patience = 23\n",
      "Epoch 73: training loss = 0.1887, validation loss = 0.5411. Learning rate = 0.00025. Patience = 24\n",
      "Epoch 74: training loss = 0.1894, validation loss = 0.5485. Learning rate = 0.00025. Patience = 25\n",
      "Epoch 75: training loss = 0.1961, validation loss = 0.5381. Learning rate = 0.00025. Patience = 26\n",
      "Epoch 76: training loss = 0.2080, validation loss = 0.5416. Learning rate = 0.00025. Patience = 27\n",
      "Epoch 77: training loss = 0.1909, validation loss = 0.5427. Learning rate = 0.00025. Patience = 28\n",
      "Epoch 78: training loss = 0.1844, validation loss = 0.5324. Learning rate = 0.00025. Patience = 0\n",
      "Epoch 79: training loss = 0.2005, validation loss = 0.5400. Learning rate = 0.00025. Patience = 1\n",
      "Epoch 80: training loss = 0.1946, validation loss = 0.5547. Learning rate = 0.00025. Patience = 2\n",
      "Epoch 81: training loss = 0.2294, validation loss = 0.5372. Learning rate = 0.00025. Patience = 3\n",
      "Epoch 82: training loss = 0.1885, validation loss = 0.5418. Learning rate = 0.00025. Patience = 4\n",
      "Epoch 83: training loss = 0.1878, validation loss = 0.5562. Learning rate = 0.00025. Patience = 5\n",
      "Epoch 84: training loss = 0.1895, validation loss = 0.5411. Learning rate = 0.00025. Patience = 6\n",
      "Epoch 85: training loss = 0.1873, validation loss = 0.5374. Learning rate = 0.00025. Patience = 7\n",
      "Epoch 86: training loss = 0.1940, validation loss = 0.5476. Learning rate = 0.00025. Patience = 8\n",
      "Epoch 87: training loss = 0.1851, validation loss = 0.5410. Learning rate = 0.00025. Patience = 9\n",
      "Epoch 88: training loss = 0.1897, validation loss = 0.5354. Learning rate = 0.00025. Patience = 10\n",
      "Epoch 89: training loss = 0.1807, validation loss = 0.5402. Learning rate = 0.000125. Patience = 11\n",
      "Epoch 90: training loss = 0.1934, validation loss = 0.5380. Learning rate = 0.000125. Patience = 12\n",
      "Epoch 91: training loss = 0.1817, validation loss = 0.5418. Learning rate = 0.000125. Patience = 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92: training loss = 0.1922, validation loss = 0.5378. Learning rate = 0.000125. Patience = 14\n",
      "Epoch 93: training loss = 0.2147, validation loss = 0.5359. Learning rate = 0.000125. Patience = 15\n",
      "Epoch 94: training loss = 0.1786, validation loss = 0.5327. Learning rate = 0.000125. Patience = 16\n",
      "Epoch 95: training loss = 0.1841, validation loss = 0.5365. Learning rate = 0.000125. Patience = 17\n",
      "Epoch 96: training loss = 0.1893, validation loss = 0.5414. Learning rate = 0.000125. Patience = 18\n",
      "Epoch 97: training loss = 0.1818, validation loss = 0.5418. Learning rate = 0.000125. Patience = 19\n",
      "Epoch 98: training loss = 0.1911, validation loss = 0.5397. Learning rate = 0.000125. Patience = 20\n",
      "Epoch 99: training loss = 0.1812, validation loss = 0.5414. Learning rate = 0.000125. Patience = 21\n",
      "Epoch 100: training loss = 0.1872, validation loss = 0.5379. Learning rate = 6.25e-05. Patience = 22\n",
      "Epoch 101: training loss = 0.1802, validation loss = 0.5379. Learning rate = 6.25e-05. Patience = 23\n",
      "Epoch 102: training loss = 0.1867, validation loss = 0.5395. Learning rate = 6.25e-05. Patience = 24\n",
      "Epoch 103: training loss = 0.1785, validation loss = 0.5422. Learning rate = 6.25e-05. Patience = 25\n",
      "Epoch 104: training loss = 0.2164, validation loss = 0.5399. Learning rate = 6.25e-05. Patience = 26\n",
      "Epoch 105: training loss = 0.1801, validation loss = 0.5391. Learning rate = 6.25e-05. Patience = 27\n",
      "Epoch 106: training loss = 0.1806, validation loss = 0.5392. Learning rate = 6.25e-05. Patience = 28\n",
      "Epoch 107: training loss = 0.1774, validation loss = 0.5421. Learning rate = 6.25e-05. Patience = 29\n",
      "Epoch 108: training loss = 0.1882, validation loss = 0.5444. Learning rate = 6.25e-05. Patience = 30\n",
      "Epoch 109: training loss = 0.1851, validation loss = 0.5462. Learning rate = 6.25e-05. Patience = 31\n",
      "Epoch 110: training loss = 0.1885, validation loss = 0.5432. Learning rate = 6.25e-05. Patience = 32\n",
      "Epoch 111: training loss = 0.2110, validation loss = 0.5432. Learning rate = 3.125e-05. Patience = 33\n",
      "Epoch 112: training loss = 0.1815, validation loss = 0.5433. Learning rate = 3.125e-05. Patience = 34\n",
      "Epoch 113: training loss = 0.1825, validation loss = 0.5423. Learning rate = 3.125e-05. Patience = 35\n",
      "Epoch 114: training loss = 0.1815, validation loss = 0.5411. Learning rate = 3.125e-05. Patience = 36\n",
      "Epoch 115: training loss = 0.2072, validation loss = 0.5403. Learning rate = 3.125e-05. Patience = 37\n",
      "Epoch 116: training loss = 0.2399, validation loss = 0.5410. Learning rate = 3.125e-05. Patience = 38\n",
      "Epoch 117: training loss = 0.1813, validation loss = 0.5414. Learning rate = 3.125e-05. Patience = 39\n",
      "Epoch 118: training loss = 0.2064, validation loss = 0.5416. Learning rate = 3.125e-05. Patience = 40\n",
      "Epoch 119: training loss = 0.1808, validation loss = 0.5414. Learning rate = 3.125e-05. Patience = 41\n",
      "Epoch 120: training loss = 0.1869, validation loss = 0.5399. Learning rate = 3.125e-05. Patience = 42\n",
      "Epoch 121: training loss = 0.1780, validation loss = 0.5406. Learning rate = 3.125e-05. Patience = 43\n",
      "Epoch 122: training loss = 0.1845, validation loss = 0.5403. Learning rate = 1.5625e-05. Patience = 44\n",
      "Epoch 123: training loss = 0.1863, validation loss = 0.5404. Learning rate = 1.5625e-05. Patience = 45\n",
      "Epoch 124: training loss = 0.1853, validation loss = 0.5394. Learning rate = 1.5625e-05. Patience = 46\n",
      "Epoch 125: training loss = 0.1792, validation loss = 0.5391. Learning rate = 1.5625e-05. Patience = 47\n",
      "Epoch 126: training loss = 0.1824, validation loss = 0.5392. Learning rate = 1.5625e-05. Patience = 48\n",
      "Epoch 127: training loss = 0.1783, validation loss = 0.5401. Learning rate = 1.5625e-05. Patience = 49\n",
      "Epoch 128: training loss = 0.1797, validation loss = 0.5408. Learning rate = 1.5625e-05. Patience = 50\n",
      "Training stopped at epoch 128; restoring weights from epoch 78\n",
      "Epoch 1: training loss = 0.8552, validation loss = 1.0381. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.7190, validation loss = 0.8942. Learning rate = 0.001. Patience = 0\n",
      "Epoch 3: training loss = 0.6411, validation loss = 0.9213. Learning rate = 0.001. Patience = 1\n",
      "Epoch 4: training loss = 0.6149, validation loss = 0.9007. Learning rate = 0.001. Patience = 2\n",
      "Epoch 5: training loss = 0.6072, validation loss = 0.9145. Learning rate = 0.001. Patience = 3\n",
      "Epoch 6: training loss = 0.5763, validation loss = 0.9367. Learning rate = 0.001. Patience = 4\n",
      "Epoch 7: training loss = 0.5665, validation loss = 0.9479. Learning rate = 0.001. Patience = 5\n",
      "Epoch 8: training loss = 0.5739, validation loss = 0.9166. Learning rate = 0.001. Patience = 6\n",
      "Epoch 9: training loss = 0.5595, validation loss = 0.9278. Learning rate = 0.001. Patience = 7\n",
      "Epoch 10: training loss = 0.5686, validation loss = 0.9137. Learning rate = 0.001. Patience = 8\n",
      "Epoch 11: training loss = 0.5528, validation loss = 0.9338. Learning rate = 0.001. Patience = 9\n",
      "Epoch 12: training loss = 0.5489, validation loss = 0.9648. Learning rate = 0.001. Patience = 10\n",
      "Epoch 13: training loss = 0.5317, validation loss = 0.9258. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 14: training loss = 0.5446, validation loss = 0.9436. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 15: training loss = 0.5204, validation loss = 0.9403. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 16: training loss = 0.5240, validation loss = 0.9422. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 17: training loss = 0.5666, validation loss = 0.9106. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 18: training loss = 0.5290, validation loss = 0.9186. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 19: training loss = 0.5190, validation loss = 0.9254. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 20: training loss = 0.5270, validation loss = 0.9323. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 21: training loss = 0.5434, validation loss = 0.9187. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 22: training loss = 0.5340, validation loss = 0.9127. Learning rate = 0.0005. Patience = 20\n",
      "Epoch 23: training loss = 0.5143, validation loss = 0.9317. Learning rate = 0.0005. Patience = 21\n",
      "Epoch 24: training loss = 0.5257, validation loss = 0.9519. Learning rate = 0.00025. Patience = 22\n",
      "Epoch 25: training loss = 0.5036, validation loss = 0.9347. Learning rate = 0.00025. Patience = 23\n",
      "Epoch 26: training loss = 0.5152, validation loss = 0.9250. Learning rate = 0.00025. Patience = 24\n",
      "Epoch 27: training loss = 0.4955, validation loss = 0.9301. Learning rate = 0.00025. Patience = 25\n",
      "Epoch 28: training loss = 0.4997, validation loss = 0.9530. Learning rate = 0.00025. Patience = 26\n",
      "Epoch 29: training loss = 0.5072, validation loss = 0.9456. Learning rate = 0.00025. Patience = 27\n",
      "Epoch 30: training loss = 0.5309, validation loss = 0.9368. Learning rate = 0.00025. Patience = 28\n",
      "Epoch 31: training loss = 0.5027, validation loss = 0.9204. Learning rate = 0.00025. Patience = 29\n",
      "Epoch 32: training loss = 0.5006, validation loss = 0.9219. Learning rate = 0.00025. Patience = 30\n",
      "Epoch 33: training loss = 0.5094, validation loss = 0.9342. Learning rate = 0.00025. Patience = 31\n",
      "Epoch 34: training loss = 0.5052, validation loss = 0.9420. Learning rate = 0.00025. Patience = 32\n",
      "Epoch 35: training loss = 0.5062, validation loss = 0.9405. Learning rate = 0.000125. Patience = 33\n",
      "Epoch 36: training loss = 0.5051, validation loss = 0.9452. Learning rate = 0.000125. Patience = 34\n",
      "Epoch 37: training loss = 0.4844, validation loss = 0.9321. Learning rate = 0.000125. Patience = 35\n",
      "Epoch 38: training loss = 0.4895, validation loss = 0.9305. Learning rate = 0.000125. Patience = 36\n",
      "Epoch 39: training loss = 0.5120, validation loss = 0.9325. Learning rate = 0.000125. Patience = 37\n",
      "Epoch 40: training loss = 0.4925, validation loss = 0.9317. Learning rate = 0.000125. Patience = 38\n",
      "Epoch 41: training loss = 0.4957, validation loss = 0.9348. Learning rate = 0.000125. Patience = 39\n",
      "Epoch 42: training loss = 0.5084, validation loss = 0.9339. Learning rate = 0.000125. Patience = 40\n",
      "Epoch 43: training loss = 0.4961, validation loss = 0.9346. Learning rate = 0.000125. Patience = 41\n",
      "Epoch 44: training loss = 0.4977, validation loss = 0.9287. Learning rate = 0.000125. Patience = 42\n",
      "Epoch 45: training loss = 0.4843, validation loss = 0.9264. Learning rate = 0.000125. Patience = 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: training loss = 0.4973, validation loss = 0.9278. Learning rate = 6.25e-05. Patience = 44\n",
      "Epoch 47: training loss = 0.4920, validation loss = 0.9276. Learning rate = 6.25e-05. Patience = 45\n",
      "Epoch 48: training loss = 0.4909, validation loss = 0.9301. Learning rate = 6.25e-05. Patience = 46\n",
      "Epoch 49: training loss = 0.4816, validation loss = 0.9295. Learning rate = 6.25e-05. Patience = 47\n",
      "Epoch 50: training loss = 0.5069, validation loss = 0.9324. Learning rate = 6.25e-05. Patience = 48\n",
      "Epoch 51: training loss = 0.4918, validation loss = 0.9327. Learning rate = 6.25e-05. Patience = 49\n",
      "Epoch 52: training loss = 0.5055, validation loss = 0.9322. Learning rate = 6.25e-05. Patience = 50\n",
      "Training stopped at epoch 52; restoring weights from epoch 2\n",
      "Epoch 1: training loss = 0.8541, validation loss = 0.3981. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.3533, validation loss = 0.3889. Learning rate = 0.001. Patience = 0\n",
      "Epoch 3: training loss = 0.3094, validation loss = 0.4119. Learning rate = 0.001. Patience = 1\n",
      "Epoch 4: training loss = 0.2793, validation loss = 0.3689. Learning rate = 0.001. Patience = 0\n",
      "Epoch 5: training loss = 0.2552, validation loss = 0.3627. Learning rate = 0.001. Patience = 0\n",
      "Epoch 6: training loss = 0.2395, validation loss = 0.3504. Learning rate = 0.001. Patience = 0\n",
      "Epoch 7: training loss = 0.2695, validation loss = 0.3373. Learning rate = 0.001. Patience = 0\n",
      "Epoch 8: training loss = 0.2428, validation loss = 0.3320. Learning rate = 0.001. Patience = 0\n",
      "Epoch 9: training loss = 0.2434, validation loss = 0.3231. Learning rate = 0.001. Patience = 0\n",
      "Epoch 10: training loss = 0.2466, validation loss = 0.3175. Learning rate = 0.001. Patience = 0\n",
      "Epoch 11: training loss = 0.2538, validation loss = 0.3182. Learning rate = 0.001. Patience = 1\n",
      "Epoch 12: training loss = 0.2370, validation loss = 0.3150. Learning rate = 0.001. Patience = 0\n",
      "Epoch 13: training loss = 0.2378, validation loss = 0.3141. Learning rate = 0.001. Patience = 0\n",
      "Epoch 14: training loss = 0.2339, validation loss = 0.3117. Learning rate = 0.001. Patience = 0\n",
      "Epoch 15: training loss = 0.2317, validation loss = 0.3117. Learning rate = 0.001. Patience = 1\n",
      "Epoch 16: training loss = 0.2327, validation loss = 0.3130. Learning rate = 0.001. Patience = 2\n",
      "Epoch 17: training loss = 0.2368, validation loss = 0.3079. Learning rate = 0.001. Patience = 0\n",
      "Epoch 18: training loss = 0.2370, validation loss = 0.3080. Learning rate = 0.001. Patience = 1\n",
      "Epoch 19: training loss = 0.2485, validation loss = 0.3075. Learning rate = 0.001. Patience = 0\n",
      "Epoch 20: training loss = 0.2152, validation loss = 0.3101. Learning rate = 0.001. Patience = 1\n",
      "Epoch 21: training loss = 0.2348, validation loss = 0.3195. Learning rate = 0.001. Patience = 2\n",
      "Epoch 22: training loss = 0.2130, validation loss = 0.3035. Learning rate = 0.001. Patience = 0\n",
      "Epoch 23: training loss = 0.2137, validation loss = 0.3147. Learning rate = 0.001. Patience = 1\n",
      "Epoch 24: training loss = 0.2169, validation loss = 0.3070. Learning rate = 0.001. Patience = 2\n",
      "Epoch 25: training loss = 0.2049, validation loss = 0.3322. Learning rate = 0.001. Patience = 3\n",
      "Epoch 26: training loss = 0.2194, validation loss = 0.3103. Learning rate = 0.001. Patience = 4\n",
      "Epoch 27: training loss = 0.2166, validation loss = 0.3266. Learning rate = 0.001. Patience = 5\n",
      "Epoch 28: training loss = 0.2176, validation loss = 0.3054. Learning rate = 0.001. Patience = 6\n",
      "Epoch 29: training loss = 0.2082, validation loss = 0.3045. Learning rate = 0.001. Patience = 7\n",
      "Epoch 30: training loss = 0.2060, validation loss = 0.3255. Learning rate = 0.001. Patience = 8\n",
      "Epoch 31: training loss = 0.2003, validation loss = 0.3064. Learning rate = 0.001. Patience = 9\n",
      "Epoch 32: training loss = 0.1968, validation loss = 0.3434. Learning rate = 0.001. Patience = 10\n",
      "Epoch 33: training loss = 0.2221, validation loss = 0.3128. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 34: training loss = 0.1984, validation loss = 0.3298. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 35: training loss = 0.2049, validation loss = 0.3302. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 36: training loss = 0.1892, validation loss = 0.3205. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 37: training loss = 0.2042, validation loss = 0.3259. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 38: training loss = 0.1846, validation loss = 0.3337. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 39: training loss = 0.1991, validation loss = 0.3285. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 40: training loss = 0.2153, validation loss = 0.3342. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 41: training loss = 0.2206, validation loss = 0.3216. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 42: training loss = 0.1872, validation loss = 0.3408. Learning rate = 0.0005. Patience = 20\n",
      "Epoch 43: training loss = 0.1997, validation loss = 0.3278. Learning rate = 0.0005. Patience = 21\n",
      "Epoch 44: training loss = 0.1929, validation loss = 0.3248. Learning rate = 0.00025. Patience = 22\n",
      "Epoch 45: training loss = 0.2059, validation loss = 0.3189. Learning rate = 0.00025. Patience = 23\n",
      "Epoch 46: training loss = 0.1942, validation loss = 0.3309. Learning rate = 0.00025. Patience = 24\n",
      "Epoch 47: training loss = 0.2059, validation loss = 0.3265. Learning rate = 0.00025. Patience = 25\n",
      "Epoch 48: training loss = 0.1935, validation loss = 0.3202. Learning rate = 0.00025. Patience = 26\n",
      "Epoch 49: training loss = 0.1913, validation loss = 0.3249. Learning rate = 0.00025. Patience = 27\n",
      "Epoch 50: training loss = 0.1868, validation loss = 0.3284. Learning rate = 0.00025. Patience = 28\n",
      "Epoch 51: training loss = 0.1825, validation loss = 0.3277. Learning rate = 0.00025. Patience = 29\n",
      "Epoch 52: training loss = 0.2118, validation loss = 0.3328. Learning rate = 0.00025. Patience = 30\n",
      "Epoch 53: training loss = 0.1834, validation loss = 0.3336. Learning rate = 0.00025. Patience = 31\n",
      "Epoch 54: training loss = 0.1953, validation loss = 0.3354. Learning rate = 0.00025. Patience = 32\n",
      "Epoch 55: training loss = 0.1990, validation loss = 0.3293. Learning rate = 0.000125. Patience = 33\n",
      "Epoch 56: training loss = 0.1844, validation loss = 0.3285. Learning rate = 0.000125. Patience = 34\n",
      "Epoch 57: training loss = 0.1879, validation loss = 0.3310. Learning rate = 0.000125. Patience = 35\n",
      "Epoch 58: training loss = 0.1886, validation loss = 0.3275. Learning rate = 0.000125. Patience = 36\n",
      "Epoch 59: training loss = 0.1771, validation loss = 0.3262. Learning rate = 0.000125. Patience = 37\n",
      "Epoch 60: training loss = 0.1785, validation loss = 0.3293. Learning rate = 0.000125. Patience = 38\n",
      "Epoch 61: training loss = 0.1831, validation loss = 0.3273. Learning rate = 0.000125. Patience = 39\n",
      "Epoch 62: training loss = 0.1995, validation loss = 0.3353. Learning rate = 0.000125. Patience = 40\n",
      "Epoch 63: training loss = 0.1947, validation loss = 0.3325. Learning rate = 0.000125. Patience = 41\n",
      "Epoch 64: training loss = 0.1863, validation loss = 0.3298. Learning rate = 0.000125. Patience = 42\n",
      "Epoch 65: training loss = 0.1833, validation loss = 0.3281. Learning rate = 0.000125. Patience = 43\n",
      "Epoch 66: training loss = 0.1957, validation loss = 0.3317. Learning rate = 6.25e-05. Patience = 44\n",
      "Epoch 67: training loss = 0.1846, validation loss = 0.3360. Learning rate = 6.25e-05. Patience = 45\n",
      "Epoch 68: training loss = 0.2018, validation loss = 0.3322. Learning rate = 6.25e-05. Patience = 46\n",
      "Epoch 69: training loss = 0.1945, validation loss = 0.3257. Learning rate = 6.25e-05. Patience = 47\n",
      "Epoch 70: training loss = 0.1822, validation loss = 0.3285. Learning rate = 6.25e-05. Patience = 48\n",
      "Epoch 71: training loss = 0.2008, validation loss = 0.3309. Learning rate = 6.25e-05. Patience = 49\n",
      "Epoch 72: training loss = 0.1828, validation loss = 0.3299. Learning rate = 6.25e-05. Patience = 50\n",
      "Training stopped at epoch 72; restoring weights from epoch 22\n",
      "Epoch 1: training loss = 0.6739, validation loss = 1.3465. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.3023, validation loss = 1.1305. Learning rate = 0.001. Patience = 0\n",
      "Epoch 3: training loss = 0.2737, validation loss = 1.1412. Learning rate = 0.001. Patience = 1\n",
      "Epoch 4: training loss = 0.2607, validation loss = 0.9823. Learning rate = 0.001. Patience = 0\n",
      "Epoch 5: training loss = 0.2097, validation loss = 0.8509. Learning rate = 0.001. Patience = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: training loss = 0.1853, validation loss = 0.7685. Learning rate = 0.001. Patience = 0\n",
      "Epoch 7: training loss = 0.1739, validation loss = 0.7442. Learning rate = 0.001. Patience = 0\n",
      "Epoch 8: training loss = 0.1625, validation loss = 0.7418. Learning rate = 0.001. Patience = 0\n",
      "Epoch 9: training loss = 0.1664, validation loss = 0.7422. Learning rate = 0.001. Patience = 1\n",
      "Epoch 10: training loss = 0.1592, validation loss = 0.7438. Learning rate = 0.001. Patience = 2\n",
      "Epoch 11: training loss = 0.1600, validation loss = 0.7462. Learning rate = 0.001. Patience = 3\n",
      "Epoch 12: training loss = 0.1609, validation loss = 0.7455. Learning rate = 0.001. Patience = 4\n",
      "Epoch 13: training loss = 0.1671, validation loss = 0.7471. Learning rate = 0.001. Patience = 5\n",
      "Epoch 14: training loss = 0.1563, validation loss = 0.7556. Learning rate = 0.001. Patience = 6\n",
      "Epoch 15: training loss = 0.1499, validation loss = 0.7745. Learning rate = 0.001. Patience = 7\n",
      "Epoch 16: training loss = 0.1460, validation loss = 0.7632. Learning rate = 0.001. Patience = 8\n",
      "Epoch 17: training loss = 0.1606, validation loss = 0.7626. Learning rate = 0.001. Patience = 9\n",
      "Epoch 18: training loss = 0.1611, validation loss = 0.7585. Learning rate = 0.001. Patience = 10\n",
      "Epoch 19: training loss = 0.1510, validation loss = 0.7597. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 20: training loss = 0.1592, validation loss = 0.7401. Learning rate = 0.0005. Patience = 0\n",
      "Epoch 21: training loss = 0.1472, validation loss = 0.7315. Learning rate = 0.0005. Patience = 0\n",
      "Epoch 22: training loss = 0.1537, validation loss = 0.7418. Learning rate = 0.0005. Patience = 1\n",
      "Epoch 23: training loss = 0.1661, validation loss = 0.7362. Learning rate = 0.0005. Patience = 2\n",
      "Epoch 24: training loss = 0.1474, validation loss = 0.7516. Learning rate = 0.0005. Patience = 3\n",
      "Epoch 25: training loss = 0.1394, validation loss = 0.7592. Learning rate = 0.0005. Patience = 4\n",
      "Epoch 26: training loss = 0.1456, validation loss = 0.7579. Learning rate = 0.0005. Patience = 5\n",
      "Epoch 27: training loss = 0.1492, validation loss = 0.7562. Learning rate = 0.0005. Patience = 6\n",
      "Epoch 28: training loss = 0.1629, validation loss = 0.7523. Learning rate = 0.0005. Patience = 7\n",
      "Epoch 29: training loss = 0.1414, validation loss = 0.7491. Learning rate = 0.0005. Patience = 8\n",
      "Epoch 30: training loss = 0.1557, validation loss = 0.7538. Learning rate = 0.0005. Patience = 9\n",
      "Epoch 31: training loss = 0.1447, validation loss = 0.7393. Learning rate = 0.0005. Patience = 10\n",
      "Epoch 32: training loss = 0.1513, validation loss = 0.7779. Learning rate = 0.00025. Patience = 11\n",
      "Epoch 33: training loss = 0.1495, validation loss = 0.7655. Learning rate = 0.00025. Patience = 12\n",
      "Epoch 34: training loss = 0.1364, validation loss = 0.7553. Learning rate = 0.00025. Patience = 13\n",
      "Epoch 35: training loss = 0.1486, validation loss = 0.7711. Learning rate = 0.00025. Patience = 14\n",
      "Epoch 36: training loss = 0.1396, validation loss = 0.7756. Learning rate = 0.00025. Patience = 15\n",
      "Epoch 37: training loss = 0.1437, validation loss = 0.7570. Learning rate = 0.00025. Patience = 16\n",
      "Epoch 38: training loss = 0.1380, validation loss = 0.7955. Learning rate = 0.00025. Patience = 17\n",
      "Epoch 39: training loss = 0.1393, validation loss = 0.7632. Learning rate = 0.00025. Patience = 18\n",
      "Epoch 40: training loss = 0.1376, validation loss = 0.7428. Learning rate = 0.00025. Patience = 19\n",
      "Epoch 41: training loss = 0.1460, validation loss = 0.7602. Learning rate = 0.00025. Patience = 20\n",
      "Epoch 42: training loss = 0.1366, validation loss = 0.7743. Learning rate = 0.00025. Patience = 21\n",
      "Epoch 43: training loss = 0.1335, validation loss = 0.7591. Learning rate = 0.000125. Patience = 22\n",
      "Epoch 44: training loss = 0.1371, validation loss = 0.7641. Learning rate = 0.000125. Patience = 23\n",
      "Epoch 45: training loss = 0.1403, validation loss = 0.7721. Learning rate = 0.000125. Patience = 24\n",
      "Epoch 46: training loss = 0.1356, validation loss = 0.7674. Learning rate = 0.000125. Patience = 25\n",
      "Epoch 47: training loss = 0.1379, validation loss = 0.7676. Learning rate = 0.000125. Patience = 26\n",
      "Epoch 48: training loss = 0.1357, validation loss = 0.7640. Learning rate = 0.000125. Patience = 27\n",
      "Epoch 49: training loss = 0.1324, validation loss = 0.7693. Learning rate = 0.000125. Patience = 28\n",
      "Epoch 50: training loss = 0.1555, validation loss = 0.7771. Learning rate = 0.000125. Patience = 29\n",
      "Epoch 51: training loss = 0.1340, validation loss = 0.7654. Learning rate = 0.000125. Patience = 30\n",
      "Epoch 52: training loss = 0.1304, validation loss = 0.7666. Learning rate = 0.000125. Patience = 31\n",
      "Epoch 53: training loss = 0.1488, validation loss = 0.7739. Learning rate = 0.000125. Patience = 32\n",
      "Epoch 54: training loss = 0.1410, validation loss = 0.7797. Learning rate = 6.25e-05. Patience = 33\n",
      "Epoch 55: training loss = 0.1357, validation loss = 0.7834. Learning rate = 6.25e-05. Patience = 34\n",
      "Epoch 56: training loss = 0.1423, validation loss = 0.7792. Learning rate = 6.25e-05. Patience = 35\n",
      "Epoch 57: training loss = 0.1374, validation loss = 0.7735. Learning rate = 6.25e-05. Patience = 36\n",
      "Epoch 58: training loss = 0.1364, validation loss = 0.7754. Learning rate = 6.25e-05. Patience = 37\n",
      "Epoch 59: training loss = 0.1394, validation loss = 0.7765. Learning rate = 6.25e-05. Patience = 38\n",
      "Epoch 60: training loss = 0.1311, validation loss = 0.7796. Learning rate = 6.25e-05. Patience = 39\n",
      "Epoch 61: training loss = 0.1385, validation loss = 0.7816. Learning rate = 6.25e-05. Patience = 40\n",
      "Epoch 62: training loss = 0.1417, validation loss = 0.7824. Learning rate = 6.25e-05. Patience = 41\n",
      "Epoch 63: training loss = 0.1325, validation loss = 0.7770. Learning rate = 6.25e-05. Patience = 42\n",
      "Epoch 64: training loss = 0.1332, validation loss = 0.7761. Learning rate = 6.25e-05. Patience = 43\n",
      "Epoch 65: training loss = 0.1412, validation loss = 0.7827. Learning rate = 3.125e-05. Patience = 44\n",
      "Epoch 66: training loss = 0.1339, validation loss = 0.7842. Learning rate = 3.125e-05. Patience = 45\n",
      "Epoch 67: training loss = 0.1326, validation loss = 0.7825. Learning rate = 3.125e-05. Patience = 46\n",
      "Epoch 68: training loss = 0.1400, validation loss = 0.7822. Learning rate = 3.125e-05. Patience = 47\n",
      "Epoch 69: training loss = 0.1426, validation loss = 0.7850. Learning rate = 3.125e-05. Patience = 48\n",
      "Epoch 70: training loss = 0.1332, validation loss = 0.7826. Learning rate = 3.125e-05. Patience = 49\n",
      "Epoch 71: training loss = 0.1515, validation loss = 0.7827. Learning rate = 3.125e-05. Patience = 50\n",
      "Training stopped at epoch 71; restoring weights from epoch 21\n",
      "Epoch 1: training loss = 0.6363, validation loss = 0.3708. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.5277, validation loss = 0.3376. Learning rate = 0.001. Patience = 0\n",
      "Epoch 3: training loss = 0.4182, validation loss = 0.2750. Learning rate = 0.001. Patience = 0\n",
      "Epoch 4: training loss = 0.4005, validation loss = 0.2662. Learning rate = 0.001. Patience = 0\n",
      "Epoch 5: training loss = 0.3849, validation loss = 0.2768. Learning rate = 0.001. Patience = 1\n",
      "Epoch 6: training loss = 0.3761, validation loss = 0.2643. Learning rate = 0.001. Patience = 0\n",
      "Epoch 7: training loss = 0.3649, validation loss = 0.2657. Learning rate = 0.001. Patience = 1\n",
      "Epoch 8: training loss = 0.3664, validation loss = 0.2669. Learning rate = 0.001. Patience = 2\n",
      "Epoch 9: training loss = 0.3454, validation loss = 0.2550. Learning rate = 0.001. Patience = 0\n",
      "Epoch 10: training loss = 0.3447, validation loss = 0.2570. Learning rate = 0.001. Patience = 1\n",
      "Epoch 11: training loss = 0.3362, validation loss = 0.2569. Learning rate = 0.001. Patience = 2\n",
      "Epoch 12: training loss = 0.3413, validation loss = 0.2592. Learning rate = 0.001. Patience = 3\n",
      "Epoch 13: training loss = 0.3312, validation loss = 0.2562. Learning rate = 0.001. Patience = 4\n",
      "Epoch 14: training loss = 0.3478, validation loss = 0.2497. Learning rate = 0.001. Patience = 0\n",
      "Epoch 15: training loss = 0.3298, validation loss = 0.2602. Learning rate = 0.001. Patience = 1\n",
      "Epoch 16: training loss = 0.3276, validation loss = 0.2626. Learning rate = 0.001. Patience = 2\n",
      "Epoch 17: training loss = 0.3178, validation loss = 0.2554. Learning rate = 0.001. Patience = 3\n",
      "Epoch 18: training loss = 0.3194, validation loss = 0.2528. Learning rate = 0.001. Patience = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: training loss = 0.3211, validation loss = 0.2589. Learning rate = 0.001. Patience = 5\n",
      "Epoch 20: training loss = 0.3194, validation loss = 0.2517. Learning rate = 0.001. Patience = 6\n",
      "Epoch 21: training loss = 0.3237, validation loss = 0.2460. Learning rate = 0.001. Patience = 0\n",
      "Epoch 22: training loss = 0.3110, validation loss = 0.2478. Learning rate = 0.001. Patience = 1\n",
      "Epoch 23: training loss = 0.3144, validation loss = 0.2496. Learning rate = 0.001. Patience = 2\n",
      "Epoch 24: training loss = 0.3128, validation loss = 0.2664. Learning rate = 0.001. Patience = 3\n",
      "Epoch 25: training loss = 0.3150, validation loss = 0.2484. Learning rate = 0.001. Patience = 4\n",
      "Epoch 26: training loss = 0.3033, validation loss = 0.2511. Learning rate = 0.001. Patience = 5\n",
      "Epoch 27: training loss = 0.3005, validation loss = 0.2520. Learning rate = 0.001. Patience = 6\n",
      "Epoch 28: training loss = 0.3004, validation loss = 0.2618. Learning rate = 0.001. Patience = 7\n",
      "Epoch 29: training loss = 0.3041, validation loss = 0.2543. Learning rate = 0.001. Patience = 8\n",
      "Epoch 30: training loss = 0.3023, validation loss = 0.2541. Learning rate = 0.001. Patience = 9\n",
      "Epoch 31: training loss = 0.3137, validation loss = 0.2382. Learning rate = 0.001. Patience = 0\n",
      "Epoch 32: training loss = 0.2932, validation loss = 0.2546. Learning rate = 0.001. Patience = 1\n",
      "Epoch 33: training loss = 0.2956, validation loss = 0.2793. Learning rate = 0.001. Patience = 2\n",
      "Epoch 34: training loss = 0.2954, validation loss = 0.2578. Learning rate = 0.001. Patience = 3\n",
      "Epoch 35: training loss = 0.2969, validation loss = 0.2775. Learning rate = 0.001. Patience = 4\n",
      "Epoch 36: training loss = 0.2925, validation loss = 0.2505. Learning rate = 0.001. Patience = 5\n",
      "Epoch 37: training loss = 0.2899, validation loss = 0.2472. Learning rate = 0.001. Patience = 6\n",
      "Epoch 38: training loss = 0.2941, validation loss = 0.2884. Learning rate = 0.001. Patience = 7\n",
      "Epoch 39: training loss = 0.2930, validation loss = 0.2564. Learning rate = 0.001. Patience = 8\n",
      "Epoch 40: training loss = 0.2807, validation loss = 0.2450. Learning rate = 0.001. Patience = 9\n",
      "Epoch 41: training loss = 0.2803, validation loss = 0.2513. Learning rate = 0.001. Patience = 10\n",
      "Epoch 42: training loss = 0.2764, validation loss = 0.2575. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 43: training loss = 0.2728, validation loss = 0.2485. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 44: training loss = 0.2772, validation loss = 0.2599. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 45: training loss = 0.2666, validation loss = 0.2575. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 46: training loss = 0.2631, validation loss = 0.2602. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 47: training loss = 0.2851, validation loss = 0.2606. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 48: training loss = 0.2704, validation loss = 0.2553. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 49: training loss = 0.2604, validation loss = 0.2627. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 50: training loss = 0.2633, validation loss = 0.2568. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 51: training loss = 0.2669, validation loss = 0.2576. Learning rate = 0.0005. Patience = 20\n",
      "Epoch 52: training loss = 0.2562, validation loss = 0.2567. Learning rate = 0.0005. Patience = 21\n",
      "Epoch 53: training loss = 0.2535, validation loss = 0.2620. Learning rate = 0.00025. Patience = 22\n",
      "Epoch 54: training loss = 0.2574, validation loss = 0.2581. Learning rate = 0.00025. Patience = 23\n",
      "Epoch 55: training loss = 0.2668, validation loss = 0.2614. Learning rate = 0.00025. Patience = 24\n",
      "Epoch 56: training loss = 0.2668, validation loss = 0.2610. Learning rate = 0.00025. Patience = 25\n",
      "Epoch 57: training loss = 0.2627, validation loss = 0.2597. Learning rate = 0.00025. Patience = 26\n",
      "Epoch 58: training loss = 0.2624, validation loss = 0.2653. Learning rate = 0.00025. Patience = 27\n",
      "Epoch 59: training loss = 0.2539, validation loss = 0.2645. Learning rate = 0.00025. Patience = 28\n",
      "Epoch 60: training loss = 0.2592, validation loss = 0.2606. Learning rate = 0.00025. Patience = 29\n",
      "Epoch 61: training loss = 0.2585, validation loss = 0.2623. Learning rate = 0.00025. Patience = 30\n",
      "Epoch 62: training loss = 0.2498, validation loss = 0.2624. Learning rate = 0.00025. Patience = 31\n",
      "Epoch 63: training loss = 0.2533, validation loss = 0.2631. Learning rate = 0.00025. Patience = 32\n",
      "Epoch 64: training loss = 0.2465, validation loss = 0.2627. Learning rate = 0.000125. Patience = 33\n",
      "Epoch 65: training loss = 0.2453, validation loss = 0.2596. Learning rate = 0.000125. Patience = 34\n",
      "Epoch 66: training loss = 0.2485, validation loss = 0.2614. Learning rate = 0.000125. Patience = 35\n",
      "Epoch 67: training loss = 0.2519, validation loss = 0.2618. Learning rate = 0.000125. Patience = 36\n",
      "Epoch 68: training loss = 0.2532, validation loss = 0.2614. Learning rate = 0.000125. Patience = 37\n",
      "Epoch 69: training loss = 0.2424, validation loss = 0.2595. Learning rate = 0.000125. Patience = 38\n",
      "Epoch 70: training loss = 0.2537, validation loss = 0.2622. Learning rate = 0.000125. Patience = 39\n",
      "Epoch 71: training loss = 0.2535, validation loss = 0.2678. Learning rate = 0.000125. Patience = 40\n",
      "Epoch 72: training loss = 0.2436, validation loss = 0.2628. Learning rate = 0.000125. Patience = 41\n",
      "Epoch 73: training loss = 0.2486, validation loss = 0.2595. Learning rate = 0.000125. Patience = 42\n",
      "Epoch 74: training loss = 0.2428, validation loss = 0.2617. Learning rate = 0.000125. Patience = 43\n",
      "Epoch 75: training loss = 0.2517, validation loss = 0.2659. Learning rate = 6.25e-05. Patience = 44\n",
      "Epoch 76: training loss = 0.2415, validation loss = 0.2628. Learning rate = 6.25e-05. Patience = 45\n",
      "Epoch 77: training loss = 0.2500, validation loss = 0.2627. Learning rate = 6.25e-05. Patience = 46\n",
      "Epoch 78: training loss = 0.2410, validation loss = 0.2630. Learning rate = 6.25e-05. Patience = 47\n",
      "Epoch 79: training loss = 0.2523, validation loss = 0.2617. Learning rate = 6.25e-05. Patience = 48\n",
      "Epoch 80: training loss = 0.2399, validation loss = 0.2638. Learning rate = 6.25e-05. Patience = 49\n",
      "Epoch 81: training loss = 0.2392, validation loss = 0.2626. Learning rate = 6.25e-05. Patience = 50\n",
      "Training stopped at epoch 81; restoring weights from epoch 31\n",
      "Epoch 1: training loss = 0.6350, validation loss = 0.4443. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.4232, validation loss = 0.4235. Learning rate = 0.001. Patience = 0\n",
      "Epoch 3: training loss = 0.3573, validation loss = 0.5271. Learning rate = 0.001. Patience = 1\n",
      "Epoch 4: training loss = 0.3161, validation loss = 0.3612. Learning rate = 0.001. Patience = 0\n",
      "Epoch 5: training loss = 0.3464, validation loss = 0.3245. Learning rate = 0.001. Patience = 0\n",
      "Epoch 6: training loss = 0.2992, validation loss = 0.3194. Learning rate = 0.001. Patience = 0\n",
      "Epoch 7: training loss = 0.2911, validation loss = 0.2665. Learning rate = 0.001. Patience = 0\n",
      "Epoch 8: training loss = 0.3099, validation loss = 0.2882. Learning rate = 0.001. Patience = 1\n",
      "Epoch 9: training loss = 0.3163, validation loss = 0.2668. Learning rate = 0.001. Patience = 2\n",
      "Epoch 10: training loss = 0.2852, validation loss = 0.2598. Learning rate = 0.001. Patience = 0\n",
      "Epoch 11: training loss = 0.2799, validation loss = 0.2604. Learning rate = 0.001. Patience = 1\n",
      "Epoch 12: training loss = 0.2687, validation loss = 0.2565. Learning rate = 0.001. Patience = 0\n",
      "Epoch 13: training loss = 0.2676, validation loss = 0.2548. Learning rate = 0.001. Patience = 0\n",
      "Epoch 14: training loss = 0.2820, validation loss = 0.2566. Learning rate = 0.001. Patience = 1\n",
      "Epoch 15: training loss = 0.2791, validation loss = 0.2427. Learning rate = 0.001. Patience = 0\n",
      "Epoch 16: training loss = 0.2657, validation loss = 0.2534. Learning rate = 0.001. Patience = 1\n",
      "Epoch 17: training loss = 0.2700, validation loss = 0.2588. Learning rate = 0.001. Patience = 2\n",
      "Epoch 18: training loss = 0.3340, validation loss = 0.2311. Learning rate = 0.001. Patience = 0\n",
      "Epoch 19: training loss = 0.3211, validation loss = 0.2705. Learning rate = 0.001. Patience = 1\n",
      "Epoch 20: training loss = 0.2532, validation loss = 0.2358. Learning rate = 0.001. Patience = 2\n",
      "Epoch 21: training loss = 0.2764, validation loss = 0.2514. Learning rate = 0.001. Patience = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: training loss = 0.2601, validation loss = 0.2308. Learning rate = 0.001. Patience = 0\n",
      "Epoch 23: training loss = 0.2748, validation loss = 0.2404. Learning rate = 0.001. Patience = 1\n",
      "Epoch 24: training loss = 0.2744, validation loss = 0.2738. Learning rate = 0.001. Patience = 2\n",
      "Epoch 25: training loss = 0.2590, validation loss = 0.2365. Learning rate = 0.001. Patience = 3\n",
      "Epoch 26: training loss = 0.2630, validation loss = 0.2378. Learning rate = 0.001. Patience = 4\n",
      "Epoch 27: training loss = 0.2659, validation loss = 0.2760. Learning rate = 0.001. Patience = 5\n",
      "Epoch 28: training loss = 0.2719, validation loss = 0.2288. Learning rate = 0.001. Patience = 0\n",
      "Epoch 29: training loss = 0.2828, validation loss = 0.2817. Learning rate = 0.001. Patience = 1\n",
      "Epoch 30: training loss = 0.2764, validation loss = 0.2538. Learning rate = 0.001. Patience = 2\n",
      "Epoch 31: training loss = 0.2521, validation loss = 0.2437. Learning rate = 0.001. Patience = 3\n",
      "Epoch 32: training loss = 0.2639, validation loss = 0.2655. Learning rate = 0.001. Patience = 4\n",
      "Epoch 33: training loss = 0.2955, validation loss = 0.2454. Learning rate = 0.001. Patience = 5\n",
      "Epoch 34: training loss = 0.2494, validation loss = 0.2770. Learning rate = 0.001. Patience = 6\n",
      "Epoch 35: training loss = 0.3253, validation loss = 0.2324. Learning rate = 0.001. Patience = 7\n",
      "Epoch 36: training loss = 0.2675, validation loss = 0.2743. Learning rate = 0.001. Patience = 8\n",
      "Epoch 37: training loss = 0.2582, validation loss = 0.2449. Learning rate = 0.001. Patience = 9\n",
      "Epoch 38: training loss = 0.2450, validation loss = 0.2573. Learning rate = 0.001. Patience = 10\n",
      "Epoch 39: training loss = 0.2510, validation loss = 0.2558. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 40: training loss = 0.2646, validation loss = 0.2410. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 41: training loss = 0.2660, validation loss = 0.2795. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 42: training loss = 0.2620, validation loss = 0.2654. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 43: training loss = 0.2725, validation loss = 0.2755. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 44: training loss = 0.2577, validation loss = 0.2594. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 45: training loss = 0.2578, validation loss = 0.2520. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 46: training loss = 0.2445, validation loss = 0.2756. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 47: training loss = 0.2455, validation loss = 0.2734. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 48: training loss = 0.2480, validation loss = 0.2471. Learning rate = 0.0005. Patience = 20\n",
      "Epoch 49: training loss = 0.2836, validation loss = 0.2763. Learning rate = 0.0005. Patience = 21\n",
      "Epoch 50: training loss = 0.2450, validation loss = 0.2728. Learning rate = 0.00025. Patience = 22\n",
      "Epoch 51: training loss = 0.2563, validation loss = 0.2603. Learning rate = 0.00025. Patience = 23\n",
      "Epoch 52: training loss = 0.2873, validation loss = 0.2759. Learning rate = 0.00025. Patience = 24\n",
      "Epoch 53: training loss = 0.2592, validation loss = 0.2775. Learning rate = 0.00025. Patience = 25\n",
      "Epoch 54: training loss = 0.2383, validation loss = 0.2751. Learning rate = 0.00025. Patience = 26\n",
      "Epoch 55: training loss = 0.2423, validation loss = 0.2634. Learning rate = 0.00025. Patience = 27\n",
      "Epoch 56: training loss = 0.2584, validation loss = 0.2608. Learning rate = 0.00025. Patience = 28\n",
      "Epoch 57: training loss = 0.2415, validation loss = 0.2833. Learning rate = 0.00025. Patience = 29\n",
      "Epoch 58: training loss = 0.2357, validation loss = 0.2782. Learning rate = 0.00025. Patience = 30\n",
      "Epoch 59: training loss = 0.2639, validation loss = 0.2679. Learning rate = 0.00025. Patience = 31\n",
      "Epoch 60: training loss = 0.2614, validation loss = 0.2687. Learning rate = 0.00025. Patience = 32\n",
      "Epoch 61: training loss = 0.2576, validation loss = 0.2885. Learning rate = 0.000125. Patience = 33\n",
      "Epoch 62: training loss = 0.2727, validation loss = 0.2840. Learning rate = 0.000125. Patience = 34\n",
      "Epoch 63: training loss = 0.2400, validation loss = 0.2752. Learning rate = 0.000125. Patience = 35\n",
      "Epoch 64: training loss = 0.2331, validation loss = 0.2631. Learning rate = 0.000125. Patience = 36\n",
      "Epoch 65: training loss = 0.2566, validation loss = 0.2634. Learning rate = 0.000125. Patience = 37\n",
      "Epoch 66: training loss = 0.2527, validation loss = 0.2764. Learning rate = 0.000125. Patience = 38\n",
      "Epoch 67: training loss = 0.2800, validation loss = 0.2834. Learning rate = 0.000125. Patience = 39\n",
      "Epoch 68: training loss = 0.2718, validation loss = 0.2906. Learning rate = 0.000125. Patience = 40\n",
      "Epoch 69: training loss = 0.2580, validation loss = 0.2838. Learning rate = 0.000125. Patience = 41\n",
      "Epoch 70: training loss = 0.2478, validation loss = 0.2827. Learning rate = 0.000125. Patience = 42\n",
      "Epoch 71: training loss = 0.2352, validation loss = 0.2688. Learning rate = 0.000125. Patience = 43\n",
      "Epoch 72: training loss = 0.2525, validation loss = 0.2717. Learning rate = 6.25e-05. Patience = 44\n",
      "Epoch 73: training loss = 0.2628, validation loss = 0.2734. Learning rate = 6.25e-05. Patience = 45\n",
      "Epoch 74: training loss = 0.2678, validation loss = 0.2696. Learning rate = 6.25e-05. Patience = 46\n",
      "Epoch 75: training loss = 0.2590, validation loss = 0.2784. Learning rate = 6.25e-05. Patience = 47\n",
      "Epoch 76: training loss = 0.2508, validation loss = 0.2833. Learning rate = 6.25e-05. Patience = 48\n",
      "Epoch 77: training loss = 0.2454, validation loss = 0.2797. Learning rate = 6.25e-05. Patience = 49\n",
      "Epoch 78: training loss = 0.2796, validation loss = 0.2760. Learning rate = 6.25e-05. Patience = 50\n",
      "Training stopped at epoch 78; restoring weights from epoch 28\n",
      "Epoch 1: training loss = 0.5691, validation loss = 0.6931. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.4314, validation loss = 0.4268. Learning rate = 0.001. Patience = 0\n",
      "Epoch 3: training loss = 0.3519, validation loss = 0.3229. Learning rate = 0.001. Patience = 0\n",
      "Epoch 4: training loss = 0.3321, validation loss = 0.2152. Learning rate = 0.001. Patience = 0\n",
      "Epoch 5: training loss = 0.2957, validation loss = 0.2670. Learning rate = 0.001. Patience = 1\n",
      "Epoch 6: training loss = 0.3173, validation loss = 0.1940. Learning rate = 0.001. Patience = 0\n",
      "Epoch 7: training loss = 0.2717, validation loss = 0.2051. Learning rate = 0.001. Patience = 1\n",
      "Epoch 8: training loss = 0.2774, validation loss = 0.2308. Learning rate = 0.001. Patience = 2\n",
      "Epoch 9: training loss = 0.2819, validation loss = 0.1850. Learning rate = 0.001. Patience = 0\n",
      "Epoch 10: training loss = 0.2582, validation loss = 0.2227. Learning rate = 0.001. Patience = 1\n",
      "Epoch 11: training loss = 0.2738, validation loss = 0.1823. Learning rate = 0.001. Patience = 0\n",
      "Epoch 12: training loss = 0.2635, validation loss = 0.1799. Learning rate = 0.001. Patience = 0\n",
      "Epoch 13: training loss = 0.2489, validation loss = 0.2047. Learning rate = 0.001. Patience = 1\n",
      "Epoch 14: training loss = 0.2680, validation loss = 0.1801. Learning rate = 0.001. Patience = 2\n",
      "Epoch 15: training loss = 0.2583, validation loss = 0.1962. Learning rate = 0.001. Patience = 3\n",
      "Epoch 16: training loss = 0.2618, validation loss = 0.1772. Learning rate = 0.001. Patience = 0\n",
      "Epoch 17: training loss = 0.2721, validation loss = 0.1931. Learning rate = 0.001. Patience = 1\n",
      "Epoch 18: training loss = 0.2510, validation loss = 0.1881. Learning rate = 0.001. Patience = 2\n",
      "Epoch 19: training loss = 0.2491, validation loss = 0.1768. Learning rate = 0.001. Patience = 0\n",
      "Epoch 20: training loss = 0.2578, validation loss = 0.1789. Learning rate = 0.001. Patience = 1\n",
      "Epoch 21: training loss = 0.2501, validation loss = 0.2063. Learning rate = 0.001. Patience = 2\n",
      "Epoch 22: training loss = 0.2554, validation loss = 0.1985. Learning rate = 0.001. Patience = 3\n",
      "Epoch 23: training loss = 0.2679, validation loss = 0.2369. Learning rate = 0.001. Patience = 4\n",
      "Epoch 24: training loss = 0.2710, validation loss = 0.1878. Learning rate = 0.001. Patience = 5\n",
      "Epoch 25: training loss = 0.2336, validation loss = 0.1768. Learning rate = 0.001. Patience = 6\n",
      "Epoch 26: training loss = 0.2563, validation loss = 0.1729. Learning rate = 0.001. Patience = 0\n",
      "Epoch 27: training loss = 0.2433, validation loss = 0.1777. Learning rate = 0.001. Patience = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: training loss = 0.2339, validation loss = 0.1763. Learning rate = 0.001. Patience = 2\n",
      "Epoch 29: training loss = 0.2402, validation loss = 0.1740. Learning rate = 0.001. Patience = 3\n",
      "Epoch 30: training loss = 0.2327, validation loss = 0.1883. Learning rate = 0.001. Patience = 4\n",
      "Epoch 31: training loss = 0.2504, validation loss = 0.1786. Learning rate = 0.001. Patience = 5\n",
      "Epoch 32: training loss = 0.2397, validation loss = 0.1839. Learning rate = 0.001. Patience = 6\n",
      "Epoch 33: training loss = 0.2326, validation loss = 0.1761. Learning rate = 0.001. Patience = 7\n",
      "Epoch 34: training loss = 0.2516, validation loss = 0.1807. Learning rate = 0.001. Patience = 8\n",
      "Epoch 35: training loss = 0.2384, validation loss = 0.1856. Learning rate = 0.001. Patience = 9\n",
      "Epoch 36: training loss = 0.2468, validation loss = 0.1752. Learning rate = 0.001. Patience = 10\n",
      "Epoch 37: training loss = 0.2288, validation loss = 0.1813. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 38: training loss = 0.2337, validation loss = 0.1824. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 39: training loss = 0.2332, validation loss = 0.1875. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 40: training loss = 0.2264, validation loss = 0.1732. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 41: training loss = 0.2212, validation loss = 0.1768. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 42: training loss = 0.2233, validation loss = 0.1734. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 43: training loss = 0.2288, validation loss = 0.1788. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 44: training loss = 0.2205, validation loss = 0.1749. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 45: training loss = 0.2470, validation loss = 0.1767. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 46: training loss = 0.2239, validation loss = 0.1716. Learning rate = 0.0005. Patience = 0\n",
      "Epoch 47: training loss = 0.2286, validation loss = 0.1676. Learning rate = 0.0005. Patience = 0\n",
      "Epoch 48: training loss = 0.2304, validation loss = 0.1688. Learning rate = 0.0005. Patience = 1\n",
      "Epoch 49: training loss = 0.2141, validation loss = 0.1685. Learning rate = 0.0005. Patience = 2\n",
      "Epoch 50: training loss = 0.2302, validation loss = 0.1664. Learning rate = 0.0005. Patience = 0\n",
      "Epoch 51: training loss = 0.2096, validation loss = 0.1700. Learning rate = 0.0005. Patience = 1\n",
      "Epoch 52: training loss = 0.2277, validation loss = 0.1702. Learning rate = 0.0005. Patience = 2\n",
      "Epoch 53: training loss = 0.2353, validation loss = 0.1734. Learning rate = 0.0005. Patience = 3\n",
      "Epoch 54: training loss = 0.2166, validation loss = 0.1714. Learning rate = 0.0005. Patience = 4\n",
      "Epoch 55: training loss = 0.2172, validation loss = 0.1696. Learning rate = 0.0005. Patience = 5\n",
      "Epoch 56: training loss = 0.2268, validation loss = 0.1725. Learning rate = 0.0005. Patience = 6\n",
      "Epoch 57: training loss = 0.2298, validation loss = 0.1801. Learning rate = 0.0005. Patience = 7\n",
      "Epoch 58: training loss = 0.2239, validation loss = 0.1730. Learning rate = 0.0005. Patience = 8\n",
      "Epoch 59: training loss = 0.2203, validation loss = 0.1739. Learning rate = 0.0005. Patience = 9\n",
      "Epoch 60: training loss = 0.2087, validation loss = 0.1705. Learning rate = 0.0005. Patience = 10\n",
      "Epoch 61: training loss = 0.2088, validation loss = 0.1687. Learning rate = 0.00025. Patience = 11\n",
      "Epoch 62: training loss = 0.2070, validation loss = 0.1707. Learning rate = 0.00025. Patience = 12\n",
      "Epoch 63: training loss = 0.2254, validation loss = 0.1726. Learning rate = 0.00025. Patience = 13\n",
      "Epoch 64: training loss = 0.2095, validation loss = 0.1700. Learning rate = 0.00025. Patience = 14\n",
      "Epoch 65: training loss = 0.2081, validation loss = 0.1712. Learning rate = 0.00025. Patience = 15\n",
      "Epoch 66: training loss = 0.2276, validation loss = 0.1666. Learning rate = 0.00025. Patience = 16\n",
      "Epoch 67: training loss = 0.2243, validation loss = 0.1680. Learning rate = 0.00025. Patience = 17\n",
      "Epoch 68: training loss = 0.2113, validation loss = 0.1729. Learning rate = 0.00025. Patience = 18\n",
      "Epoch 69: training loss = 0.2013, validation loss = 0.1672. Learning rate = 0.00025. Patience = 19\n",
      "Epoch 70: training loss = 0.2245, validation loss = 0.1670. Learning rate = 0.00025. Patience = 20\n",
      "Epoch 71: training loss = 0.2027, validation loss = 0.1685. Learning rate = 0.00025. Patience = 21\n",
      "Epoch 72: training loss = 0.2178, validation loss = 0.1695. Learning rate = 0.000125. Patience = 22\n",
      "Epoch 73: training loss = 0.2300, validation loss = 0.1682. Learning rate = 0.000125. Patience = 23\n",
      "Epoch 74: training loss = 0.2161, validation loss = 0.1678. Learning rate = 0.000125. Patience = 24\n",
      "Epoch 75: training loss = 0.2152, validation loss = 0.1694. Learning rate = 0.000125. Patience = 25\n",
      "Epoch 76: training loss = 0.2328, validation loss = 0.1721. Learning rate = 0.000125. Patience = 26\n",
      "Epoch 77: training loss = 0.2170, validation loss = 0.1706. Learning rate = 0.000125. Patience = 27\n",
      "Epoch 78: training loss = 0.2068, validation loss = 0.1717. Learning rate = 0.000125. Patience = 28\n",
      "Epoch 79: training loss = 0.2000, validation loss = 0.1700. Learning rate = 0.000125. Patience = 29\n",
      "Epoch 80: training loss = 0.2206, validation loss = 0.1698. Learning rate = 0.000125. Patience = 30\n",
      "Epoch 81: training loss = 0.2172, validation loss = 0.1701. Learning rate = 0.000125. Patience = 31\n",
      "Epoch 82: training loss = 0.2364, validation loss = 0.1685. Learning rate = 0.000125. Patience = 32\n",
      "Epoch 83: training loss = 0.2219, validation loss = 0.1689. Learning rate = 6.25e-05. Patience = 33\n",
      "Epoch 84: training loss = 0.2181, validation loss = 0.1692. Learning rate = 6.25e-05. Patience = 34\n",
      "Epoch 85: training loss = 0.2253, validation loss = 0.1689. Learning rate = 6.25e-05. Patience = 35\n",
      "Epoch 86: training loss = 0.2130, validation loss = 0.1694. Learning rate = 6.25e-05. Patience = 36\n",
      "Epoch 87: training loss = 0.2134, validation loss = 0.1695. Learning rate = 6.25e-05. Patience = 37\n",
      "Epoch 88: training loss = 0.2366, validation loss = 0.1696. Learning rate = 6.25e-05. Patience = 38\n",
      "Epoch 89: training loss = 0.2167, validation loss = 0.1694. Learning rate = 6.25e-05. Patience = 39\n",
      "Epoch 90: training loss = 0.2135, validation loss = 0.1694. Learning rate = 6.25e-05. Patience = 40\n",
      "Epoch 91: training loss = 0.2167, validation loss = 0.1694. Learning rate = 6.25e-05. Patience = 41\n",
      "Epoch 92: training loss = 0.2224, validation loss = 0.1695. Learning rate = 6.25e-05. Patience = 42\n",
      "Epoch 93: training loss = 0.2021, validation loss = 0.1687. Learning rate = 6.25e-05. Patience = 43\n",
      "Epoch 94: training loss = 0.2088, validation loss = 0.1682. Learning rate = 3.125e-05. Patience = 44\n",
      "Epoch 95: training loss = 0.2018, validation loss = 0.1685. Learning rate = 3.125e-05. Patience = 45\n",
      "Epoch 96: training loss = 0.2366, validation loss = 0.1691. Learning rate = 3.125e-05. Patience = 46\n",
      "Epoch 97: training loss = 0.2049, validation loss = 0.1692. Learning rate = 3.125e-05. Patience = 47\n",
      "Epoch 98: training loss = 0.1995, validation loss = 0.1694. Learning rate = 3.125e-05. Patience = 48\n",
      "Epoch 99: training loss = 0.2067, validation loss = 0.1692. Learning rate = 3.125e-05. Patience = 49\n",
      "Epoch 100: training loss = 0.1974, validation loss = 0.1692. Learning rate = 3.125e-05. Patience = 50\n",
      "Training stopped at epoch 100; restoring weights from epoch 50\n",
      "Epoch 1: training loss = 0.7684, validation loss = 0.9521. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.4914, validation loss = 0.7067. Learning rate = 0.001. Patience = 0\n",
      "Epoch 3: training loss = 0.4527, validation loss = 0.5394. Learning rate = 0.001. Patience = 0\n",
      "Epoch 4: training loss = 0.4197, validation loss = 0.5968. Learning rate = 0.001. Patience = 1\n",
      "Epoch 5: training loss = 0.3972, validation loss = 0.5757. Learning rate = 0.001. Patience = 2\n",
      "Epoch 6: training loss = 0.3962, validation loss = 0.5531. Learning rate = 0.001. Patience = 3\n",
      "Epoch 7: training loss = 0.3829, validation loss = 0.5713. Learning rate = 0.001. Patience = 4\n",
      "Epoch 8: training loss = 0.3776, validation loss = 0.5221. Learning rate = 0.001. Patience = 0\n",
      "Epoch 9: training loss = 0.3797, validation loss = 0.5219. Learning rate = 0.001. Patience = 0\n",
      "Epoch 10: training loss = 0.3621, validation loss = 0.5225. Learning rate = 0.001. Patience = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: training loss = 0.3435, validation loss = 0.5305. Learning rate = 0.001. Patience = 2\n",
      "Epoch 12: training loss = 0.3552, validation loss = 0.5550. Learning rate = 0.001. Patience = 3\n",
      "Epoch 13: training loss = 0.3546, validation loss = 0.5054. Learning rate = 0.001. Patience = 0\n",
      "Epoch 14: training loss = 0.3365, validation loss = 0.5030. Learning rate = 0.001. Patience = 0\n",
      "Epoch 15: training loss = 0.3522, validation loss = 0.5292. Learning rate = 0.001. Patience = 1\n",
      "Epoch 16: training loss = 0.3327, validation loss = 0.5405. Learning rate = 0.001. Patience = 2\n",
      "Epoch 17: training loss = 0.3278, validation loss = 0.4668. Learning rate = 0.001. Patience = 0\n",
      "Epoch 18: training loss = 0.3370, validation loss = 0.4934. Learning rate = 0.001. Patience = 1\n",
      "Epoch 19: training loss = 0.3320, validation loss = 0.4647. Learning rate = 0.001. Patience = 0\n",
      "Epoch 20: training loss = 0.3233, validation loss = 0.4824. Learning rate = 0.001. Patience = 1\n",
      "Epoch 21: training loss = 0.3175, validation loss = 0.4675. Learning rate = 0.001. Patience = 2\n",
      "Epoch 22: training loss = 0.3314, validation loss = 0.4505. Learning rate = 0.001. Patience = 0\n",
      "Epoch 23: training loss = 0.3106, validation loss = 0.4932. Learning rate = 0.001. Patience = 1\n",
      "Epoch 24: training loss = 0.3210, validation loss = 0.5120. Learning rate = 0.001. Patience = 2\n",
      "Epoch 25: training loss = 0.3311, validation loss = 0.4700. Learning rate = 0.001. Patience = 3\n",
      "Epoch 26: training loss = 0.3180, validation loss = 0.4408. Learning rate = 0.001. Patience = 0\n",
      "Epoch 27: training loss = 0.3020, validation loss = 0.5085. Learning rate = 0.001. Patience = 1\n",
      "Epoch 28: training loss = 0.3213, validation loss = 0.4890. Learning rate = 0.001. Patience = 2\n",
      "Epoch 29: training loss = 0.2991, validation loss = 0.5574. Learning rate = 0.001. Patience = 3\n",
      "Epoch 30: training loss = 0.3119, validation loss = 0.5146. Learning rate = 0.001. Patience = 4\n",
      "Epoch 31: training loss = 0.3197, validation loss = 0.4803. Learning rate = 0.001. Patience = 5\n",
      "Epoch 32: training loss = 0.3184, validation loss = 0.4373. Learning rate = 0.001. Patience = 0\n",
      "Epoch 33: training loss = 0.3056, validation loss = 0.5500. Learning rate = 0.001. Patience = 1\n",
      "Epoch 34: training loss = 0.3019, validation loss = 0.5473. Learning rate = 0.001. Patience = 2\n",
      "Epoch 35: training loss = 0.2911, validation loss = 0.4199. Learning rate = 0.001. Patience = 0\n",
      "Epoch 36: training loss = 0.3228, validation loss = 0.4285. Learning rate = 0.001. Patience = 1\n",
      "Epoch 37: training loss = 0.3061, validation loss = 0.5932. Learning rate = 0.001. Patience = 2\n",
      "Epoch 38: training loss = 0.3061, validation loss = 0.4812. Learning rate = 0.001. Patience = 3\n",
      "Epoch 39: training loss = 0.2911, validation loss = 0.5080. Learning rate = 0.001. Patience = 4\n",
      "Epoch 40: training loss = 0.3023, validation loss = 0.4315. Learning rate = 0.001. Patience = 5\n",
      "Epoch 41: training loss = 0.2829, validation loss = 0.5008. Learning rate = 0.001. Patience = 6\n",
      "Epoch 42: training loss = 0.2668, validation loss = 0.4961. Learning rate = 0.001. Patience = 7\n",
      "Epoch 43: training loss = 0.2784, validation loss = 0.4895. Learning rate = 0.001. Patience = 8\n",
      "Epoch 44: training loss = 0.2612, validation loss = 0.4908. Learning rate = 0.001. Patience = 9\n",
      "Epoch 45: training loss = 0.2604, validation loss = 0.5201. Learning rate = 0.001. Patience = 10\n",
      "Epoch 46: training loss = 0.2688, validation loss = 0.4927. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 47: training loss = 0.2825, validation loss = 0.4782. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 48: training loss = 0.2714, validation loss = 0.4651. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 49: training loss = 0.2614, validation loss = 0.5413. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 50: training loss = 0.2567, validation loss = 0.5134. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 51: training loss = 0.2802, validation loss = 0.4564. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 52: training loss = 0.2539, validation loss = 0.4868. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 53: training loss = 0.2513, validation loss = 0.5220. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 54: training loss = 0.2642, validation loss = 0.4806. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 55: training loss = 0.2571, validation loss = 0.5089. Learning rate = 0.0005. Patience = 20\n",
      "Epoch 56: training loss = 0.2686, validation loss = 0.5014. Learning rate = 0.0005. Patience = 21\n",
      "Epoch 57: training loss = 0.2651, validation loss = 0.4911. Learning rate = 0.00025. Patience = 22\n",
      "Epoch 58: training loss = 0.2579, validation loss = 0.5070. Learning rate = 0.00025. Patience = 23\n",
      "Epoch 59: training loss = 0.2520, validation loss = 0.4947. Learning rate = 0.00025. Patience = 24\n",
      "Epoch 60: training loss = 0.2498, validation loss = 0.5111. Learning rate = 0.00025. Patience = 25\n",
      "Epoch 61: training loss = 0.2574, validation loss = 0.4986. Learning rate = 0.00025. Patience = 26\n",
      "Epoch 62: training loss = 0.2674, validation loss = 0.4790. Learning rate = 0.00025. Patience = 27\n",
      "Epoch 63: training loss = 0.2489, validation loss = 0.4934. Learning rate = 0.00025. Patience = 28\n",
      "Epoch 64: training loss = 0.2423, validation loss = 0.4945. Learning rate = 0.00025. Patience = 29\n",
      "Epoch 65: training loss = 0.2418, validation loss = 0.5010. Learning rate = 0.00025. Patience = 30\n",
      "Epoch 66: training loss = 0.2639, validation loss = 0.4964. Learning rate = 0.00025. Patience = 31\n",
      "Epoch 67: training loss = 0.2362, validation loss = 0.4997. Learning rate = 0.00025. Patience = 32\n",
      "Epoch 68: training loss = 0.2445, validation loss = 0.5276. Learning rate = 0.000125. Patience = 33\n",
      "Epoch 69: training loss = 0.2337, validation loss = 0.4908. Learning rate = 0.000125. Patience = 34\n",
      "Epoch 70: training loss = 0.2377, validation loss = 0.4888. Learning rate = 0.000125. Patience = 35\n",
      "Epoch 71: training loss = 0.2417, validation loss = 0.4966. Learning rate = 0.000125. Patience = 36\n",
      "Epoch 72: training loss = 0.2358, validation loss = 0.5010. Learning rate = 0.000125. Patience = 37\n",
      "Epoch 73: training loss = 0.2371, validation loss = 0.5045. Learning rate = 0.000125. Patience = 38\n",
      "Epoch 74: training loss = 0.2516, validation loss = 0.4763. Learning rate = 0.000125. Patience = 39\n",
      "Epoch 75: training loss = 0.2471, validation loss = 0.4778. Learning rate = 0.000125. Patience = 40\n",
      "Epoch 76: training loss = 0.2373, validation loss = 0.4895. Learning rate = 0.000125. Patience = 41\n",
      "Epoch 77: training loss = 0.2545, validation loss = 0.5024. Learning rate = 0.000125. Patience = 42\n",
      "Epoch 78: training loss = 0.2289, validation loss = 0.4931. Learning rate = 0.000125. Patience = 43\n",
      "Epoch 79: training loss = 0.2358, validation loss = 0.5052. Learning rate = 6.25e-05. Patience = 44\n",
      "Epoch 80: training loss = 0.2359, validation loss = 0.5072. Learning rate = 6.25e-05. Patience = 45\n",
      "Epoch 81: training loss = 0.2364, validation loss = 0.5025. Learning rate = 6.25e-05. Patience = 46\n",
      "Epoch 82: training loss = 0.2457, validation loss = 0.5018. Learning rate = 6.25e-05. Patience = 47\n",
      "Epoch 83: training loss = 0.2369, validation loss = 0.5017. Learning rate = 6.25e-05. Patience = 48\n",
      "Epoch 84: training loss = 0.2455, validation loss = 0.5109. Learning rate = 6.25e-05. Patience = 49\n",
      "Epoch 85: training loss = 0.2467, validation loss = 0.5146. Learning rate = 6.25e-05. Patience = 50\n",
      "Training stopped at epoch 85; restoring weights from epoch 35\n",
      "Epoch 1: training loss = 0.8152, validation loss = 0.8186. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.6505, validation loss = 0.6240. Learning rate = 0.001. Patience = 0\n",
      "Epoch 3: training loss = 0.5174, validation loss = 0.6599. Learning rate = 0.001. Patience = 1\n",
      "Epoch 4: training loss = 0.4990, validation loss = 0.6148. Learning rate = 0.001. Patience = 0\n",
      "Epoch 5: training loss = 0.4876, validation loss = 0.5589. Learning rate = 0.001. Patience = 0\n",
      "Epoch 6: training loss = 0.4830, validation loss = 0.5896. Learning rate = 0.001. Patience = 1\n",
      "Epoch 7: training loss = 0.4535, validation loss = 0.5486. Learning rate = 0.001. Patience = 0\n",
      "Epoch 8: training loss = 0.4434, validation loss = 0.5617. Learning rate = 0.001. Patience = 1\n",
      "Epoch 9: training loss = 0.4329, validation loss = 0.5427. Learning rate = 0.001. Patience = 0\n",
      "Epoch 10: training loss = 0.4291, validation loss = 0.5708. Learning rate = 0.001. Patience = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: training loss = 0.4464, validation loss = 0.5558. Learning rate = 0.001. Patience = 2\n",
      "Epoch 12: training loss = 0.4237, validation loss = 0.5336. Learning rate = 0.001. Patience = 0\n",
      "Epoch 13: training loss = 0.4114, validation loss = 0.5521. Learning rate = 0.001. Patience = 1\n",
      "Epoch 14: training loss = 0.4355, validation loss = 0.5527. Learning rate = 0.001. Patience = 2\n",
      "Epoch 15: training loss = 0.4221, validation loss = 0.5441. Learning rate = 0.001. Patience = 3\n",
      "Epoch 16: training loss = 0.4065, validation loss = 0.5500. Learning rate = 0.001. Patience = 4\n",
      "Epoch 17: training loss = 0.4091, validation loss = 0.5303. Learning rate = 0.001. Patience = 0\n",
      "Epoch 18: training loss = 0.3974, validation loss = 0.5356. Learning rate = 0.001. Patience = 1\n",
      "Epoch 19: training loss = 0.3946, validation loss = 0.5331. Learning rate = 0.001. Patience = 2\n",
      "Epoch 20: training loss = 0.3735, validation loss = 0.5316. Learning rate = 0.001. Patience = 3\n",
      "Epoch 21: training loss = 0.3891, validation loss = 0.5471. Learning rate = 0.001. Patience = 4\n",
      "Epoch 22: training loss = 0.3808, validation loss = 0.5314. Learning rate = 0.001. Patience = 5\n",
      "Epoch 23: training loss = 0.3603, validation loss = 0.5412. Learning rate = 0.001. Patience = 6\n",
      "Epoch 24: training loss = 0.3637, validation loss = 0.5611. Learning rate = 0.001. Patience = 7\n",
      "Epoch 25: training loss = 0.3650, validation loss = 0.5368. Learning rate = 0.001. Patience = 8\n",
      "Epoch 26: training loss = 0.3678, validation loss = 0.5299. Learning rate = 0.001. Patience = 0\n",
      "Epoch 27: training loss = 0.3478, validation loss = 0.5501. Learning rate = 0.001. Patience = 1\n",
      "Epoch 28: training loss = 0.3533, validation loss = 0.5696. Learning rate = 0.001. Patience = 2\n",
      "Epoch 29: training loss = 0.3440, validation loss = 0.5372. Learning rate = 0.001. Patience = 3\n",
      "Epoch 30: training loss = 0.3530, validation loss = 0.5627. Learning rate = 0.001. Patience = 4\n",
      "Epoch 31: training loss = 0.3552, validation loss = 0.5560. Learning rate = 0.001. Patience = 5\n",
      "Epoch 32: training loss = 0.3384, validation loss = 0.5468. Learning rate = 0.001. Patience = 6\n",
      "Epoch 33: training loss = 0.3365, validation loss = 0.5555. Learning rate = 0.001. Patience = 7\n",
      "Epoch 34: training loss = 0.3521, validation loss = 0.5587. Learning rate = 0.001. Patience = 8\n",
      "Epoch 35: training loss = 0.3362, validation loss = 0.5677. Learning rate = 0.001. Patience = 9\n",
      "Epoch 36: training loss = 0.3331, validation loss = 0.5751. Learning rate = 0.001. Patience = 10\n",
      "Epoch 37: training loss = 0.3354, validation loss = 0.5983. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 38: training loss = 0.3199, validation loss = 0.5762. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 39: training loss = 0.3240, validation loss = 0.5865. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 40: training loss = 0.3153, validation loss = 0.5847. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 41: training loss = 0.3011, validation loss = 0.5831. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 42: training loss = 0.3086, validation loss = 0.5844. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 43: training loss = 0.3069, validation loss = 0.6065. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 44: training loss = 0.3128, validation loss = 0.5948. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 45: training loss = 0.3250, validation loss = 0.5870. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 46: training loss = 0.3126, validation loss = 0.5950. Learning rate = 0.0005. Patience = 20\n",
      "Epoch 47: training loss = 0.3013, validation loss = 0.6111. Learning rate = 0.0005. Patience = 21\n",
      "Epoch 48: training loss = 0.3038, validation loss = 0.5953. Learning rate = 0.00025. Patience = 22\n",
      "Epoch 49: training loss = 0.3041, validation loss = 0.6054. Learning rate = 0.00025. Patience = 23\n",
      "Epoch 50: training loss = 0.3095, validation loss = 0.6049. Learning rate = 0.00025. Patience = 24\n",
      "Epoch 51: training loss = 0.3133, validation loss = 0.5972. Learning rate = 0.00025. Patience = 25\n",
      "Epoch 52: training loss = 0.2949, validation loss = 0.6043. Learning rate = 0.00025. Patience = 26\n",
      "Epoch 53: training loss = 0.2975, validation loss = 0.6127. Learning rate = 0.00025. Patience = 27\n",
      "Epoch 54: training loss = 0.2962, validation loss = 0.6070. Learning rate = 0.00025. Patience = 28\n",
      "Epoch 55: training loss = 0.2930, validation loss = 0.6026. Learning rate = 0.00025. Patience = 29\n",
      "Epoch 56: training loss = 0.2988, validation loss = 0.6011. Learning rate = 0.00025. Patience = 30\n",
      "Epoch 57: training loss = 0.3030, validation loss = 0.6171. Learning rate = 0.00025. Patience = 31\n",
      "Epoch 58: training loss = 0.2955, validation loss = 0.6053. Learning rate = 0.00025. Patience = 32\n",
      "Epoch 59: training loss = 0.2873, validation loss = 0.6142. Learning rate = 0.000125. Patience = 33\n",
      "Epoch 60: training loss = 0.3011, validation loss = 0.6135. Learning rate = 0.000125. Patience = 34\n",
      "Epoch 61: training loss = 0.2911, validation loss = 0.6172. Learning rate = 0.000125. Patience = 35\n",
      "Epoch 62: training loss = 0.2896, validation loss = 0.6233. Learning rate = 0.000125. Patience = 36\n",
      "Epoch 63: training loss = 0.2943, validation loss = 0.6220. Learning rate = 0.000125. Patience = 37\n",
      "Epoch 64: training loss = 0.2896, validation loss = 0.6211. Learning rate = 0.000125. Patience = 38\n",
      "Epoch 65: training loss = 0.3028, validation loss = 0.6211. Learning rate = 0.000125. Patience = 39\n",
      "Epoch 66: training loss = 0.3000, validation loss = 0.6257. Learning rate = 0.000125. Patience = 40\n",
      "Epoch 67: training loss = 0.2862, validation loss = 0.6208. Learning rate = 0.000125. Patience = 41\n",
      "Epoch 68: training loss = 0.2822, validation loss = 0.6256. Learning rate = 0.000125. Patience = 42\n",
      "Epoch 69: training loss = 0.2852, validation loss = 0.6265. Learning rate = 0.000125. Patience = 43\n",
      "Epoch 70: training loss = 0.2827, validation loss = 0.6253. Learning rate = 6.25e-05. Patience = 44\n",
      "Epoch 71: training loss = 0.2868, validation loss = 0.6223. Learning rate = 6.25e-05. Patience = 45\n",
      "Epoch 72: training loss = 0.3000, validation loss = 0.6193. Learning rate = 6.25e-05. Patience = 46\n",
      "Epoch 73: training loss = 0.2911, validation loss = 0.6194. Learning rate = 6.25e-05. Patience = 47\n",
      "Epoch 74: training loss = 0.2933, validation loss = 0.6220. Learning rate = 6.25e-05. Patience = 48\n",
      "Epoch 75: training loss = 0.2780, validation loss = 0.6250. Learning rate = 6.25e-05. Patience = 49\n",
      "Epoch 76: training loss = 0.2902, validation loss = 0.6278. Learning rate = 6.25e-05. Patience = 50\n",
      "Training stopped at epoch 76; restoring weights from epoch 26\n",
      "Epoch 1: training loss = 0.9120, validation loss = 0.7750. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.6168, validation loss = 0.4919. Learning rate = 0.001. Patience = 0\n",
      "Epoch 3: training loss = 0.4470, validation loss = 0.4615. Learning rate = 0.001. Patience = 0\n",
      "Epoch 4: training loss = 0.4459, validation loss = 0.4592. Learning rate = 0.001. Patience = 0\n",
      "Epoch 5: training loss = 0.4559, validation loss = 0.4510. Learning rate = 0.001. Patience = 0\n",
      "Epoch 6: training loss = 0.4342, validation loss = 0.4434. Learning rate = 0.001. Patience = 0\n",
      "Epoch 7: training loss = 0.4067, validation loss = 0.4347. Learning rate = 0.001. Patience = 0\n",
      "Epoch 8: training loss = 0.4116, validation loss = 0.4427. Learning rate = 0.001. Patience = 1\n",
      "Epoch 9: training loss = 0.3648, validation loss = 0.4312. Learning rate = 0.001. Patience = 0\n",
      "Epoch 10: training loss = 0.3998, validation loss = 0.4218. Learning rate = 0.001. Patience = 0\n",
      "Epoch 11: training loss = 0.4063, validation loss = 0.4208. Learning rate = 0.001. Patience = 0\n",
      "Epoch 12: training loss = 0.3337, validation loss = 0.4202. Learning rate = 0.001. Patience = 0\n",
      "Epoch 13: training loss = 0.3483, validation loss = 0.4317. Learning rate = 0.001. Patience = 1\n",
      "Epoch 14: training loss = 0.3117, validation loss = 0.4084. Learning rate = 0.001. Patience = 0\n",
      "Epoch 15: training loss = 0.3199, validation loss = 0.4195. Learning rate = 0.001. Patience = 1\n",
      "Epoch 16: training loss = 0.3096, validation loss = 0.4230. Learning rate = 0.001. Patience = 2\n",
      "Epoch 17: training loss = 0.3170, validation loss = 0.4120. Learning rate = 0.001. Patience = 3\n",
      "Epoch 18: training loss = 0.3176, validation loss = 0.4131. Learning rate = 0.001. Patience = 4\n",
      "Epoch 19: training loss = 0.2973, validation loss = 0.4081. Learning rate = 0.001. Patience = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: training loss = 0.2661, validation loss = 0.4168. Learning rate = 0.001. Patience = 1\n",
      "Epoch 21: training loss = 0.2767, validation loss = 0.4127. Learning rate = 0.001. Patience = 2\n",
      "Epoch 22: training loss = 0.2773, validation loss = 0.3921. Learning rate = 0.001. Patience = 0\n",
      "Epoch 23: training loss = 0.2590, validation loss = 0.4130. Learning rate = 0.001. Patience = 1\n",
      "Epoch 24: training loss = 0.2556, validation loss = 0.3979. Learning rate = 0.001. Patience = 2\n",
      "Epoch 25: training loss = 0.2538, validation loss = 0.3985. Learning rate = 0.001. Patience = 3\n",
      "Epoch 26: training loss = 0.2530, validation loss = 0.3859. Learning rate = 0.001. Patience = 0\n",
      "Epoch 27: training loss = 0.2658, validation loss = 0.4195. Learning rate = 0.001. Patience = 1\n",
      "Epoch 28: training loss = 0.2431, validation loss = 0.3782. Learning rate = 0.001. Patience = 0\n",
      "Epoch 29: training loss = 0.2478, validation loss = 0.3929. Learning rate = 0.001. Patience = 1\n",
      "Epoch 30: training loss = 0.2229, validation loss = 0.3831. Learning rate = 0.001. Patience = 2\n",
      "Epoch 31: training loss = 0.2451, validation loss = 0.3848. Learning rate = 0.001. Patience = 3\n",
      "Epoch 32: training loss = 0.2274, validation loss = 0.3752. Learning rate = 0.001. Patience = 0\n",
      "Epoch 33: training loss = 0.2286, validation loss = 0.3702. Learning rate = 0.001. Patience = 0\n",
      "Epoch 34: training loss = 0.2375, validation loss = 0.3967. Learning rate = 0.001. Patience = 1\n",
      "Epoch 35: training loss = 0.2253, validation loss = 0.3657. Learning rate = 0.001. Patience = 0\n",
      "Epoch 36: training loss = 0.2289, validation loss = 0.3579. Learning rate = 0.001. Patience = 0\n",
      "Epoch 37: training loss = 0.2033, validation loss = 0.3710. Learning rate = 0.001. Patience = 1\n",
      "Epoch 38: training loss = 0.2228, validation loss = 0.3468. Learning rate = 0.001. Patience = 0\n",
      "Epoch 39: training loss = 0.2147, validation loss = 0.3597. Learning rate = 0.001. Patience = 1\n",
      "Epoch 40: training loss = 0.2027, validation loss = 0.3585. Learning rate = 0.001. Patience = 2\n",
      "Epoch 41: training loss = 0.2095, validation loss = 0.3467. Learning rate = 0.001. Patience = 0\n",
      "Epoch 42: training loss = 0.2016, validation loss = 0.3552. Learning rate = 0.001. Patience = 1\n",
      "Epoch 43: training loss = 0.2118, validation loss = 0.3514. Learning rate = 0.001. Patience = 2\n",
      "Epoch 44: training loss = 0.2069, validation loss = 0.3486. Learning rate = 0.001. Patience = 3\n",
      "Epoch 45: training loss = 0.2056, validation loss = 0.3493. Learning rate = 0.001. Patience = 4\n",
      "Epoch 46: training loss = 0.1981, validation loss = 0.3484. Learning rate = 0.001. Patience = 5\n",
      "Epoch 47: training loss = 0.1940, validation loss = 0.3583. Learning rate = 0.001. Patience = 6\n",
      "Epoch 48: training loss = 0.1861, validation loss = 0.3383. Learning rate = 0.001. Patience = 0\n",
      "Epoch 49: training loss = 0.1818, validation loss = 0.3522. Learning rate = 0.001. Patience = 1\n",
      "Epoch 50: training loss = 0.1843, validation loss = 0.3699. Learning rate = 0.001. Patience = 2\n",
      "Epoch 51: training loss = 0.1812, validation loss = 0.3433. Learning rate = 0.001. Patience = 3\n",
      "Epoch 52: training loss = 0.1852, validation loss = 0.3495. Learning rate = 0.001. Patience = 4\n",
      "Epoch 53: training loss = 0.1854, validation loss = 0.3357. Learning rate = 0.001. Patience = 0\n",
      "Epoch 54: training loss = 0.1798, validation loss = 0.3514. Learning rate = 0.001. Patience = 1\n",
      "Epoch 55: training loss = 0.1696, validation loss = 0.3424. Learning rate = 0.001. Patience = 2\n",
      "Epoch 56: training loss = 0.1726, validation loss = 0.3382. Learning rate = 0.001. Patience = 3\n",
      "Epoch 57: training loss = 0.1916, validation loss = 0.3623. Learning rate = 0.001. Patience = 4\n",
      "Epoch 58: training loss = 0.1713, validation loss = 0.3353. Learning rate = 0.001. Patience = 0\n",
      "Epoch 59: training loss = 0.1745, validation loss = 0.3634. Learning rate = 0.001. Patience = 1\n",
      "Epoch 60: training loss = 0.1804, validation loss = 0.3418. Learning rate = 0.001. Patience = 2\n",
      "Epoch 61: training loss = 0.1662, validation loss = 0.3325. Learning rate = 0.001. Patience = 0\n",
      "Epoch 62: training loss = 0.1668, validation loss = 0.3454. Learning rate = 0.001. Patience = 1\n",
      "Epoch 63: training loss = 0.1769, validation loss = 0.3418. Learning rate = 0.001. Patience = 2\n",
      "Epoch 64: training loss = 0.1686, validation loss = 0.3421. Learning rate = 0.001. Patience = 3\n",
      "Epoch 65: training loss = 0.1668, validation loss = 0.3476. Learning rate = 0.001. Patience = 4\n",
      "Epoch 66: training loss = 0.1711, validation loss = 0.3396. Learning rate = 0.001. Patience = 5\n",
      "Epoch 67: training loss = 0.1659, validation loss = 0.3312. Learning rate = 0.001. Patience = 0\n",
      "Epoch 68: training loss = 0.1598, validation loss = 0.3653. Learning rate = 0.001. Patience = 1\n",
      "Epoch 69: training loss = 0.1665, validation loss = 0.3433. Learning rate = 0.001. Patience = 2\n",
      "Epoch 70: training loss = 0.1525, validation loss = 0.3427. Learning rate = 0.001. Patience = 3\n",
      "Epoch 71: training loss = 0.1651, validation loss = 0.3351. Learning rate = 0.001. Patience = 4\n",
      "Epoch 72: training loss = 0.1486, validation loss = 0.3322. Learning rate = 0.001. Patience = 5\n",
      "Epoch 73: training loss = 0.1557, validation loss = 0.3423. Learning rate = 0.001. Patience = 6\n",
      "Epoch 74: training loss = 0.1580, validation loss = 0.3368. Learning rate = 0.001. Patience = 7\n",
      "Epoch 75: training loss = 0.1586, validation loss = 0.3423. Learning rate = 0.001. Patience = 8\n",
      "Epoch 76: training loss = 0.1657, validation loss = 0.3444. Learning rate = 0.001. Patience = 9\n",
      "Epoch 77: training loss = 0.1516, validation loss = 0.3357. Learning rate = 0.001. Patience = 10\n",
      "Epoch 78: training loss = 0.1567, validation loss = 0.3648. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 79: training loss = 0.1492, validation loss = 0.3577. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 80: training loss = 0.1599, validation loss = 0.3600. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 81: training loss = 0.1602, validation loss = 0.3446. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 82: training loss = 0.1469, validation loss = 0.3539. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 83: training loss = 0.1732, validation loss = 0.3479. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 84: training loss = 0.1469, validation loss = 0.3507. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 85: training loss = 0.1449, validation loss = 0.3505. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 86: training loss = 0.1579, validation loss = 0.3427. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 87: training loss = 0.1493, validation loss = 0.3513. Learning rate = 0.0005. Patience = 20\n",
      "Epoch 88: training loss = 0.1461, validation loss = 0.3486. Learning rate = 0.0005. Patience = 21\n",
      "Epoch 89: training loss = 0.1500, validation loss = 0.3433. Learning rate = 0.00025. Patience = 22\n",
      "Epoch 90: training loss = 0.1502, validation loss = 0.3479. Learning rate = 0.00025. Patience = 23\n",
      "Epoch 91: training loss = 0.1659, validation loss = 0.3424. Learning rate = 0.00025. Patience = 24\n",
      "Epoch 92: training loss = 0.1429, validation loss = 0.3453. Learning rate = 0.00025. Patience = 25\n",
      "Epoch 93: training loss = 0.1558, validation loss = 0.3556. Learning rate = 0.00025. Patience = 26\n",
      "Epoch 94: training loss = 0.1484, validation loss = 0.3493. Learning rate = 0.00025. Patience = 27\n",
      "Epoch 95: training loss = 0.1481, validation loss = 0.3461. Learning rate = 0.00025. Patience = 28\n",
      "Epoch 96: training loss = 0.1558, validation loss = 0.3546. Learning rate = 0.00025. Patience = 29\n",
      "Epoch 97: training loss = 0.1392, validation loss = 0.3552. Learning rate = 0.00025. Patience = 30\n",
      "Epoch 98: training loss = 0.1465, validation loss = 0.3579. Learning rate = 0.00025. Patience = 31\n",
      "Epoch 99: training loss = 0.1583, validation loss = 0.3527. Learning rate = 0.00025. Patience = 32\n",
      "Epoch 100: training loss = 0.1468, validation loss = 0.3499. Learning rate = 0.000125. Patience = 33\n",
      "Epoch 101: training loss = 0.1563, validation loss = 0.3501. Learning rate = 0.000125. Patience = 34\n",
      "Epoch 102: training loss = 0.1377, validation loss = 0.3525. Learning rate = 0.000125. Patience = 35\n",
      "Epoch 103: training loss = 0.1391, validation loss = 0.3566. Learning rate = 0.000125. Patience = 36\n",
      "Epoch 104: training loss = 0.1650, validation loss = 0.3545. Learning rate = 0.000125. Patience = 37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105: training loss = 0.1404, validation loss = 0.3495. Learning rate = 0.000125. Patience = 38\n",
      "Epoch 106: training loss = 0.1445, validation loss = 0.3516. Learning rate = 0.000125. Patience = 39\n",
      "Epoch 107: training loss = 0.1392, validation loss = 0.3501. Learning rate = 0.000125. Patience = 40\n",
      "Epoch 108: training loss = 0.1369, validation loss = 0.3527. Learning rate = 0.000125. Patience = 41\n",
      "Epoch 109: training loss = 0.1644, validation loss = 0.3526. Learning rate = 0.000125. Patience = 42\n",
      "Epoch 110: training loss = 0.1486, validation loss = 0.3557. Learning rate = 0.000125. Patience = 43\n",
      "Epoch 111: training loss = 0.1544, validation loss = 0.3526. Learning rate = 6.25e-05. Patience = 44\n",
      "Epoch 112: training loss = 0.1493, validation loss = 0.3514. Learning rate = 6.25e-05. Patience = 45\n",
      "Epoch 113: training loss = 0.1456, validation loss = 0.3515. Learning rate = 6.25e-05. Patience = 46\n",
      "Epoch 114: training loss = 0.1426, validation loss = 0.3527. Learning rate = 6.25e-05. Patience = 47\n",
      "Epoch 115: training loss = 0.1481, validation loss = 0.3559. Learning rate = 6.25e-05. Patience = 48\n",
      "Epoch 116: training loss = 0.1370, validation loss = 0.3556. Learning rate = 6.25e-05. Patience = 49\n",
      "Epoch 117: training loss = 0.1371, validation loss = 0.3533. Learning rate = 6.25e-05. Patience = 50\n",
      "Training stopped at epoch 117; restoring weights from epoch 67\n",
      "Epoch 1: training loss = 0.6110, validation loss = 10.1501. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.5053, validation loss = 21.5844. Learning rate = 0.001. Patience = 1\n",
      "Epoch 3: training loss = 0.5363, validation loss = 20.8368. Learning rate = 0.001. Patience = 2\n",
      "Epoch 4: training loss = 0.5004, validation loss = 21.0308. Learning rate = 0.001. Patience = 3\n",
      "Epoch 5: training loss = 0.4921, validation loss = 34.0612. Learning rate = 0.001. Patience = 4\n",
      "Epoch 6: training loss = 0.4684, validation loss = 32.3489. Learning rate = 0.001. Patience = 5\n",
      "Epoch 7: training loss = 0.4707, validation loss = 41.3871. Learning rate = 0.001. Patience = 6\n",
      "Epoch 8: training loss = 0.4284, validation loss = 41.2534. Learning rate = 0.001. Patience = 7\n",
      "Epoch 9: training loss = 0.4208, validation loss = 42.9415. Learning rate = 0.001. Patience = 8\n",
      "Epoch 10: training loss = 0.4664, validation loss = 43.4617. Learning rate = 0.001. Patience = 9\n",
      "Epoch 11: training loss = 0.3943, validation loss = 46.9041. Learning rate = 0.001. Patience = 10\n",
      "Epoch 12: training loss = 0.4072, validation loss = 43.1609. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 13: training loss = 0.4270, validation loss = 44.5031. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 14: training loss = 0.3902, validation loss = 47.6686. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 15: training loss = 0.3972, validation loss = 47.3991. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 16: training loss = 0.4044, validation loss = 47.6564. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 17: training loss = 0.4024, validation loss = 44.5085. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 18: training loss = 0.3937, validation loss = 42.0879. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 19: training loss = 0.4213, validation loss = 42.3219. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 20: training loss = 0.3881, validation loss = 43.0374. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 21: training loss = 0.3808, validation loss = 40.4339. Learning rate = 0.0005. Patience = 20\n",
      "Epoch 22: training loss = 0.3784, validation loss = 41.7176. Learning rate = 0.0005. Patience = 21\n",
      "Epoch 23: training loss = 0.3819, validation loss = 43.2518. Learning rate = 0.00025. Patience = 22\n",
      "Epoch 24: training loss = 0.3740, validation loss = 42.6764. Learning rate = 0.00025. Patience = 23\n",
      "Epoch 25: training loss = 0.3691, validation loss = 41.6488. Learning rate = 0.00025. Patience = 24\n",
      "Epoch 26: training loss = 0.3794, validation loss = 41.9700. Learning rate = 0.00025. Patience = 25\n",
      "Epoch 27: training loss = 0.3684, validation loss = 44.2397. Learning rate = 0.00025. Patience = 26\n",
      "Epoch 28: training loss = 0.3744, validation loss = 41.0855. Learning rate = 0.00025. Patience = 27\n",
      "Epoch 29: training loss = 0.3805, validation loss = 40.0413. Learning rate = 0.00025. Patience = 28\n",
      "Epoch 30: training loss = 0.3777, validation loss = 40.6334. Learning rate = 0.00025. Patience = 29\n",
      "Epoch 31: training loss = 0.3751, validation loss = 39.9137. Learning rate = 0.00025. Patience = 30\n",
      "Epoch 32: training loss = 0.4252, validation loss = 39.6607. Learning rate = 0.00025. Patience = 31\n",
      "Epoch 33: training loss = 0.3737, validation loss = 42.4021. Learning rate = 0.00025. Patience = 32\n",
      "Epoch 34: training loss = 0.3821, validation loss = 40.7875. Learning rate = 0.000125. Patience = 33\n",
      "Epoch 35: training loss = 0.4097, validation loss = 38.4901. Learning rate = 0.000125. Patience = 34\n",
      "Epoch 36: training loss = 0.3719, validation loss = 38.5703. Learning rate = 0.000125. Patience = 35\n",
      "Epoch 37: training loss = 0.3843, validation loss = 39.6996. Learning rate = 0.000125. Patience = 36\n",
      "Epoch 38: training loss = 0.3823, validation loss = 40.4556. Learning rate = 0.000125. Patience = 37\n",
      "Epoch 39: training loss = 0.3957, validation loss = 41.3668. Learning rate = 0.000125. Patience = 38\n",
      "Epoch 40: training loss = 0.3724, validation loss = 39.9565. Learning rate = 0.000125. Patience = 39\n",
      "Epoch 41: training loss = 0.3846, validation loss = 40.2980. Learning rate = 0.000125. Patience = 40\n",
      "Epoch 42: training loss = 0.3790, validation loss = 39.9831. Learning rate = 0.000125. Patience = 41\n",
      "Epoch 43: training loss = 0.3709, validation loss = 38.7048. Learning rate = 0.000125. Patience = 42\n",
      "Epoch 44: training loss = 0.3545, validation loss = 40.2944. Learning rate = 0.000125. Patience = 43\n",
      "Epoch 45: training loss = 0.4008, validation loss = 39.0889. Learning rate = 6.25e-05. Patience = 44\n",
      "Epoch 46: training loss = 0.3542, validation loss = 38.8920. Learning rate = 6.25e-05. Patience = 45\n",
      "Epoch 47: training loss = 0.3938, validation loss = 39.4967. Learning rate = 6.25e-05. Patience = 46\n",
      "Epoch 48: training loss = 0.3673, validation loss = 39.2940. Learning rate = 6.25e-05. Patience = 47\n",
      "Epoch 49: training loss = 0.3746, validation loss = 39.1037. Learning rate = 6.25e-05. Patience = 48\n",
      "Epoch 50: training loss = 0.3882, validation loss = 38.5913. Learning rate = 6.25e-05. Patience = 49\n",
      "Epoch 51: training loss = 0.3845, validation loss = 37.9609. Learning rate = 6.25e-05. Patience = 50\n",
      "Training stopped at epoch 51; restoring weights from epoch 1\n",
      "Epoch 1: training loss = 0.7822, validation loss = 0.8348. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.5454, validation loss = 0.6397. Learning rate = 0.001. Patience = 0\n",
      "Epoch 3: training loss = 0.4954, validation loss = 0.5840. Learning rate = 0.001. Patience = 0\n",
      "Epoch 4: training loss = 0.5054, validation loss = 0.6121. Learning rate = 0.001. Patience = 1\n",
      "Epoch 5: training loss = 0.4668, validation loss = 0.5686. Learning rate = 0.001. Patience = 0\n",
      "Epoch 6: training loss = 0.4714, validation loss = 0.5835. Learning rate = 0.001. Patience = 1\n",
      "Epoch 7: training loss = 0.4267, validation loss = 0.5308. Learning rate = 0.001. Patience = 0\n",
      "Epoch 8: training loss = 0.4164, validation loss = 0.5557. Learning rate = 0.001. Patience = 1\n",
      "Epoch 9: training loss = 0.4337, validation loss = 0.5523. Learning rate = 0.001. Patience = 2\n",
      "Epoch 10: training loss = 0.4304, validation loss = 0.5374. Learning rate = 0.001. Patience = 3\n",
      "Epoch 11: training loss = 0.3988, validation loss = 0.5329. Learning rate = 0.001. Patience = 4\n",
      "Epoch 12: training loss = 0.3895, validation loss = 0.5308. Learning rate = 0.001. Patience = 5\n",
      "Epoch 13: training loss = 0.3826, validation loss = 0.5548. Learning rate = 0.001. Patience = 6\n",
      "Epoch 14: training loss = 0.3956, validation loss = 0.5236. Learning rate = 0.001. Patience = 0\n",
      "Epoch 15: training loss = 0.3789, validation loss = 0.5102. Learning rate = 0.001. Patience = 0\n",
      "Epoch 16: training loss = 0.3749, validation loss = 0.5300. Learning rate = 0.001. Patience = 1\n",
      "Epoch 17: training loss = 0.3697, validation loss = 0.5639. Learning rate = 0.001. Patience = 2\n",
      "Epoch 18: training loss = 0.3680, validation loss = 0.4998. Learning rate = 0.001. Patience = 0\n",
      "Epoch 19: training loss = 0.3785, validation loss = 0.5570. Learning rate = 0.001. Patience = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: training loss = 0.3762, validation loss = 0.5226. Learning rate = 0.001. Patience = 2\n",
      "Epoch 21: training loss = 0.3601, validation loss = 0.5286. Learning rate = 0.001. Patience = 3\n",
      "Epoch 22: training loss = 0.3512, validation loss = 0.5582. Learning rate = 0.001. Patience = 4\n",
      "Epoch 23: training loss = 0.3611, validation loss = 0.5303. Learning rate = 0.001. Patience = 5\n",
      "Epoch 24: training loss = 0.3608, validation loss = 0.5078. Learning rate = 0.001. Patience = 6\n",
      "Epoch 25: training loss = 0.3434, validation loss = 0.5268. Learning rate = 0.001. Patience = 7\n",
      "Epoch 26: training loss = 0.3435, validation loss = 0.5276. Learning rate = 0.001. Patience = 8\n",
      "Epoch 27: training loss = 0.3442, validation loss = 0.5019. Learning rate = 0.001. Patience = 9\n",
      "Epoch 28: training loss = 0.3324, validation loss = 0.5406. Learning rate = 0.001. Patience = 10\n",
      "Epoch 29: training loss = 0.3487, validation loss = 0.5061. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 30: training loss = 0.3319, validation loss = 0.5193. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 31: training loss = 0.3275, validation loss = 0.5238. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 32: training loss = 0.3254, validation loss = 0.5211. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 33: training loss = 0.3622, validation loss = 0.5199. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 34: training loss = 0.3204, validation loss = 0.5136. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 35: training loss = 0.3145, validation loss = 0.5325. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 36: training loss = 0.3201, validation loss = 0.5160. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 37: training loss = 0.3190, validation loss = 0.5305. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 38: training loss = 0.3117, validation loss = 0.5213. Learning rate = 0.0005. Patience = 20\n",
      "Epoch 39: training loss = 0.3312, validation loss = 0.5186. Learning rate = 0.0005. Patience = 21\n",
      "Epoch 40: training loss = 0.3161, validation loss = 0.5211. Learning rate = 0.00025. Patience = 22\n",
      "Epoch 41: training loss = 0.3066, validation loss = 0.5098. Learning rate = 0.00025. Patience = 23\n",
      "Epoch 42: training loss = 0.3062, validation loss = 0.5235. Learning rate = 0.00025. Patience = 24\n",
      "Epoch 43: training loss = 0.3120, validation loss = 0.5169. Learning rate = 0.00025. Patience = 25\n",
      "Epoch 44: training loss = 0.3017, validation loss = 0.5161. Learning rate = 0.00025. Patience = 26\n",
      "Epoch 45: training loss = 0.3223, validation loss = 0.5187. Learning rate = 0.00025. Patience = 27\n",
      "Epoch 46: training loss = 0.3065, validation loss = 0.5305. Learning rate = 0.00025. Patience = 28\n",
      "Epoch 47: training loss = 0.3190, validation loss = 0.5238. Learning rate = 0.00025. Patience = 29\n",
      "Epoch 48: training loss = 0.3220, validation loss = 0.5220. Learning rate = 0.00025. Patience = 30\n",
      "Epoch 49: training loss = 0.3015, validation loss = 0.5205. Learning rate = 0.00025. Patience = 31\n",
      "Epoch 50: training loss = 0.2985, validation loss = 0.5244. Learning rate = 0.00025. Patience = 32\n",
      "Epoch 51: training loss = 0.3241, validation loss = 0.5232. Learning rate = 0.000125. Patience = 33\n",
      "Epoch 52: training loss = 0.2984, validation loss = 0.5232. Learning rate = 0.000125. Patience = 34\n",
      "Epoch 53: training loss = 0.3060, validation loss = 0.5248. Learning rate = 0.000125. Patience = 35\n",
      "Epoch 54: training loss = 0.3043, validation loss = 0.5231. Learning rate = 0.000125. Patience = 36\n",
      "Epoch 55: training loss = 0.3008, validation loss = 0.5224. Learning rate = 0.000125. Patience = 37\n",
      "Epoch 56: training loss = 0.2966, validation loss = 0.5195. Learning rate = 0.000125. Patience = 38\n",
      "Epoch 57: training loss = 0.3118, validation loss = 0.5197. Learning rate = 0.000125. Patience = 39\n",
      "Epoch 58: training loss = 0.3080, validation loss = 0.5198. Learning rate = 0.000125. Patience = 40\n",
      "Epoch 59: training loss = 0.3018, validation loss = 0.5204. Learning rate = 0.000125. Patience = 41\n",
      "Epoch 60: training loss = 0.3291, validation loss = 0.5228. Learning rate = 0.000125. Patience = 42\n",
      "Epoch 61: training loss = 0.3065, validation loss = 0.5252. Learning rate = 0.000125. Patience = 43\n",
      "Epoch 62: training loss = 0.2892, validation loss = 0.5249. Learning rate = 6.25e-05. Patience = 44\n",
      "Epoch 63: training loss = 0.3132, validation loss = 0.5265. Learning rate = 6.25e-05. Patience = 45\n",
      "Epoch 64: training loss = 0.3041, validation loss = 0.5263. Learning rate = 6.25e-05. Patience = 46\n",
      "Epoch 65: training loss = 0.3030, validation loss = 0.5249. Learning rate = 6.25e-05. Patience = 47\n",
      "Epoch 66: training loss = 0.2923, validation loss = 0.5233. Learning rate = 6.25e-05. Patience = 48\n",
      "Epoch 67: training loss = 0.3171, validation loss = 0.5227. Learning rate = 6.25e-05. Patience = 49\n",
      "Epoch 68: training loss = 0.3192, validation loss = 0.5224. Learning rate = 6.25e-05. Patience = 50\n",
      "Training stopped at epoch 68; restoring weights from epoch 18\n",
      "Epoch 1: training loss = 0.8322, validation loss = 1.1835. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.5346, validation loss = 0.8843. Learning rate = 0.001. Patience = 0\n",
      "Epoch 3: training loss = 0.4622, validation loss = 0.9584. Learning rate = 0.001. Patience = 1\n",
      "Epoch 4: training loss = 0.4192, validation loss = 0.8268. Learning rate = 0.001. Patience = 0\n",
      "Epoch 5: training loss = 0.4100, validation loss = 0.8644. Learning rate = 0.001. Patience = 1\n",
      "Epoch 6: training loss = 0.3831, validation loss = 0.7819. Learning rate = 0.001. Patience = 0\n",
      "Epoch 7: training loss = 0.3817, validation loss = 0.8276. Learning rate = 0.001. Patience = 1\n",
      "Epoch 8: training loss = 0.3740, validation loss = 0.7917. Learning rate = 0.001. Patience = 2\n",
      "Epoch 9: training loss = 0.3703, validation loss = 0.7861. Learning rate = 0.001. Patience = 3\n",
      "Epoch 10: training loss = 0.3475, validation loss = 0.8094. Learning rate = 0.001. Patience = 4\n",
      "Epoch 11: training loss = 0.3571, validation loss = 0.7518. Learning rate = 0.001. Patience = 0\n",
      "Epoch 12: training loss = 0.3439, validation loss = 0.8524. Learning rate = 0.001. Patience = 1\n",
      "Epoch 13: training loss = 0.3333, validation loss = 0.8595. Learning rate = 0.001. Patience = 2\n",
      "Epoch 14: training loss = 0.3290, validation loss = 0.7797. Learning rate = 0.001. Patience = 3\n",
      "Epoch 15: training loss = 0.3232, validation loss = 0.7623. Learning rate = 0.001. Patience = 4\n",
      "Epoch 16: training loss = 0.3151, validation loss = 0.8155. Learning rate = 0.001. Patience = 5\n",
      "Epoch 17: training loss = 0.2993, validation loss = 0.8934. Learning rate = 0.001. Patience = 6\n",
      "Epoch 18: training loss = 0.3078, validation loss = 0.7852. Learning rate = 0.001. Patience = 7\n",
      "Epoch 19: training loss = 0.2936, validation loss = 0.7435. Learning rate = 0.001. Patience = 0\n",
      "Epoch 20: training loss = 0.2828, validation loss = 0.8412. Learning rate = 0.001. Patience = 1\n",
      "Epoch 21: training loss = 0.2947, validation loss = 0.8024. Learning rate = 0.001. Patience = 2\n",
      "Epoch 22: training loss = 0.2924, validation loss = 0.7767. Learning rate = 0.001. Patience = 3\n",
      "Epoch 23: training loss = 0.2801, validation loss = 0.8025. Learning rate = 0.001. Patience = 4\n",
      "Epoch 24: training loss = 0.2865, validation loss = 0.8769. Learning rate = 0.001. Patience = 5\n",
      "Epoch 25: training loss = 0.2766, validation loss = 0.8310. Learning rate = 0.001. Patience = 6\n",
      "Epoch 26: training loss = 0.2716, validation loss = 0.8271. Learning rate = 0.001. Patience = 7\n",
      "Epoch 27: training loss = 0.2655, validation loss = 0.8666. Learning rate = 0.001. Patience = 8\n",
      "Epoch 28: training loss = 0.2653, validation loss = 0.8812. Learning rate = 0.001. Patience = 9\n",
      "Epoch 29: training loss = 0.2633, validation loss = 0.8962. Learning rate = 0.001. Patience = 10\n",
      "Epoch 30: training loss = 0.2562, validation loss = 0.8096. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 31: training loss = 0.2644, validation loss = 0.8622. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 32: training loss = 0.2645, validation loss = 0.7791. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 33: training loss = 0.2630, validation loss = 0.8579. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 34: training loss = 0.2537, validation loss = 0.8193. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 35: training loss = 0.2511, validation loss = 0.8479. Learning rate = 0.0005. Patience = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: training loss = 0.2497, validation loss = 0.8195. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 37: training loss = 0.2440, validation loss = 0.8344. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 38: training loss = 0.2487, validation loss = 0.7969. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 39: training loss = 0.2496, validation loss = 0.8537. Learning rate = 0.0005. Patience = 20\n",
      "Epoch 40: training loss = 0.2452, validation loss = 0.8203. Learning rate = 0.0005. Patience = 21\n",
      "Epoch 41: training loss = 0.2469, validation loss = 0.8625. Learning rate = 0.00025. Patience = 22\n",
      "Epoch 42: training loss = 0.2453, validation loss = 0.8314. Learning rate = 0.00025. Patience = 23\n",
      "Epoch 43: training loss = 0.2497, validation loss = 0.8377. Learning rate = 0.00025. Patience = 24\n",
      "Epoch 44: training loss = 0.2352, validation loss = 0.8345. Learning rate = 0.00025. Patience = 25\n",
      "Epoch 45: training loss = 0.2411, validation loss = 0.8287. Learning rate = 0.00025. Patience = 26\n",
      "Epoch 46: training loss = 0.2462, validation loss = 0.8284. Learning rate = 0.00025. Patience = 27\n",
      "Epoch 47: training loss = 0.2407, validation loss = 0.8288. Learning rate = 0.00025. Patience = 28\n",
      "Epoch 48: training loss = 0.2373, validation loss = 0.8166. Learning rate = 0.00025. Patience = 29\n",
      "Epoch 49: training loss = 0.2383, validation loss = 0.8445. Learning rate = 0.00025. Patience = 30\n",
      "Epoch 50: training loss = 0.2444, validation loss = 0.8066. Learning rate = 0.00025. Patience = 31\n",
      "Epoch 51: training loss = 0.2349, validation loss = 0.8462. Learning rate = 0.00025. Patience = 32\n",
      "Epoch 52: training loss = 0.2313, validation loss = 0.8267. Learning rate = 0.000125. Patience = 33\n",
      "Epoch 53: training loss = 0.2461, validation loss = 0.8118. Learning rate = 0.000125. Patience = 34\n",
      "Epoch 54: training loss = 0.2306, validation loss = 0.8073. Learning rate = 0.000125. Patience = 35\n",
      "Epoch 55: training loss = 0.2342, validation loss = 0.8303. Learning rate = 0.000125. Patience = 36\n",
      "Epoch 56: training loss = 0.2471, validation loss = 0.8087. Learning rate = 0.000125. Patience = 37\n",
      "Epoch 57: training loss = 0.2487, validation loss = 0.8039. Learning rate = 0.000125. Patience = 38\n",
      "Epoch 58: training loss = 0.2363, validation loss = 0.8322. Learning rate = 0.000125. Patience = 39\n",
      "Epoch 59: training loss = 0.2291, validation loss = 0.8261. Learning rate = 0.000125. Patience = 40\n",
      "Epoch 60: training loss = 0.2382, validation loss = 0.8417. Learning rate = 0.000125. Patience = 41\n",
      "Epoch 61: training loss = 0.2310, validation loss = 0.8290. Learning rate = 0.000125. Patience = 42\n",
      "Epoch 62: training loss = 0.2423, validation loss = 0.8328. Learning rate = 0.000125. Patience = 43\n",
      "Epoch 63: training loss = 0.2328, validation loss = 0.8230. Learning rate = 6.25e-05. Patience = 44\n",
      "Epoch 64: training loss = 0.2350, validation loss = 0.8329. Learning rate = 6.25e-05. Patience = 45\n",
      "Epoch 65: training loss = 0.2311, validation loss = 0.8448. Learning rate = 6.25e-05. Patience = 46\n",
      "Epoch 66: training loss = 0.2376, validation loss = 0.8277. Learning rate = 6.25e-05. Patience = 47\n",
      "Epoch 67: training loss = 0.2320, validation loss = 0.8145. Learning rate = 6.25e-05. Patience = 48\n",
      "Epoch 68: training loss = 0.2424, validation loss = 0.8278. Learning rate = 6.25e-05. Patience = 49\n",
      "Epoch 69: training loss = 0.2314, validation loss = 0.8432. Learning rate = 6.25e-05. Patience = 50\n",
      "Training stopped at epoch 69; restoring weights from epoch 19\n",
      "Epoch 1: training loss = 0.6557, validation loss = 0.6609. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.4105, validation loss = 0.8394. Learning rate = 0.001. Patience = 1\n",
      "Epoch 3: training loss = 0.3134, validation loss = 0.5778. Learning rate = 0.001. Patience = 0\n",
      "Epoch 4: training loss = 0.3017, validation loss = 0.6946. Learning rate = 0.001. Patience = 1\n",
      "Epoch 5: training loss = 0.3001, validation loss = 0.3892. Learning rate = 0.001. Patience = 0\n",
      "Epoch 6: training loss = 0.2762, validation loss = 0.5939. Learning rate = 0.001. Patience = 1\n",
      "Epoch 7: training loss = 0.2644, validation loss = 0.3061. Learning rate = 0.001. Patience = 0\n",
      "Epoch 8: training loss = 0.2616, validation loss = 0.3810. Learning rate = 0.001. Patience = 1\n",
      "Epoch 9: training loss = 0.2613, validation loss = 0.2926. Learning rate = 0.001. Patience = 0\n",
      "Epoch 10: training loss = 0.2522, validation loss = 0.3585. Learning rate = 0.001. Patience = 1\n",
      "Epoch 11: training loss = 0.2411, validation loss = 0.3179. Learning rate = 0.001. Patience = 2\n",
      "Epoch 12: training loss = 0.2515, validation loss = 0.3081. Learning rate = 0.001. Patience = 3\n",
      "Epoch 13: training loss = 0.2525, validation loss = 0.3512. Learning rate = 0.001. Patience = 4\n",
      "Epoch 14: training loss = 0.2593, validation loss = 0.2775. Learning rate = 0.001. Patience = 0\n",
      "Epoch 15: training loss = 0.2376, validation loss = 0.3452. Learning rate = 0.001. Patience = 1\n",
      "Epoch 16: training loss = 0.2427, validation loss = 0.2928. Learning rate = 0.001. Patience = 2\n",
      "Epoch 17: training loss = 0.2537, validation loss = 0.3043. Learning rate = 0.001. Patience = 3\n",
      "Epoch 18: training loss = 0.2419, validation loss = 0.3070. Learning rate = 0.001. Patience = 4\n",
      "Epoch 19: training loss = 0.2494, validation loss = 0.3783. Learning rate = 0.001. Patience = 5\n",
      "Epoch 20: training loss = 0.2401, validation loss = 0.2348. Learning rate = 0.001. Patience = 0\n",
      "Epoch 21: training loss = 0.2367, validation loss = 0.4281. Learning rate = 0.001. Patience = 1\n",
      "Epoch 22: training loss = 0.2344, validation loss = 0.2513. Learning rate = 0.001. Patience = 2\n",
      "Epoch 23: training loss = 0.2330, validation loss = 0.3237. Learning rate = 0.001. Patience = 3\n",
      "Epoch 24: training loss = 0.2212, validation loss = 0.2958. Learning rate = 0.001. Patience = 4\n",
      "Epoch 25: training loss = 0.2228, validation loss = 0.2976. Learning rate = 0.001. Patience = 5\n",
      "Epoch 26: training loss = 0.2179, validation loss = 0.3157. Learning rate = 0.001. Patience = 6\n",
      "Epoch 27: training loss = 0.2183, validation loss = 0.2633. Learning rate = 0.001. Patience = 7\n",
      "Epoch 28: training loss = 0.2219, validation loss = 0.3011. Learning rate = 0.001. Patience = 8\n",
      "Epoch 29: training loss = 0.2202, validation loss = 0.3247. Learning rate = 0.001. Patience = 9\n",
      "Epoch 30: training loss = 0.2629, validation loss = 0.3478. Learning rate = 0.001. Patience = 10\n",
      "Epoch 31: training loss = 0.2379, validation loss = 0.2614. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 32: training loss = 0.2232, validation loss = 0.2955. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 33: training loss = 0.2267, validation loss = 0.2949. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 34: training loss = 0.2266, validation loss = 0.3200. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 35: training loss = 0.2377, validation loss = 0.3351. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 36: training loss = 0.2145, validation loss = 0.2760. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 37: training loss = 0.2321, validation loss = 0.2966. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 38: training loss = 0.2233, validation loss = 0.3075. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 39: training loss = 0.2081, validation loss = 0.3514. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 40: training loss = 0.2478, validation loss = 0.2703. Learning rate = 0.0005. Patience = 20\n",
      "Epoch 41: training loss = 0.2111, validation loss = 0.3434. Learning rate = 0.0005. Patience = 21\n",
      "Epoch 42: training loss = 0.2507, validation loss = 0.3166. Learning rate = 0.00025. Patience = 22\n",
      "Epoch 43: training loss = 0.2302, validation loss = 0.4167. Learning rate = 0.00025. Patience = 23\n",
      "Epoch 44: training loss = 0.2319, validation loss = 0.2759. Learning rate = 0.00025. Patience = 24\n",
      "Epoch 45: training loss = 0.2251, validation loss = 0.3437. Learning rate = 0.00025. Patience = 25\n",
      "Epoch 46: training loss = 0.2201, validation loss = 0.3251. Learning rate = 0.00025. Patience = 26\n",
      "Epoch 47: training loss = 0.2148, validation loss = 0.3504. Learning rate = 0.00025. Patience = 27\n",
      "Epoch 48: training loss = 0.2168, validation loss = 0.3809. Learning rate = 0.00025. Patience = 28\n",
      "Epoch 49: training loss = 0.2040, validation loss = 0.2713. Learning rate = 0.00025. Patience = 29\n",
      "Epoch 50: training loss = 0.2383, validation loss = 0.3447. Learning rate = 0.00025. Patience = 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: training loss = 0.2048, validation loss = 0.3304. Learning rate = 0.00025. Patience = 31\n",
      "Epoch 52: training loss = 0.2080, validation loss = 0.3183. Learning rate = 0.00025. Patience = 32\n",
      "Epoch 53: training loss = 0.2285, validation loss = 0.3386. Learning rate = 0.000125. Patience = 33\n",
      "Epoch 54: training loss = 0.2124, validation loss = 0.3172. Learning rate = 0.000125. Patience = 34\n",
      "Epoch 55: training loss = 0.2046, validation loss = 0.3281. Learning rate = 0.000125. Patience = 35\n",
      "Epoch 56: training loss = 0.1984, validation loss = 0.3049. Learning rate = 0.000125. Patience = 36\n",
      "Epoch 57: training loss = 0.2153, validation loss = 0.3225. Learning rate = 0.000125. Patience = 37\n",
      "Epoch 58: training loss = 0.2156, validation loss = 0.3253. Learning rate = 0.000125. Patience = 38\n",
      "Epoch 59: training loss = 0.2026, validation loss = 0.3152. Learning rate = 0.000125. Patience = 39\n",
      "Epoch 60: training loss = 0.2181, validation loss = 0.3084. Learning rate = 0.000125. Patience = 40\n",
      "Epoch 61: training loss = 0.2070, validation loss = 0.3064. Learning rate = 0.000125. Patience = 41\n",
      "Epoch 62: training loss = 0.2166, validation loss = 0.3770. Learning rate = 0.000125. Patience = 42\n",
      "Epoch 63: training loss = 0.2090, validation loss = 0.3502. Learning rate = 0.000125. Patience = 43\n",
      "Epoch 64: training loss = 0.2055, validation loss = 0.3189. Learning rate = 6.25e-05. Patience = 44\n",
      "Epoch 65: training loss = 0.2248, validation loss = 0.3164. Learning rate = 6.25e-05. Patience = 45\n",
      "Epoch 66: training loss = 0.2166, validation loss = 0.3210. Learning rate = 6.25e-05. Patience = 46\n",
      "Epoch 67: training loss = 0.2301, validation loss = 0.3240. Learning rate = 6.25e-05. Patience = 47\n",
      "Epoch 68: training loss = 0.2166, validation loss = 0.3600. Learning rate = 6.25e-05. Patience = 48\n",
      "Epoch 69: training loss = 0.2025, validation loss = 0.3428. Learning rate = 6.25e-05. Patience = 49\n",
      "Epoch 70: training loss = 0.2162, validation loss = 0.3235. Learning rate = 6.25e-05. Patience = 50\n",
      "Training stopped at epoch 70; restoring weights from epoch 20\n",
      "Epoch 1: training loss = 0.5618, validation loss = 0.5179. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.4345, validation loss = 0.7570. Learning rate = 0.001. Patience = 1\n",
      "Epoch 3: training loss = 0.3797, validation loss = 0.5137. Learning rate = 0.001. Patience = 0\n",
      "Epoch 4: training loss = 0.3676, validation loss = 0.4972. Learning rate = 0.001. Patience = 0\n",
      "Epoch 5: training loss = 0.3635, validation loss = 0.5475. Learning rate = 0.001. Patience = 1\n",
      "Epoch 6: training loss = 0.3249, validation loss = 0.4020. Learning rate = 0.001. Patience = 0\n",
      "Epoch 7: training loss = 0.3477, validation loss = 0.4058. Learning rate = 0.001. Patience = 1\n",
      "Epoch 8: training loss = 0.3350, validation loss = 0.3900. Learning rate = 0.001. Patience = 0\n",
      "Epoch 9: training loss = 0.3287, validation loss = 0.5133. Learning rate = 0.001. Patience = 1\n",
      "Epoch 10: training loss = 0.3848, validation loss = 0.3574. Learning rate = 0.001. Patience = 0\n",
      "Epoch 11: training loss = 0.3560, validation loss = 0.4171. Learning rate = 0.001. Patience = 1\n",
      "Epoch 12: training loss = 0.3043, validation loss = 0.4605. Learning rate = 0.001. Patience = 2\n",
      "Epoch 13: training loss = 0.3024, validation loss = 0.3539. Learning rate = 0.001. Patience = 0\n",
      "Epoch 14: training loss = 0.3387, validation loss = 0.4149. Learning rate = 0.001. Patience = 1\n",
      "Epoch 15: training loss = 0.3020, validation loss = 0.4035. Learning rate = 0.001. Patience = 2\n",
      "Epoch 16: training loss = 0.2866, validation loss = 0.3698. Learning rate = 0.001. Patience = 3\n",
      "Epoch 17: training loss = 0.2982, validation loss = 0.3547. Learning rate = 0.001. Patience = 4\n",
      "Epoch 18: training loss = 0.2899, validation loss = 0.3756. Learning rate = 0.001. Patience = 5\n",
      "Epoch 19: training loss = 0.3251, validation loss = 0.3344. Learning rate = 0.001. Patience = 0\n",
      "Epoch 20: training loss = 0.3126, validation loss = 0.3939. Learning rate = 0.001. Patience = 1\n",
      "Epoch 21: training loss = 0.2887, validation loss = 0.3100. Learning rate = 0.001. Patience = 0\n",
      "Epoch 22: training loss = 0.2714, validation loss = 0.3770. Learning rate = 0.001. Patience = 1\n",
      "Epoch 23: training loss = 0.2656, validation loss = 0.3180. Learning rate = 0.001. Patience = 2\n",
      "Epoch 24: training loss = 0.2687, validation loss = 0.3499. Learning rate = 0.001. Patience = 3\n",
      "Epoch 25: training loss = 0.2807, validation loss = 0.3208. Learning rate = 0.001. Patience = 4\n",
      "Epoch 26: training loss = 0.2814, validation loss = 0.3660. Learning rate = 0.001. Patience = 5\n",
      "Epoch 27: training loss = 0.2678, validation loss = 0.3474. Learning rate = 0.001. Patience = 6\n",
      "Epoch 28: training loss = 0.2588, validation loss = 0.3972. Learning rate = 0.001. Patience = 7\n",
      "Epoch 29: training loss = 0.2869, validation loss = 0.2884. Learning rate = 0.001. Patience = 0\n",
      "Epoch 30: training loss = 0.2911, validation loss = 0.4096. Learning rate = 0.001. Patience = 1\n",
      "Epoch 31: training loss = 0.2560, validation loss = 0.2970. Learning rate = 0.001. Patience = 2\n",
      "Epoch 32: training loss = 0.2578, validation loss = 0.3649. Learning rate = 0.001. Patience = 3\n",
      "Epoch 33: training loss = 0.2789, validation loss = 0.2913. Learning rate = 0.001. Patience = 4\n",
      "Epoch 34: training loss = 0.3063, validation loss = 0.3412. Learning rate = 0.001. Patience = 5\n",
      "Epoch 35: training loss = 0.2949, validation loss = 0.3211. Learning rate = 0.001. Patience = 6\n",
      "Epoch 36: training loss = 0.2787, validation loss = 0.3309. Learning rate = 0.001. Patience = 7\n",
      "Epoch 37: training loss = 0.2488, validation loss = 0.3133. Learning rate = 0.001. Patience = 8\n",
      "Epoch 38: training loss = 0.2417, validation loss = 0.3387. Learning rate = 0.001. Patience = 9\n",
      "Epoch 39: training loss = 0.2649, validation loss = 0.2915. Learning rate = 0.001. Patience = 10\n",
      "Epoch 40: training loss = 0.2674, validation loss = 0.3478. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 41: training loss = 0.2428, validation loss = 0.2973. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 42: training loss = 0.2565, validation loss = 0.3031. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 43: training loss = 0.2660, validation loss = 0.3163. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 44: training loss = 0.2597, validation loss = 0.2986. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 45: training loss = 0.2807, validation loss = 0.3266. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 46: training loss = 0.2592, validation loss = 0.3023. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 47: training loss = 0.2376, validation loss = 0.3136. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 48: training loss = 0.2769, validation loss = 0.3336. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 49: training loss = 0.2477, validation loss = 0.3250. Learning rate = 0.0005. Patience = 20\n",
      "Epoch 50: training loss = 0.2457, validation loss = 0.3007. Learning rate = 0.0005. Patience = 21\n",
      "Epoch 51: training loss = 0.3189, validation loss = 0.3300. Learning rate = 0.00025. Patience = 22\n",
      "Epoch 52: training loss = 0.2813, validation loss = 0.3186. Learning rate = 0.00025. Patience = 23\n",
      "Epoch 53: training loss = 0.2562, validation loss = 0.3091. Learning rate = 0.00025. Patience = 24\n",
      "Epoch 54: training loss = 0.2714, validation loss = 0.3182. Learning rate = 0.00025. Patience = 25\n",
      "Epoch 55: training loss = 0.2917, validation loss = 0.3470. Learning rate = 0.00025. Patience = 26\n",
      "Epoch 56: training loss = 0.2361, validation loss = 0.3136. Learning rate = 0.00025. Patience = 27\n",
      "Epoch 57: training loss = 0.2955, validation loss = 0.3056. Learning rate = 0.00025. Patience = 28\n",
      "Epoch 58: training loss = 0.2593, validation loss = 0.3171. Learning rate = 0.00025. Patience = 29\n",
      "Epoch 59: training loss = 0.2602, validation loss = 0.3246. Learning rate = 0.00025. Patience = 30\n",
      "Epoch 60: training loss = 0.2316, validation loss = 0.3071. Learning rate = 0.00025. Patience = 31\n",
      "Epoch 61: training loss = 0.2387, validation loss = 0.3108. Learning rate = 0.00025. Patience = 32\n",
      "Epoch 62: training loss = 0.2614, validation loss = 0.3143. Learning rate = 0.000125. Patience = 33\n",
      "Epoch 63: training loss = 0.2700, validation loss = 0.3111. Learning rate = 0.000125. Patience = 34\n",
      "Epoch 64: training loss = 0.2663, validation loss = 0.3090. Learning rate = 0.000125. Patience = 35\n",
      "Epoch 65: training loss = 0.2564, validation loss = 0.3126. Learning rate = 0.000125. Patience = 36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66: training loss = 0.2714, validation loss = 0.3193. Learning rate = 0.000125. Patience = 37\n",
      "Epoch 67: training loss = 0.2489, validation loss = 0.3129. Learning rate = 0.000125. Patience = 38\n",
      "Epoch 68: training loss = 0.2439, validation loss = 0.3122. Learning rate = 0.000125. Patience = 39\n",
      "Epoch 69: training loss = 0.2427, validation loss = 0.3220. Learning rate = 0.000125. Patience = 40\n",
      "Epoch 70: training loss = 0.2341, validation loss = 0.3196. Learning rate = 0.000125. Patience = 41\n",
      "Epoch 71: training loss = 0.2427, validation loss = 0.3202. Learning rate = 0.000125. Patience = 42\n",
      "Epoch 72: training loss = 0.2307, validation loss = 0.3116. Learning rate = 0.000125. Patience = 43\n",
      "Epoch 73: training loss = 0.2684, validation loss = 0.3122. Learning rate = 6.25e-05. Patience = 44\n",
      "Epoch 74: training loss = 0.2766, validation loss = 0.3136. Learning rate = 6.25e-05. Patience = 45\n",
      "Epoch 75: training loss = 0.2475, validation loss = 0.3142. Learning rate = 6.25e-05. Patience = 46\n",
      "Epoch 76: training loss = 0.2486, validation loss = 0.3142. Learning rate = 6.25e-05. Patience = 47\n",
      "Epoch 77: training loss = 0.2374, validation loss = 0.3154. Learning rate = 6.25e-05. Patience = 48\n",
      "Epoch 78: training loss = 0.2767, validation loss = 0.3122. Learning rate = 6.25e-05. Patience = 49\n",
      "Epoch 79: training loss = 0.2786, validation loss = 0.3157. Learning rate = 6.25e-05. Patience = 50\n",
      "Training stopped at epoch 79; restoring weights from epoch 29\n",
      "Epoch 1: training loss = 0.6430, validation loss = 0.4991. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.3563, validation loss = 0.3078. Learning rate = 0.001. Patience = 0\n",
      "Epoch 3: training loss = 0.2747, validation loss = 0.2877. Learning rate = 0.001. Patience = 0\n",
      "Epoch 4: training loss = 0.2398, validation loss = 0.2696. Learning rate = 0.001. Patience = 0\n",
      "Epoch 5: training loss = 0.2274, validation loss = 0.2736. Learning rate = 0.001. Patience = 1\n",
      "Epoch 6: training loss = 0.2087, validation loss = 0.2577. Learning rate = 0.001. Patience = 0\n",
      "Epoch 7: training loss = 0.2161, validation loss = 0.2513. Learning rate = 0.001. Patience = 0\n",
      "Epoch 8: training loss = 0.2114, validation loss = 0.2429. Learning rate = 0.001. Patience = 0\n",
      "Epoch 9: training loss = 0.2002, validation loss = 0.2395. Learning rate = 0.001. Patience = 0\n",
      "Epoch 10: training loss = 0.1985, validation loss = 0.2396. Learning rate = 0.001. Patience = 1\n",
      "Epoch 11: training loss = 0.1960, validation loss = 0.2312. Learning rate = 0.001. Patience = 0\n",
      "Epoch 12: training loss = 0.2049, validation loss = 0.2325. Learning rate = 0.001. Patience = 1\n",
      "Epoch 13: training loss = 0.1976, validation loss = 0.2274. Learning rate = 0.001. Patience = 0\n",
      "Epoch 14: training loss = 0.1925, validation loss = 0.2306. Learning rate = 0.001. Patience = 1\n",
      "Epoch 15: training loss = 0.1992, validation loss = 0.2257. Learning rate = 0.001. Patience = 0\n",
      "Epoch 16: training loss = 0.1841, validation loss = 0.2290. Learning rate = 0.001. Patience = 1\n",
      "Epoch 17: training loss = 0.1859, validation loss = 0.2266. Learning rate = 0.001. Patience = 2\n",
      "Epoch 18: training loss = 0.1812, validation loss = 0.2192. Learning rate = 0.001. Patience = 0\n",
      "Epoch 19: training loss = 0.1821, validation loss = 0.2198. Learning rate = 0.001. Patience = 1\n",
      "Epoch 20: training loss = 0.1869, validation loss = 0.2194. Learning rate = 0.001. Patience = 2\n",
      "Epoch 21: training loss = 0.1728, validation loss = 0.2185. Learning rate = 0.001. Patience = 0\n",
      "Epoch 22: training loss = 0.1713, validation loss = 0.2190. Learning rate = 0.001. Patience = 1\n",
      "Epoch 23: training loss = 0.1766, validation loss = 0.2253. Learning rate = 0.001. Patience = 2\n",
      "Epoch 24: training loss = 0.1796, validation loss = 0.2236. Learning rate = 0.001. Patience = 3\n",
      "Epoch 25: training loss = 0.1936, validation loss = 0.2207. Learning rate = 0.001. Patience = 4\n",
      "Epoch 26: training loss = 0.1755, validation loss = 0.2194. Learning rate = 0.001. Patience = 5\n",
      "Epoch 27: training loss = 0.1672, validation loss = 0.2238. Learning rate = 0.001. Patience = 6\n",
      "Epoch 28: training loss = 0.1667, validation loss = 0.2220. Learning rate = 0.001. Patience = 7\n",
      "Epoch 29: training loss = 0.1717, validation loss = 0.2153. Learning rate = 0.001. Patience = 0\n",
      "Epoch 30: training loss = 0.1717, validation loss = 0.2249. Learning rate = 0.001. Patience = 1\n",
      "Epoch 31: training loss = 0.1762, validation loss = 0.2258. Learning rate = 0.001. Patience = 2\n",
      "Epoch 32: training loss = 0.1728, validation loss = 0.2193. Learning rate = 0.001. Patience = 3\n",
      "Epoch 33: training loss = 0.1615, validation loss = 0.2299. Learning rate = 0.001. Patience = 4\n",
      "Epoch 34: training loss = 0.1663, validation loss = 0.2192. Learning rate = 0.001. Patience = 5\n",
      "Epoch 35: training loss = 0.1664, validation loss = 0.2265. Learning rate = 0.001. Patience = 6\n",
      "Epoch 36: training loss = 0.1661, validation loss = 0.2227. Learning rate = 0.001. Patience = 7\n",
      "Epoch 37: training loss = 0.1707, validation loss = 0.2226. Learning rate = 0.001. Patience = 8\n",
      "Epoch 38: training loss = 0.1688, validation loss = 0.2213. Learning rate = 0.001. Patience = 9\n",
      "Epoch 39: training loss = 0.1680, validation loss = 0.2229. Learning rate = 0.001. Patience = 10\n",
      "Epoch 40: training loss = 0.1605, validation loss = 0.2268. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 41: training loss = 0.1646, validation loss = 0.2240. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 42: training loss = 0.1587, validation loss = 0.2249. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 43: training loss = 0.1614, validation loss = 0.2272. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 44: training loss = 0.1597, validation loss = 0.2238. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 45: training loss = 0.1495, validation loss = 0.2193. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 46: training loss = 0.1661, validation loss = 0.2229. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 47: training loss = 0.1635, validation loss = 0.2188. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 48: training loss = 0.1540, validation loss = 0.2206. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 49: training loss = 0.1485, validation loss = 0.2208. Learning rate = 0.0005. Patience = 20\n",
      "Epoch 50: training loss = 0.1527, validation loss = 0.2180. Learning rate = 0.0005. Patience = 21\n",
      "Epoch 51: training loss = 0.1606, validation loss = 0.2192. Learning rate = 0.00025. Patience = 22\n",
      "Epoch 52: training loss = 0.1535, validation loss = 0.2228. Learning rate = 0.00025. Patience = 23\n",
      "Epoch 53: training loss = 0.1495, validation loss = 0.2225. Learning rate = 0.00025. Patience = 24\n",
      "Epoch 54: training loss = 0.1515, validation loss = 0.2209. Learning rate = 0.00025. Patience = 25\n",
      "Epoch 55: training loss = 0.1519, validation loss = 0.2194. Learning rate = 0.00025. Patience = 26\n",
      "Epoch 56: training loss = 0.1476, validation loss = 0.2197. Learning rate = 0.00025. Patience = 27\n",
      "Epoch 57: training loss = 0.1562, validation loss = 0.2201. Learning rate = 0.00025. Patience = 28\n",
      "Epoch 58: training loss = 0.1580, validation loss = 0.2226. Learning rate = 0.00025. Patience = 29\n",
      "Epoch 59: training loss = 0.1525, validation loss = 0.2217. Learning rate = 0.00025. Patience = 30\n",
      "Epoch 60: training loss = 0.1628, validation loss = 0.2205. Learning rate = 0.00025. Patience = 31\n",
      "Epoch 61: training loss = 0.1543, validation loss = 0.2190. Learning rate = 0.00025. Patience = 32\n",
      "Epoch 62: training loss = 0.1661, validation loss = 0.2216. Learning rate = 0.000125. Patience = 33\n",
      "Epoch 63: training loss = 0.1530, validation loss = 0.2254. Learning rate = 0.000125. Patience = 34\n",
      "Epoch 64: training loss = 0.1491, validation loss = 0.2225. Learning rate = 0.000125. Patience = 35\n",
      "Epoch 65: training loss = 0.1485, validation loss = 0.2201. Learning rate = 0.000125. Patience = 36\n",
      "Epoch 66: training loss = 0.1435, validation loss = 0.2201. Learning rate = 0.000125. Patience = 37\n",
      "Epoch 67: training loss = 0.1517, validation loss = 0.2210. Learning rate = 0.000125. Patience = 38\n",
      "Epoch 68: training loss = 0.1468, validation loss = 0.2215. Learning rate = 0.000125. Patience = 39\n",
      "Epoch 69: training loss = 0.1542, validation loss = 0.2217. Learning rate = 0.000125. Patience = 40\n",
      "Epoch 70: training loss = 0.1536, validation loss = 0.2210. Learning rate = 0.000125. Patience = 41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71: training loss = 0.1527, validation loss = 0.2219. Learning rate = 0.000125. Patience = 42\n",
      "Epoch 72: training loss = 0.1482, validation loss = 0.2217. Learning rate = 0.000125. Patience = 43\n",
      "Epoch 73: training loss = 0.1508, validation loss = 0.2205. Learning rate = 6.25e-05. Patience = 44\n",
      "Epoch 74: training loss = 0.1542, validation loss = 0.2200. Learning rate = 6.25e-05. Patience = 45\n",
      "Epoch 75: training loss = 0.1608, validation loss = 0.2198. Learning rate = 6.25e-05. Patience = 46\n",
      "Epoch 76: training loss = 0.1446, validation loss = 0.2204. Learning rate = 6.25e-05. Patience = 47\n",
      "Epoch 77: training loss = 0.1501, validation loss = 0.2206. Learning rate = 6.25e-05. Patience = 48\n",
      "Epoch 78: training loss = 0.1426, validation loss = 0.2213. Learning rate = 6.25e-05. Patience = 49\n",
      "Epoch 79: training loss = 0.1415, validation loss = 0.2208. Learning rate = 6.25e-05. Patience = 50\n",
      "Training stopped at epoch 79; restoring weights from epoch 29\n",
      "Epoch 1: training loss = 0.7767, validation loss = 1.0735. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.4544, validation loss = 0.7125. Learning rate = 0.001. Patience = 0\n",
      "Epoch 3: training loss = 0.3678, validation loss = 0.4564. Learning rate = 0.001. Patience = 0\n",
      "Epoch 4: training loss = 0.3408, validation loss = 0.3811. Learning rate = 0.001. Patience = 0\n",
      "Epoch 5: training loss = 0.3232, validation loss = 0.3713. Learning rate = 0.001. Patience = 0\n",
      "Epoch 6: training loss = 0.3136, validation loss = 0.4767. Learning rate = 0.001. Patience = 1\n",
      "Epoch 7: training loss = 0.3359, validation loss = 0.4097. Learning rate = 0.001. Patience = 2\n",
      "Epoch 8: training loss = 0.3151, validation loss = 0.4342. Learning rate = 0.001. Patience = 3\n",
      "Epoch 9: training loss = 0.3013, validation loss = 0.3894. Learning rate = 0.001. Patience = 4\n",
      "Epoch 10: training loss = 0.3183, validation loss = 0.3947. Learning rate = 0.001. Patience = 5\n",
      "Epoch 11: training loss = 0.3106, validation loss = 0.4403. Learning rate = 0.001. Patience = 6\n",
      "Epoch 12: training loss = 0.2912, validation loss = 0.4051. Learning rate = 0.001. Patience = 7\n",
      "Epoch 13: training loss = 0.2881, validation loss = 0.3914. Learning rate = 0.001. Patience = 8\n",
      "Epoch 14: training loss = 0.3064, validation loss = 0.3810. Learning rate = 0.001. Patience = 9\n",
      "Epoch 15: training loss = 0.2889, validation loss = 0.4030. Learning rate = 0.001. Patience = 10\n",
      "Epoch 16: training loss = 0.2770, validation loss = 0.4352. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 17: training loss = 0.2965, validation loss = 0.3594. Learning rate = 0.0005. Patience = 0\n",
      "Epoch 18: training loss = 0.2772, validation loss = 0.4046. Learning rate = 0.0005. Patience = 1\n",
      "Epoch 19: training loss = 0.3039, validation loss = 0.3857. Learning rate = 0.0005. Patience = 2\n",
      "Epoch 20: training loss = 0.2836, validation loss = 0.3866. Learning rate = 0.0005. Patience = 3\n",
      "Epoch 21: training loss = 0.2784, validation loss = 0.3814. Learning rate = 0.0005. Patience = 4\n",
      "Epoch 22: training loss = 0.2729, validation loss = 0.3840. Learning rate = 0.0005. Patience = 5\n",
      "Epoch 23: training loss = 0.2841, validation loss = 0.3701. Learning rate = 0.0005. Patience = 6\n",
      "Epoch 24: training loss = 0.2777, validation loss = 0.3528. Learning rate = 0.0005. Patience = 0\n",
      "Epoch 25: training loss = 0.2689, validation loss = 0.3898. Learning rate = 0.0005. Patience = 1\n",
      "Epoch 26: training loss = 0.2753, validation loss = 0.3677. Learning rate = 0.0005. Patience = 2\n",
      "Epoch 27: training loss = 0.2757, validation loss = 0.3682. Learning rate = 0.0005. Patience = 3\n",
      "Epoch 28: training loss = 0.2716, validation loss = 0.3809. Learning rate = 0.0005. Patience = 4\n",
      "Epoch 29: training loss = 0.2824, validation loss = 0.3759. Learning rate = 0.0005. Patience = 5\n",
      "Epoch 30: training loss = 0.2653, validation loss = 0.3901. Learning rate = 0.0005. Patience = 6\n",
      "Epoch 31: training loss = 0.2875, validation loss = 0.3551. Learning rate = 0.0005. Patience = 7\n",
      "Epoch 32: training loss = 0.2637, validation loss = 0.4230. Learning rate = 0.0005. Patience = 8\n",
      "Epoch 33: training loss = 0.2929, validation loss = 0.3579. Learning rate = 0.0005. Patience = 9\n",
      "Epoch 34: training loss = 0.2630, validation loss = 0.3580. Learning rate = 0.0005. Patience = 10\n",
      "Epoch 35: training loss = 0.2804, validation loss = 0.3753. Learning rate = 0.00025. Patience = 11\n",
      "Epoch 36: training loss = 0.2587, validation loss = 0.3473. Learning rate = 0.00025. Patience = 0\n",
      "Epoch 37: training loss = 0.2614, validation loss = 0.3687. Learning rate = 0.00025. Patience = 1\n",
      "Epoch 38: training loss = 0.2540, validation loss = 0.3859. Learning rate = 0.00025. Patience = 2\n",
      "Epoch 39: training loss = 0.2749, validation loss = 0.3595. Learning rate = 0.00025. Patience = 3\n",
      "Epoch 40: training loss = 0.2586, validation loss = 0.3751. Learning rate = 0.00025. Patience = 4\n",
      "Epoch 41: training loss = 0.2632, validation loss = 0.3657. Learning rate = 0.00025. Patience = 5\n",
      "Epoch 42: training loss = 0.2495, validation loss = 0.3504. Learning rate = 0.00025. Patience = 6\n",
      "Epoch 43: training loss = 0.2716, validation loss = 0.3764. Learning rate = 0.00025. Patience = 7\n",
      "Epoch 44: training loss = 0.2625, validation loss = 0.3461. Learning rate = 0.00025. Patience = 0\n",
      "Epoch 45: training loss = 0.2551, validation loss = 0.3655. Learning rate = 0.00025. Patience = 1\n",
      "Epoch 46: training loss = 0.2657, validation loss = 0.3455. Learning rate = 0.00025. Patience = 0\n",
      "Epoch 47: training loss = 0.2586, validation loss = 0.3882. Learning rate = 0.00025. Patience = 1\n",
      "Epoch 48: training loss = 0.2815, validation loss = 0.3776. Learning rate = 0.00025. Patience = 2\n",
      "Epoch 49: training loss = 0.2499, validation loss = 0.3573. Learning rate = 0.00025. Patience = 3\n",
      "Epoch 50: training loss = 0.2563, validation loss = 0.3540. Learning rate = 0.00025. Patience = 4\n",
      "Epoch 51: training loss = 0.2639, validation loss = 0.3623. Learning rate = 0.00025. Patience = 5\n",
      "Epoch 52: training loss = 0.2667, validation loss = 0.3495. Learning rate = 0.00025. Patience = 6\n",
      "Epoch 53: training loss = 0.2650, validation loss = 0.3596. Learning rate = 0.00025. Patience = 7\n",
      "Epoch 54: training loss = 0.2607, validation loss = 0.3626. Learning rate = 0.00025. Patience = 8\n",
      "Epoch 55: training loss = 0.2541, validation loss = 0.3555. Learning rate = 0.00025. Patience = 9\n",
      "Epoch 56: training loss = 0.2516, validation loss = 0.3536. Learning rate = 0.00025. Patience = 10\n",
      "Epoch 57: training loss = 0.2518, validation loss = 0.3541. Learning rate = 0.000125. Patience = 11\n",
      "Epoch 58: training loss = 0.2591, validation loss = 0.3624. Learning rate = 0.000125. Patience = 12\n",
      "Epoch 59: training loss = 0.2643, validation loss = 0.3508. Learning rate = 0.000125. Patience = 13\n",
      "Epoch 60: training loss = 0.2475, validation loss = 0.3529. Learning rate = 0.000125. Patience = 14\n",
      "Epoch 61: training loss = 0.2398, validation loss = 0.3611. Learning rate = 0.000125. Patience = 15\n",
      "Epoch 62: training loss = 0.2414, validation loss = 0.3573. Learning rate = 0.000125. Patience = 16\n",
      "Epoch 63: training loss = 0.2485, validation loss = 0.3521. Learning rate = 0.000125. Patience = 17\n",
      "Epoch 64: training loss = 0.2388, validation loss = 0.3602. Learning rate = 0.000125. Patience = 18\n",
      "Epoch 65: training loss = 0.2702, validation loss = 0.3515. Learning rate = 0.000125. Patience = 19\n",
      "Epoch 66: training loss = 0.2526, validation loss = 0.3531. Learning rate = 0.000125. Patience = 20\n",
      "Epoch 67: training loss = 0.2606, validation loss = 0.3699. Learning rate = 0.000125. Patience = 21\n",
      "Epoch 68: training loss = 0.2511, validation loss = 0.3604. Learning rate = 6.25e-05. Patience = 22\n",
      "Epoch 69: training loss = 0.2452, validation loss = 0.3612. Learning rate = 6.25e-05. Patience = 23\n",
      "Epoch 70: training loss = 0.2439, validation loss = 0.3572. Learning rate = 6.25e-05. Patience = 24\n",
      "Epoch 71: training loss = 0.2554, validation loss = 0.3641. Learning rate = 6.25e-05. Patience = 25\n",
      "Epoch 72: training loss = 0.2490, validation loss = 0.3536. Learning rate = 6.25e-05. Patience = 26\n",
      "Epoch 73: training loss = 0.2476, validation loss = 0.3485. Learning rate = 6.25e-05. Patience = 27\n",
      "Epoch 74: training loss = 0.2407, validation loss = 0.3588. Learning rate = 6.25e-05. Patience = 28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75: training loss = 0.2760, validation loss = 0.3592. Learning rate = 6.25e-05. Patience = 29\n",
      "Epoch 76: training loss = 0.2348, validation loss = 0.3630. Learning rate = 6.25e-05. Patience = 30\n",
      "Epoch 77: training loss = 0.2444, validation loss = 0.3549. Learning rate = 6.25e-05. Patience = 31\n",
      "Epoch 78: training loss = 0.2441, validation loss = 0.3568. Learning rate = 6.25e-05. Patience = 32\n",
      "Epoch 79: training loss = 0.2484, validation loss = 0.3574. Learning rate = 3.125e-05. Patience = 33\n",
      "Epoch 80: training loss = 0.2543, validation loss = 0.3588. Learning rate = 3.125e-05. Patience = 34\n",
      "Epoch 81: training loss = 0.2416, validation loss = 0.3557. Learning rate = 3.125e-05. Patience = 35\n",
      "Epoch 82: training loss = 0.2648, validation loss = 0.3540. Learning rate = 3.125e-05. Patience = 36\n",
      "Epoch 83: training loss = 0.2391, validation loss = 0.3546. Learning rate = 3.125e-05. Patience = 37\n",
      "Epoch 84: training loss = 0.2504, validation loss = 0.3554. Learning rate = 3.125e-05. Patience = 38\n",
      "Epoch 85: training loss = 0.2456, validation loss = 0.3510. Learning rate = 3.125e-05. Patience = 39\n",
      "Epoch 86: training loss = 0.2459, validation loss = 0.3561. Learning rate = 3.125e-05. Patience = 40\n",
      "Epoch 87: training loss = 0.2591, validation loss = 0.3560. Learning rate = 3.125e-05. Patience = 41\n",
      "Epoch 88: training loss = 0.2411, validation loss = 0.3604. Learning rate = 3.125e-05. Patience = 42\n",
      "Epoch 89: training loss = 0.2371, validation loss = 0.3584. Learning rate = 3.125e-05. Patience = 43\n",
      "Epoch 90: training loss = 0.2509, validation loss = 0.3569. Learning rate = 1.5625e-05. Patience = 44\n",
      "Epoch 91: training loss = 0.2488, validation loss = 0.3564. Learning rate = 1.5625e-05. Patience = 45\n",
      "Epoch 92: training loss = 0.2517, validation loss = 0.3552. Learning rate = 1.5625e-05. Patience = 46\n",
      "Epoch 93: training loss = 0.2428, validation loss = 0.3553. Learning rate = 1.5625e-05. Patience = 47\n",
      "Epoch 94: training loss = 0.2475, validation loss = 0.3573. Learning rate = 1.5625e-05. Patience = 48\n",
      "Epoch 95: training loss = 0.2506, validation loss = 0.3566. Learning rate = 1.5625e-05. Patience = 49\n",
      "Epoch 96: training loss = 0.2448, validation loss = 0.3571. Learning rate = 1.5625e-05. Patience = 50\n",
      "Training stopped at epoch 96; restoring weights from epoch 46\n",
      "Epoch 1: training loss = 0.6706, validation loss = 0.3040. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.3626, validation loss = 0.2697. Learning rate = 0.001. Patience = 0\n",
      "Epoch 3: training loss = 0.3116, validation loss = 0.4453. Learning rate = 0.001. Patience = 1\n",
      "Epoch 4: training loss = 0.3604, validation loss = 0.2739. Learning rate = 0.001. Patience = 2\n",
      "Epoch 5: training loss = 0.3173, validation loss = 0.2639. Learning rate = 0.001. Patience = 0\n",
      "Epoch 6: training loss = 0.2786, validation loss = 0.2230. Learning rate = 0.001. Patience = 0\n",
      "Epoch 7: training loss = 0.2942, validation loss = 0.1841. Learning rate = 0.001. Patience = 0\n",
      "Epoch 8: training loss = 0.2818, validation loss = 0.1605. Learning rate = 0.001. Patience = 0\n",
      "Epoch 9: training loss = 0.2701, validation loss = 0.1270. Learning rate = 0.001. Patience = 0\n",
      "Epoch 10: training loss = 0.2813, validation loss = 0.1574. Learning rate = 0.001. Patience = 1\n",
      "Epoch 11: training loss = 0.2547, validation loss = 0.1216. Learning rate = 0.001. Patience = 0\n",
      "Epoch 12: training loss = 0.2717, validation loss = 0.1490. Learning rate = 0.001. Patience = 1\n",
      "Epoch 13: training loss = 0.2504, validation loss = 0.1353. Learning rate = 0.001. Patience = 2\n",
      "Epoch 14: training loss = 0.2461, validation loss = 0.1219. Learning rate = 0.001. Patience = 3\n",
      "Epoch 15: training loss = 0.2682, validation loss = 0.1417. Learning rate = 0.001. Patience = 4\n",
      "Epoch 16: training loss = 0.2452, validation loss = 0.1321. Learning rate = 0.001. Patience = 5\n",
      "Epoch 17: training loss = 0.2354, validation loss = 0.1342. Learning rate = 0.001. Patience = 6\n",
      "Epoch 18: training loss = 0.2333, validation loss = 0.1340. Learning rate = 0.001. Patience = 7\n",
      "Epoch 19: training loss = 0.2716, validation loss = 0.1197. Learning rate = 0.001. Patience = 0\n",
      "Epoch 20: training loss = 0.2492, validation loss = 0.1200. Learning rate = 0.001. Patience = 1\n",
      "Epoch 21: training loss = 0.2533, validation loss = 0.1587. Learning rate = 0.001. Patience = 2\n",
      "Epoch 22: training loss = 0.2308, validation loss = 0.1188. Learning rate = 0.001. Patience = 0\n",
      "Epoch 23: training loss = 0.2659, validation loss = 0.1372. Learning rate = 0.001. Patience = 1\n",
      "Epoch 24: training loss = 0.2363, validation loss = 0.1231. Learning rate = 0.001. Patience = 2\n",
      "Epoch 25: training loss = 0.2261, validation loss = 0.1339. Learning rate = 0.001. Patience = 3\n",
      "Epoch 26: training loss = 0.2262, validation loss = 0.1223. Learning rate = 0.001. Patience = 4\n",
      "Epoch 27: training loss = 0.2428, validation loss = 0.1388. Learning rate = 0.001. Patience = 5\n",
      "Epoch 28: training loss = 0.2283, validation loss = 0.1130. Learning rate = 0.001. Patience = 0\n",
      "Epoch 29: training loss = 0.2401, validation loss = 0.1491. Learning rate = 0.001. Patience = 1\n",
      "Epoch 30: training loss = 0.2377, validation loss = 0.1142. Learning rate = 0.001. Patience = 2\n",
      "Epoch 31: training loss = 0.2188, validation loss = 0.1386. Learning rate = 0.001. Patience = 3\n",
      "Epoch 32: training loss = 0.2345, validation loss = 0.1566. Learning rate = 0.001. Patience = 4\n",
      "Epoch 33: training loss = 0.2327, validation loss = 0.1166. Learning rate = 0.001. Patience = 5\n",
      "Epoch 34: training loss = 0.2268, validation loss = 0.1770. Learning rate = 0.001. Patience = 6\n",
      "Epoch 35: training loss = 0.2284, validation loss = 0.1102. Learning rate = 0.001. Patience = 0\n",
      "Epoch 36: training loss = 0.2350, validation loss = 0.2139. Learning rate = 0.001. Patience = 1\n",
      "Epoch 37: training loss = 0.2355, validation loss = 0.1059. Learning rate = 0.001. Patience = 0\n",
      "Epoch 38: training loss = 0.2207, validation loss = 0.1412. Learning rate = 0.001. Patience = 1\n",
      "Epoch 39: training loss = 0.2526, validation loss = 0.1368. Learning rate = 0.001. Patience = 2\n",
      "Epoch 40: training loss = 0.2438, validation loss = 0.1360. Learning rate = 0.001. Patience = 3\n",
      "Epoch 41: training loss = 0.2127, validation loss = 0.1144. Learning rate = 0.001. Patience = 4\n",
      "Epoch 42: training loss = 0.2330, validation loss = 0.1781. Learning rate = 0.001. Patience = 5\n",
      "Epoch 43: training loss = 0.2262, validation loss = 0.1140. Learning rate = 0.001. Patience = 6\n",
      "Epoch 44: training loss = 0.2208, validation loss = 0.1944. Learning rate = 0.001. Patience = 7\n",
      "Epoch 45: training loss = 0.2320, validation loss = 0.1203. Learning rate = 0.001. Patience = 8\n",
      "Epoch 46: training loss = 0.2170, validation loss = 0.1436. Learning rate = 0.001. Patience = 9\n",
      "Epoch 47: training loss = 0.2526, validation loss = 0.1378. Learning rate = 0.001. Patience = 10\n",
      "Epoch 48: training loss = 0.2245, validation loss = 0.1437. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 49: training loss = 0.2607, validation loss = 0.1359. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 50: training loss = 0.2224, validation loss = 0.1409. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 51: training loss = 0.2151, validation loss = 0.1335. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 52: training loss = 0.2152, validation loss = 0.1323. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 53: training loss = 0.2026, validation loss = 0.1375. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 54: training loss = 0.2335, validation loss = 0.1569. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 55: training loss = 0.2362, validation loss = 0.1393. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 56: training loss = 0.2202, validation loss = 0.1337. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 57: training loss = 0.2154, validation loss = 0.1568. Learning rate = 0.0005. Patience = 20\n",
      "Epoch 58: training loss = 0.2123, validation loss = 0.1393. Learning rate = 0.0005. Patience = 21\n",
      "Epoch 59: training loss = 0.2147, validation loss = 0.1387. Learning rate = 0.00025. Patience = 22\n",
      "Epoch 60: training loss = 0.2046, validation loss = 0.1386. Learning rate = 0.00025. Patience = 23\n",
      "Epoch 61: training loss = 0.2203, validation loss = 0.1497. Learning rate = 0.00025. Patience = 24\n",
      "Epoch 62: training loss = 0.2056, validation loss = 0.1448. Learning rate = 0.00025. Patience = 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63: training loss = 0.2054, validation loss = 0.1339. Learning rate = 0.00025. Patience = 26\n",
      "Epoch 64: training loss = 0.1983, validation loss = 0.1489. Learning rate = 0.00025. Patience = 27\n",
      "Epoch 65: training loss = 0.2358, validation loss = 0.1428. Learning rate = 0.00025. Patience = 28\n",
      "Epoch 66: training loss = 0.2231, validation loss = 0.1593. Learning rate = 0.00025. Patience = 29\n",
      "Epoch 67: training loss = 0.2139, validation loss = 0.1404. Learning rate = 0.00025. Patience = 30\n",
      "Epoch 68: training loss = 0.2413, validation loss = 0.1448. Learning rate = 0.00025. Patience = 31\n",
      "Epoch 69: training loss = 0.2174, validation loss = 0.1653. Learning rate = 0.00025. Patience = 32\n",
      "Epoch 70: training loss = 0.2025, validation loss = 0.1478. Learning rate = 0.000125. Patience = 33\n",
      "Epoch 71: training loss = 0.1961, validation loss = 0.1447. Learning rate = 0.000125. Patience = 34\n",
      "Epoch 72: training loss = 0.2155, validation loss = 0.1448. Learning rate = 0.000125. Patience = 35\n",
      "Epoch 73: training loss = 0.2092, validation loss = 0.1555. Learning rate = 0.000125. Patience = 36\n",
      "Epoch 74: training loss = 0.2231, validation loss = 0.1505. Learning rate = 0.000125. Patience = 37\n",
      "Epoch 75: training loss = 0.2325, validation loss = 0.1475. Learning rate = 0.000125. Patience = 38\n",
      "Epoch 76: training loss = 0.2076, validation loss = 0.1413. Learning rate = 0.000125. Patience = 39\n",
      "Epoch 77: training loss = 0.2080, validation loss = 0.1472. Learning rate = 0.000125. Patience = 40\n",
      "Epoch 78: training loss = 0.2219, validation loss = 0.1508. Learning rate = 0.000125. Patience = 41\n",
      "Epoch 79: training loss = 0.1989, validation loss = 0.1557. Learning rate = 0.000125. Patience = 42\n",
      "Epoch 80: training loss = 0.2105, validation loss = 0.1510. Learning rate = 0.000125. Patience = 43\n",
      "Epoch 81: training loss = 0.2156, validation loss = 0.1562. Learning rate = 6.25e-05. Patience = 44\n",
      "Epoch 82: training loss = 0.2072, validation loss = 0.1570. Learning rate = 6.25e-05. Patience = 45\n",
      "Epoch 83: training loss = 0.2249, validation loss = 0.1559. Learning rate = 6.25e-05. Patience = 46\n",
      "Epoch 84: training loss = 0.2133, validation loss = 0.1534. Learning rate = 6.25e-05. Patience = 47\n",
      "Epoch 85: training loss = 0.2136, validation loss = 0.1500. Learning rate = 6.25e-05. Patience = 48\n",
      "Epoch 86: training loss = 0.2038, validation loss = 0.1478. Learning rate = 6.25e-05. Patience = 49\n",
      "Epoch 87: training loss = 0.2094, validation loss = 0.1496. Learning rate = 6.25e-05. Patience = 50\n",
      "Training stopped at epoch 87; restoring weights from epoch 37\n",
      "Epoch 1: training loss = 0.9691, validation loss = 11.9656. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.6986, validation loss = 11.7087. Learning rate = 0.001. Patience = 0\n",
      "Epoch 3: training loss = 0.6798, validation loss = 11.3813. Learning rate = 0.001. Patience = 0\n",
      "Epoch 4: training loss = 0.6335, validation loss = 11.5428. Learning rate = 0.001. Patience = 1\n",
      "Epoch 5: training loss = 0.8513, validation loss = 11.7726. Learning rate = 0.001. Patience = 2\n",
      "Epoch 6: training loss = 0.7158, validation loss = 10.9715. Learning rate = 0.001. Patience = 0\n",
      "Epoch 7: training loss = 0.5514, validation loss = 10.7076. Learning rate = 0.001. Patience = 0\n",
      "Epoch 8: training loss = 0.5205, validation loss = 11.2590. Learning rate = 0.001. Patience = 1\n",
      "Epoch 9: training loss = 0.4772, validation loss = 11.4602. Learning rate = 0.001. Patience = 2\n",
      "Epoch 10: training loss = 0.4416, validation loss = 11.5137. Learning rate = 0.001. Patience = 3\n",
      "Epoch 11: training loss = 0.4227, validation loss = 11.6490. Learning rate = 0.001. Patience = 4\n",
      "Epoch 12: training loss = 0.3997, validation loss = 12.0294. Learning rate = 0.001. Patience = 5\n",
      "Epoch 13: training loss = 0.3828, validation loss = 12.0232. Learning rate = 0.001. Patience = 6\n",
      "Epoch 14: training loss = 0.5626, validation loss = 11.8868. Learning rate = 0.001. Patience = 7\n",
      "Epoch 15: training loss = 0.3645, validation loss = 12.0967. Learning rate = 0.001. Patience = 8\n",
      "Epoch 16: training loss = 0.3624, validation loss = 12.2724. Learning rate = 0.001. Patience = 9\n",
      "Epoch 17: training loss = 0.4964, validation loss = 12.1297. Learning rate = 0.001. Patience = 10\n",
      "Epoch 18: training loss = 0.3693, validation loss = 12.0285. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 19: training loss = 0.3698, validation loss = 12.4090. Learning rate = 0.0005. Patience = 12\n",
      "Epoch 20: training loss = 0.3624, validation loss = 11.7912. Learning rate = 0.0005. Patience = 13\n",
      "Epoch 21: training loss = 0.3671, validation loss = 12.0311. Learning rate = 0.0005. Patience = 14\n",
      "Epoch 22: training loss = 0.3786, validation loss = 13.6450. Learning rate = 0.0005. Patience = 15\n",
      "Epoch 23: training loss = 0.3570, validation loss = 12.4501. Learning rate = 0.0005. Patience = 16\n",
      "Epoch 24: training loss = 0.3543, validation loss = 12.1189. Learning rate = 0.0005. Patience = 17\n",
      "Epoch 25: training loss = 0.3579, validation loss = 12.2068. Learning rate = 0.0005. Patience = 18\n",
      "Epoch 26: training loss = 0.3513, validation loss = 12.5949. Learning rate = 0.0005. Patience = 19\n",
      "Epoch 27: training loss = 0.3579, validation loss = 12.6006. Learning rate = 0.0005. Patience = 20\n",
      "Epoch 28: training loss = 0.3576, validation loss = 12.5471. Learning rate = 0.0005. Patience = 21\n",
      "Epoch 29: training loss = 0.3552, validation loss = 12.4070. Learning rate = 0.00025. Patience = 22\n",
      "Epoch 30: training loss = 0.3456, validation loss = 12.4339. Learning rate = 0.00025. Patience = 23\n",
      "Epoch 31: training loss = 0.3963, validation loss = 12.4978. Learning rate = 0.00025. Patience = 24\n",
      "Epoch 32: training loss = 0.3485, validation loss = 12.1573. Learning rate = 0.00025. Patience = 25\n",
      "Epoch 33: training loss = 0.3476, validation loss = 12.1804. Learning rate = 0.00025. Patience = 26\n",
      "Epoch 34: training loss = 0.3499, validation loss = 12.3712. Learning rate = 0.00025. Patience = 27\n",
      "Epoch 35: training loss = 0.3490, validation loss = 12.3093. Learning rate = 0.00025. Patience = 28\n",
      "Epoch 36: training loss = 0.3475, validation loss = 12.2317. Learning rate = 0.00025. Patience = 29\n",
      "Epoch 37: training loss = 0.3494, validation loss = 12.1441. Learning rate = 0.00025. Patience = 30\n",
      "Epoch 38: training loss = 0.3501, validation loss = 12.0780. Learning rate = 0.00025. Patience = 31\n",
      "Epoch 39: training loss = 0.3519, validation loss = 11.9326. Learning rate = 0.00025. Patience = 32\n",
      "Epoch 40: training loss = 0.5290, validation loss = 12.0817. Learning rate = 0.000125. Patience = 33\n",
      "Epoch 41: training loss = 0.3557, validation loss = 12.0382. Learning rate = 0.000125. Patience = 34\n",
      "Epoch 42: training loss = 0.3867, validation loss = 12.0291. Learning rate = 0.000125. Patience = 35\n",
      "Epoch 43: training loss = 0.3468, validation loss = 12.2356. Learning rate = 0.000125. Patience = 36\n",
      "Epoch 44: training loss = 0.3458, validation loss = 12.3402. Learning rate = 0.000125. Patience = 37\n",
      "Epoch 45: training loss = 0.3501, validation loss = 12.2368. Learning rate = 0.000125. Patience = 38\n",
      "Epoch 46: training loss = 0.3511, validation loss = 12.0971. Learning rate = 0.000125. Patience = 39\n",
      "Epoch 47: training loss = 0.3433, validation loss = 12.0645. Learning rate = 0.000125. Patience = 40\n",
      "Epoch 48: training loss = 0.5797, validation loss = 12.1369. Learning rate = 0.000125. Patience = 41\n",
      "Epoch 49: training loss = 0.3465, validation loss = 12.0139. Learning rate = 0.000125. Patience = 42\n",
      "Epoch 50: training loss = 0.3524, validation loss = 11.8843. Learning rate = 0.000125. Patience = 43\n",
      "Epoch 51: training loss = 0.3553, validation loss = 11.9052. Learning rate = 6.25e-05. Patience = 44\n",
      "Epoch 52: training loss = 0.5029, validation loss = 11.9551. Learning rate = 6.25e-05. Patience = 45\n",
      "Epoch 53: training loss = 0.5719, validation loss = 11.9869. Learning rate = 6.25e-05. Patience = 46\n",
      "Epoch 54: training loss = 0.4772, validation loss = 11.9899. Learning rate = 6.25e-05. Patience = 47\n",
      "Epoch 55: training loss = 0.3480, validation loss = 12.0300. Learning rate = 6.25e-05. Patience = 48\n",
      "Epoch 56: training loss = 0.3893, validation loss = 12.0673. Learning rate = 6.25e-05. Patience = 49\n",
      "Epoch 57: training loss = 0.5345, validation loss = 11.9874. Learning rate = 6.25e-05. Patience = 50\n",
      "Training stopped at epoch 57; restoring weights from epoch 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training loss = 0.8930, validation loss = 0.8276. Learning rate = 0.001. Patience = 0\n",
      "Epoch 2: training loss = 0.6104, validation loss = 0.5341. Learning rate = 0.001. Patience = 0\n",
      "Epoch 3: training loss = 0.4816, validation loss = 0.4295. Learning rate = 0.001. Patience = 0\n",
      "Epoch 4: training loss = 0.4631, validation loss = 0.3891. Learning rate = 0.001. Patience = 0\n",
      "Epoch 5: training loss = 0.4234, validation loss = 0.3926. Learning rate = 0.001. Patience = 1\n",
      "Epoch 6: training loss = 0.4165, validation loss = 0.3687. Learning rate = 0.001. Patience = 0\n",
      "Epoch 7: training loss = 0.4043, validation loss = 0.3598. Learning rate = 0.001. Patience = 0\n",
      "Epoch 8: training loss = 0.3963, validation loss = 0.3486. Learning rate = 0.001. Patience = 0\n",
      "Epoch 9: training loss = 0.4025, validation loss = 0.3438. Learning rate = 0.001. Patience = 0\n",
      "Epoch 10: training loss = 0.3927, validation loss = 0.3465. Learning rate = 0.001. Patience = 1\n",
      "Epoch 11: training loss = 0.3928, validation loss = 0.3322. Learning rate = 0.001. Patience = 0\n",
      "Epoch 12: training loss = 0.3760, validation loss = 0.3228. Learning rate = 0.001. Patience = 0\n",
      "Epoch 13: training loss = 0.3703, validation loss = 0.3256. Learning rate = 0.001. Patience = 1\n",
      "Epoch 14: training loss = 0.3606, validation loss = 0.3189. Learning rate = 0.001. Patience = 0\n",
      "Epoch 15: training loss = 0.3677, validation loss = 0.3131. Learning rate = 0.001. Patience = 0\n",
      "Epoch 16: training loss = 0.3488, validation loss = 0.3124. Learning rate = 0.001. Patience = 0\n",
      "Epoch 17: training loss = 0.3463, validation loss = 0.3113. Learning rate = 0.001. Patience = 0\n",
      "Epoch 18: training loss = 0.3557, validation loss = 0.3189. Learning rate = 0.001. Patience = 1\n",
      "Epoch 19: training loss = 0.3457, validation loss = 0.3125. Learning rate = 0.001. Patience = 2\n",
      "Epoch 20: training loss = 0.3313, validation loss = 0.3133. Learning rate = 0.001. Patience = 3\n",
      "Epoch 21: training loss = 0.3463, validation loss = 0.3012. Learning rate = 0.001. Patience = 0\n",
      "Epoch 22: training loss = 0.3346, validation loss = 0.3151. Learning rate = 0.001. Patience = 1\n",
      "Epoch 23: training loss = 0.3423, validation loss = 0.3026. Learning rate = 0.001. Patience = 2\n",
      "Epoch 24: training loss = 0.3298, validation loss = 0.3080. Learning rate = 0.001. Patience = 3\n",
      "Epoch 25: training loss = 0.3313, validation loss = 0.3097. Learning rate = 0.001. Patience = 4\n",
      "Epoch 26: training loss = 0.3173, validation loss = 0.3204. Learning rate = 0.001. Patience = 5\n",
      "Epoch 27: training loss = 0.3241, validation loss = 0.3007. Learning rate = 0.001. Patience = 0\n",
      "Epoch 28: training loss = 0.3189, validation loss = 0.3008. Learning rate = 0.001. Patience = 1\n",
      "Epoch 29: training loss = 0.3104, validation loss = 0.3085. Learning rate = 0.001. Patience = 2\n",
      "Epoch 30: training loss = 0.3087, validation loss = 0.3079. Learning rate = 0.001. Patience = 3\n",
      "Epoch 31: training loss = 0.3119, validation loss = 0.3040. Learning rate = 0.001. Patience = 4\n",
      "Epoch 32: training loss = 0.3063, validation loss = 0.2974. Learning rate = 0.001. Patience = 0\n",
      "Epoch 33: training loss = 0.2979, validation loss = 0.3011. Learning rate = 0.001. Patience = 1\n",
      "Epoch 34: training loss = 0.2978, validation loss = 0.3065. Learning rate = 0.001. Patience = 2\n",
      "Epoch 35: training loss = 0.3020, validation loss = 0.2979. Learning rate = 0.001. Patience = 3\n",
      "Epoch 36: training loss = 0.3073, validation loss = 0.3048. Learning rate = 0.001. Patience = 4\n",
      "Epoch 37: training loss = 0.2995, validation loss = 0.3019. Learning rate = 0.001. Patience = 5\n",
      "Epoch 38: training loss = 0.2988, validation loss = 0.3147. Learning rate = 0.001. Patience = 6\n",
      "Epoch 39: training loss = 0.2953, validation loss = 0.3030. Learning rate = 0.001. Patience = 7\n",
      "Epoch 40: training loss = 0.2963, validation loss = 0.3050. Learning rate = 0.001. Patience = 8\n",
      "Epoch 41: training loss = 0.2854, validation loss = 0.3196. Learning rate = 0.001. Patience = 9\n",
      "Epoch 42: training loss = 0.2873, validation loss = 0.3004. Learning rate = 0.001. Patience = 10\n",
      "Epoch 43: training loss = 0.2787, validation loss = 0.3100. Learning rate = 0.0005. Patience = 11\n",
      "Epoch 44: training loss = 0.2729, validation loss = 0.2960. Learning rate = 0.0005. Patience = 0\n",
      "Epoch 45: training loss = 0.2798, validation loss = 0.3055. Learning rate = 0.0005. Patience = 1\n",
      "Epoch 46: training loss = 0.2791, validation loss = 0.3188. Learning rate = 0.0005. Patience = 2\n",
      "Epoch 47: training loss = 0.2685, validation loss = 0.3042. Learning rate = 0.0005. Patience = 3\n",
      "Epoch 48: training loss = 0.2704, validation loss = 0.3104. Learning rate = 0.0005. Patience = 4\n",
      "Epoch 49: training loss = 0.2667, validation loss = 0.3097. Learning rate = 0.0005. Patience = 5\n",
      "Epoch 50: training loss = 0.2606, validation loss = 0.3133. Learning rate = 0.0005. Patience = 6\n",
      "Epoch 51: training loss = 0.2684, validation loss = 0.3097. Learning rate = 0.0005. Patience = 7\n",
      "Epoch 52: training loss = 0.2738, validation loss = 0.3142. Learning rate = 0.0005. Patience = 8\n",
      "Epoch 53: training loss = 0.2689, validation loss = 0.3185. Learning rate = 0.0005. Patience = 9\n",
      "Epoch 54: training loss = 0.2710, validation loss = 0.3158. Learning rate = 0.0005. Patience = 10\n",
      "Epoch 55: training loss = 0.2751, validation loss = 0.3148. Learning rate = 0.00025. Patience = 11\n",
      "Epoch 56: training loss = 0.2600, validation loss = 0.3082. Learning rate = 0.00025. Patience = 12\n",
      "Epoch 57: training loss = 0.2613, validation loss = 0.3074. Learning rate = 0.00025. Patience = 13\n",
      "Epoch 58: training loss = 0.2678, validation loss = 0.3139. Learning rate = 0.00025. Patience = 14\n",
      "Epoch 59: training loss = 0.2598, validation loss = 0.3154. Learning rate = 0.00025. Patience = 15\n",
      "Epoch 60: training loss = 0.2562, validation loss = 0.3129. Learning rate = 0.00025. Patience = 16\n",
      "Epoch 61: training loss = 0.2602, validation loss = 0.3089. Learning rate = 0.00025. Patience = 17\n",
      "Epoch 62: training loss = 0.2524, validation loss = 0.3075. Learning rate = 0.00025. Patience = 18\n",
      "Epoch 63: training loss = 0.2605, validation loss = 0.3102. Learning rate = 0.00025. Patience = 19\n",
      "Epoch 64: training loss = 0.2554, validation loss = 0.3098. Learning rate = 0.00025. Patience = 20\n",
      "Epoch 65: training loss = 0.2592, validation loss = 0.3102. Learning rate = 0.00025. Patience = 21\n",
      "Epoch 66: training loss = 0.2614, validation loss = 0.3138. Learning rate = 0.000125. Patience = 22\n",
      "Epoch 67: training loss = 0.2585, validation loss = 0.3128. Learning rate = 0.000125. Patience = 23\n",
      "Epoch 68: training loss = 0.2523, validation loss = 0.3106. Learning rate = 0.000125. Patience = 24\n",
      "Epoch 69: training loss = 0.2569, validation loss = 0.3111. Learning rate = 0.000125. Patience = 25\n",
      "Epoch 70: training loss = 0.2504, validation loss = 0.3112. Learning rate = 0.000125. Patience = 26\n",
      "Epoch 71: training loss = 0.2481, validation loss = 0.3113. Learning rate = 0.000125. Patience = 27\n",
      "Epoch 72: training loss = 0.2519, validation loss = 0.3107. Learning rate = 0.000125. Patience = 28\n",
      "Epoch 73: training loss = 0.2603, validation loss = 0.3125. Learning rate = 0.000125. Patience = 29\n",
      "Epoch 74: training loss = 0.2554, validation loss = 0.3137. Learning rate = 0.000125. Patience = 30\n",
      "Epoch 75: training loss = 0.2424, validation loss = 0.3143. Learning rate = 0.000125. Patience = 31\n",
      "Epoch 76: training loss = 0.2585, validation loss = 0.3137. Learning rate = 0.000125. Patience = 32\n",
      "Epoch 77: training loss = 0.2545, validation loss = 0.3136. Learning rate = 6.25e-05. Patience = 33\n",
      "Epoch 78: training loss = 0.2505, validation loss = 0.3124. Learning rate = 6.25e-05. Patience = 34\n",
      "Epoch 79: training loss = 0.2485, validation loss = 0.3118. Learning rate = 6.25e-05. Patience = 35\n",
      "Epoch 80: training loss = 0.2483, validation loss = 0.3116. Learning rate = 6.25e-05. Patience = 36\n",
      "Epoch 81: training loss = 0.2476, validation loss = 0.3121. Learning rate = 6.25e-05. Patience = 37\n",
      "Epoch 82: training loss = 0.2470, validation loss = 0.3125. Learning rate = 6.25e-05. Patience = 38\n",
      "Epoch 83: training loss = 0.2425, validation loss = 0.3130. Learning rate = 6.25e-05. Patience = 39\n",
      "Epoch 84: training loss = 0.2584, validation loss = 0.3141. Learning rate = 6.25e-05. Patience = 40\n",
      "Epoch 85: training loss = 0.2515, validation loss = 0.3133. Learning rate = 6.25e-05. Patience = 41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86: training loss = 0.2492, validation loss = 0.3135. Learning rate = 6.25e-05. Patience = 42\n",
      "Epoch 87: training loss = 0.2514, validation loss = 0.3155. Learning rate = 6.25e-05. Patience = 43\n",
      "Epoch 88: training loss = 0.2466, validation loss = 0.3168. Learning rate = 3.125e-05. Patience = 44\n",
      "Epoch 89: training loss = 0.2527, validation loss = 0.3166. Learning rate = 3.125e-05. Patience = 45\n",
      "Epoch 90: training loss = 0.2452, validation loss = 0.3167. Learning rate = 3.125e-05. Patience = 46\n",
      "Epoch 91: training loss = 0.2560, validation loss = 0.3167. Learning rate = 3.125e-05. Patience = 47\n",
      "Epoch 92: training loss = 0.2510, validation loss = 0.3167. Learning rate = 3.125e-05. Patience = 48\n",
      "Epoch 93: training loss = 0.2478, validation loss = 0.3164. Learning rate = 3.125e-05. Patience = 49\n",
      "Epoch 94: training loss = 0.2494, validation loss = 0.3159. Learning rate = 3.125e-05. Patience = 50\n",
      "Training stopped at epoch 94; restoring weights from epoch 44\n"
     ]
    }
   ],
   "source": [
    "df_result = pd.DataFrame()\n",
    "#\n",
    "for family in df['family'].unique():\n",
    "    # perform train-test splitting\n",
    "    df_train, df_valid, df_test, scaler = train_test_splitting(df = df[df['family'] == family].reset_index(drop = True).drop('family', axis = 1),\n",
    "                                                               dict_params = dict_params)\n",
    "    #\n",
    "    horizon_forecast = dict_params['data']['horizon_forecast']\n",
    "    # training set data\n",
    "    x_train, y_train, date_x_train, date_y_train = get_x_y(df = df_train, df_future = df_valid, dict_params = dict_params,\n",
    "                                                           test_set = False, horizon_forecast = horizon_forecast)\n",
    "    # validation set data\n",
    "    x_valid, y_valid, date_x_valid, date_y_valid = get_x_y(df = df_valid, df_future = df_test, dict_params = dict_params,\n",
    "                                                           test_set = False, horizon_forecast = horizon_forecast)\n",
    "    # test set data\n",
    "    x_test, y_test, date_x_test, date_y_test = get_x_y(df = df_test, df_future = None, dict_params = dict_params, test_set = True,\n",
    "                                                       horizon_forecast = horizon_forecast)\n",
    "    # create datasets and dataloader\n",
    "    dataset_train = CreateDataset(x = x_train, y = y_train)\n",
    "    dataset_valid = CreateDataset(x = x_valid, y = y_valid)\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size = dict_params['training']['batch_size'], shuffle = True)\n",
    "    dataloader_valid = DataLoader(dataset_valid, batch_size = dict_params['training']['batch_size'], shuffle = False)\n",
    "    # define the model\n",
    "    if 'model' in locals():\n",
    "        del model\n",
    "    len_series = dataloader_train.dataset.x.shape[2]\n",
    "    num_feat = dataloader_train.dataset.x.shape[1]\n",
    "    len_output = dataloader_train.dataset.y.shape[2]\n",
    "    model = TCN(len_series = len_series, num_feat = num_feat, len_output = len_output, dict_params = dict_params,\n",
    "                gated_activation = False)\n",
    "    # perform training\n",
    "    model, list_loss_train, list_loss_valid = TrainTCN(model = model, dict_params = dict_params,\n",
    "                                                       dataloader_train = dataloader_train,\n",
    "                                                       dataloader_valid = dataloader_valid).train_model()\n",
    "    # load best parameters\n",
    "    if 'model' in locals():\n",
    "        del model\n",
    "    len_series = dataloader_train.dataset.x.shape[2]\n",
    "    num_feat = dataloader_train.dataset.x.shape[1]\n",
    "    len_output = dataloader_train.dataset.y.shape[2]\n",
    "    model = TCN(len_series = len_series, num_feat = num_feat, len_output = len_output, dict_params = dict_params,\n",
    "                gated_activation = False)\n",
    "    #\n",
    "    model.load_state_dict(torch.load('../data/artifacts/weights.p'))\n",
    "    model.eval()\n",
    "    # get time series and the corresponding predictions\n",
    "    y_true_train, y_hat_train = get_y_true_y_hat(model = model, x = x_train, y = y_train, date_y = date_y_train, scaler = scaler)\n",
    "    y_true_valid, y_hat_valid = get_y_true_y_hat(model = model, x = x_valid, y = y_valid, date_y = date_y_valid, scaler = scaler)\n",
    "    y_true_test, y_hat_test = get_y_true_y_hat(model = model, x = x_test, y = y_test, date_y = date_y_test, scaler = scaler)\n",
    "    # compute mape on training, validation and test set\n",
    "    mape_train = compute_mape(y_true = y_true_train, y_hat = y_hat_train)\n",
    "    mape_valid = compute_mape(y_true = y_true_valid, y_hat = y_hat_valid)\n",
    "    mape_test = compute_mape(y_true = y_true_test, y_hat = y_hat_test)\n",
    "    #\n",
    "    df_result = pd.concat((df_result, pd.DataFrame({'family': [family], 'mape_train': [mape_train], 'mape_valid': [mape_valid],\n",
    "                                                    'mape_test': [mape_test]})))\n",
    "#\n",
    "df_result = df_result.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "b08a4598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7999706.5"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result['mape_test'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "a2c3f7c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13a52f19d50>]"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(df[df['family'] == 'AUTOMOTIVE']['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "2a7e5c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_date = []\n",
    "list_y_true = []\n",
    "list_y_hat = []\n",
    "preds = model(x_test)\n",
    "for i in range(np.unique(date_y_test).shape[0]):\n",
    "    date = np.unique(date_y_test)[i]\n",
    "    list_date.append(date)\n",
    "    idx = np.where(date_y_test == date)\n",
    "    list_y_true.append(y_test.numpy()[idx].mean())\n",
    "    list_y_hat.append(preds.detach().numpy()[idx].mean())\n",
    "list_y_true = np.array(list_y_true)\n",
    "list_y_hat = np.array(list_y_hat)\n",
    "# scale back\n",
    "list_y_true = scaler.inverse_transform(list_y_true.reshape(-1, 1)).ravel()\n",
    "list_y_hat = scaler.inverse_transform(list_y_hat.reshape(-1, 1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "393d03bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = [10, 6])\n",
    "plt.plot(list_y_true, c = 'r', label = 'True')\n",
    "plt.plot(np.arange(len(list_y_hat)) - 0*x_test.shape[2], list_y_hat, c = 'b', label = 'Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Date', fontsize = 14)\n",
    "plt.ylabel('Sales', fontsize = 14)\n",
    "plt.savefig('../docs/figures_for_readme/result.png', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trading",
   "language": "python",
   "name": "trading"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
